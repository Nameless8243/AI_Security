---
id: 01_core_concepts
title: "01 â€“ Core Concepts / Alap fogalmak"
lang: ["hu", "en"]
version: "3.1"
vault: "AI Security Research Vault 2.0"
section_type: "00_Foundations"
role: "core_concepts"
tags:
  - ai_security
  - foundations
  - underscore_slugs
---
ğŸš¨ COPY START ğŸš¨
# Core Concepts  
*The pillars that define how AI systems can be trusted, attacked, or defended*  

---

## ğŸŒ Overview  

**EN:**  
Core Concepts represent the **theoretical backbone** of AI Security â€” the essential principles that describe how machine learning systems function, where they fail, and how they can be protected.  
They form the *bridge between AI science and cybersecurity engineering*. ğŸ§ ğŸ›¡ï¸  

Understanding these concepts is crucial because AI systems do not operate like traditional IT assets.  
They evolve, adapt, and sometimes behave unpredictably. Security, therefore, must account for **data dynamics**, **model behavior**, and **ethical context** â€” not just code or firewalls.  

**HU:**  
A **Core Concepts** az MI-biztonsÃ¡g **elmÃ©leti gerincÃ©t** alkotjÃ¡k â€“ azok az alapelvek, amelyek leÃ­rjÃ¡k, hogyan mÅ±kÃ¶dnek, hol hibÃ¡zhatnak Ã©s hogyan vÃ©dhetÅ‘k meg a gÃ©pi tanulÃ¡si rendszerek. ğŸ§©  
Ezek kÃ©pezik a **hÃ­d** szerepÃ©t az MI-tudomÃ¡ny Ã©s a kiberbiztonsÃ¡gi mÃ©rnÃ¶ki gyakorlat kÃ¶zÃ¶tt.  

Az MI-rendszerek nem mÅ±kÃ¶dnek Ãºgy, mint a hagyomÃ¡nyos informatikai eszkÃ¶zÃ¶k â€“ **vÃ¡ltoznak, tanulnak, nÃ©ha kiszÃ¡mÃ­thatatlanul viselkednek**.  
EzÃ©rt az MI-biztonsÃ¡g nemcsak kÃ³drÃ³l vagy tÅ±zfalakrÃ³l szÃ³l, hanem az **adatdinamikÃ¡rÃ³l, a modellviselkedÃ©srÅ‘l Ã©s az etikai kontextusrÃ³l** is.  

---

## ğŸ§© 1. Data Integrity and Provenance  

**EN:**  
Every AI system is only as trustworthy as its data.  
**Data Integrity** ensures that inputs are unaltered, consistent, and authentic.  
**Data Provenance** tracks *where* data comes from and *how* it has been transformed â€” the foundation of accountability.  

Drift, poisoning, or unauthorized manipulation of data can silently erode security guarantees.  
Thatâ€™s why data must be continuously verified through hashing, digital signatures, and lineage tracking.  

**HU:**  
Minden MI-rendszer annyira megbÃ­zhatÃ³, amennyire az adatai azok.  
A **Data Integrity** biztosÃ­tja, hogy a bemenetek vÃ¡ltozatlanok, konzisztenssek Ã©s hitelesek legyenek.  
A **Data Provenance** pedig nyomon kÃ¶veti, *honnan* szÃ¡rmaznak az adatok Ã©s *hogyan* alakultak Ã¡t â€“ ez az elszÃ¡moltathatÃ³sÃ¡g alapja.  

Az adatok elcsÃºszÃ¡sa, megmÃ©rgezÃ©se vagy jogosulatlan mÃ³dosÃ­tÃ¡sa **csendesen alÃ¡Ã¡shatja** a biztonsÃ¡got.  
EzÃ©rt elengedhetetlen a folyamatos ellenÅ‘rzÃ©s hash-ekkel, digitÃ¡lis alÃ¡Ã­rÃ¡sokkal Ã©s adatvonal-kÃ¶vetÃ©ssel.  

---

## ğŸ’¡ 2. Model Reliability and Drift  

**EN:**  
Models are **dynamic entities**, not static code. Their reliability depends on how well they generalize beyond training data.  
When real-world conditions evolve, **[[model_drift|Model Drift]]** emerges â€” causing the modelâ€™s predictions to deviate from truth.  

The security implication: drift can mask adversarial manipulation or cause unnoticed failures in automated pipelines.  
Continuous retraining, validation, and drift detection are therefore part of every secure AI lifecycle.  

**HU:**  
A modellek **dinamikus rendszerek**, nem statikus programok. MegbÃ­zhatÃ³sÃ¡guk azon mÃºlik, mennyire kÃ©pesek Ã¡ltalÃ¡nosÃ­tani a tanÃ­tÃ³adatokon tÃºl.  
Ha a valÃ³sÃ¡g megvÃ¡ltozik, megjelenik a **[[model_drift|Model Drift]]**, amely eltÃ©rÃ­ti az elÅ‘rejelzÃ©seket a valÃ³sÃ¡gtÃ³l.  

BiztonsÃ¡gi szempontbÃ³l ez veszÃ©lyes: a drift **elfedheti az adverszÃ¡riÃ¡lis manipulÃ¡ciÃ³t** vagy rejtett hibÃ¡kat okozhat az automatizÃ¡lt rendszerekben.  
EzÃ©rt az ÃºjratanÃ­tÃ¡s, validÃ¡ciÃ³ Ã©s drift-megfigyelÃ©s minden biztonsÃ¡gos MI-Ã©letciklus rÃ©sze.  

---

## ğŸ›¡ï¸ 3. Trust Boundaries and Zero Trust  

**EN:**  
AI systems operate across multiple trust layers: users, data pipelines, APIs, and models.  
Each layer can be **compromised independently**, meaning that no single component should be implicitly trusted.  
This principle is formalized in **[[zero_trust_for_ai|Zero Trust for AI]]**, which enforces verification and authentication at every step.  

In practice, Zero Trust means:  
- never assuming an input is safe,  
- never assuming a model is unaltered, and  
- never assuming outputs are harmless.  

**HU:**  
Az MI-rendszerek tÃ¶bb **bizalmi rÃ©teg** kÃ¶zÃ¶tt mÅ±kÃ¶dnek: felhasznÃ¡lÃ³k, adatfolyamok, API-k Ã©s modellek.  
BÃ¡rmelyik rÃ©teg **Ã¶nÃ¡llÃ³an is kompromittÃ¡lÃ³dhat**, ezÃ©rt semmit sem szabad implicit mÃ³don megbÃ­zottnak tekinteni.  
Ezt az elvet a **[[zero_trust_for_ai|Zero Trust for AI]]** foglalja rendszerbe, amely minden ponton hitelesÃ­tÃ©st Ã©s ellenÅ‘rzÃ©st kÃ¶vetel.  

A gyakorlatban a Zero Trust azt jelenti:  
- soha ne feltÃ©telezd, hogy a bemenet biztonsÃ¡gos,  
- soha ne feltÃ©telezd, hogy a modell sÃ©rtetlen,  
- Ã©s soha ne feltÃ©telezd, hogy a kimenet Ã¡rtalmatlan.  

---

## âš™ï¸ 4. Explainability and Interpretability  

**EN:**  
A secure AI is not only one that resists attacks â€” it is one that can be **understood and verified**.  
**[[explainability|Explainability]]** and **[[interpretability|Interpretability]]** transform opaque black-box behavior into something traceable and auditable.  

For example, the contribution of each feature can be expressed using *Integrated Gradients*:  

$$
\mathrm{IG}_i(x) = (x_i - x'_i) \times \int_{0}^{1} \frac{\partial F(x' + \alpha(x - x'))}{\partial x_i} \, d\alpha
$$  

Such transparency is essential for debugging, fairness analysis, and compliance with governance frameworks like **[[ai_governance|AI Governance]]**.  

**HU:**  
A biztonsÃ¡gos MI nem csak az, amely ellenÃ¡ll a tÃ¡madÃ¡soknak â€“ hanem az is, amely **Ã©rthetÅ‘ Ã©s ellenÅ‘rizhetÅ‘**.  
Az **[[explainability|Explainability]]** Ã©s az **[[interpretability|Interpretability]]** lehetÅ‘vÃ© teszik, hogy az Ã¡tlÃ¡thatatlan â€fekete dobozâ€ mÅ±kÃ¶dÃ©s **vizsgÃ¡lhatÃ³ Ã©s auditÃ¡lhatÃ³** legyen.  

PÃ©ldÃ¡ul a jellemzÅ‘k hozzÃ¡jÃ¡rulÃ¡sa az alÃ¡bbi *Integrated Gradients* kÃ©plettel Ã­rhatÃ³ le:  

$$
\mathrm{IG}_i(x) = (x_i - x'_i) \times \int_{0}^{1} \frac{\partial F(x' + \alpha(x - x'))}{\partial x_i} \, d\alpha
$$  

Ez a transzparencia elengedhetetlen a hibakeresÃ©shez, a fairness elemzÃ©shez Ã©s az olyan irÃ¡nyÃ­tÃ¡si keretek betartÃ¡sÃ¡hoz, mint az **[[ai_governance|AI Governance]]**.  

---

## ğŸ” 5. Privacy and Inference Control  

**EN:**  
AI systems often process personal or proprietary information.  
If not carefully designed, they can leak data through outputs â€” as in **[[membership_inference_attacks|Membership Inference]]** or **[[model_stealing|Model Stealing]]**.  

Privacy-enhancing methods such as **[[differential_privacy|Differential Privacy]]** inject statistical noise during training to obscure individual records:  

$$
P(f(D) \in S) \leq e^\epsilon \cdot P(f(D') \in S)
$$  

Here, \( \epsilon \) represents the *privacy budget*: smaller values mean stronger privacy but lower utility.  

**HU:**  
Az MI-rendszerek gyakran kezelnek szemÃ©lyes vagy tulajdonosi informÃ¡ciÃ³kat.  
Ha nem megfelelÅ‘en vannak megtervezve, **adatokat szivÃ¡rogtathatnak** a kimeneteken keresztÃ¼l â€“ pÃ©ldÃ¡ul **[[membership_inference_attacks|Membership Inference]]** vagy **[[model_stealing|Model Stealing]]** formÃ¡jÃ¡ban.  

Az olyan adatvÃ©delmi technikÃ¡k, mint a **[[differential_privacy|Differential Privacy]]**, statisztikai zajt adnak a tanÃ­tÃ¡shoz, hogy elrejtsÃ©k az egyedi rekordokat:  

$$
P(f(D) \in S) \leq e^\epsilon \cdot P(f(D') \in S)
$$  

Itt \( \epsilon \) az *adatvÃ©delmi kÃ¶ltsÃ©gvetÃ©s*: minÃ©l kisebb, annÃ¡l erÅ‘sebb a vÃ©delem â€“ de annÃ¡l gyengÃ©bb az eredmÃ©ny hasznossÃ¡ga.  

---

## ğŸ§  6. Governance, Ethics, and Accountability  

**EN:**  
Even the most secure AI can cause harm if deployed without ethical or legal oversight.  
**[[ai_governance|AI Governance]]** defines processes for transparency, fairness, and human control.  
It connects technical defenses with regulatory compliance and social responsibility.  

AI Security and AI Ethics must coexist â€” without governance, trust collapses; without security, governance is meaningless.  

**HU:**  
A legbiztonsÃ¡gosabb MI is okozhat kÃ¡rt, ha **etikai vagy jogi felÃ¼gyelet nÃ©lkÃ¼l** mÅ±kÃ¶dik.  
Az **[[ai_governance|AI Governance]]** szabÃ¡lyozza az Ã¡tlÃ¡thatÃ³sÃ¡got, a fairness-t Ã©s az emberi kontrollt, Ã¶sszekÃ¶tve a technikai vÃ©delmet a jogi Ã©s tÃ¡rsadalmi felelÅ‘ssÃ©ggel.  

Az MI-biztonsÃ¡g Ã©s az MI-etika **egymÃ¡s nÃ©lkÃ¼l Ã©rtelmetlen**:  
irÃ¡nyÃ­tÃ¡s nÃ©lkÃ¼l a bizalom Ã¶sszeomlik, biztonsÃ¡g nÃ©lkÃ¼l pedig az irÃ¡nyÃ­tÃ¡s Ã¼res forma.  

---

## ğŸ“Š 7. Continuous Assurance and Observability  

**EN:**  
Security is not a one-time state â€” itâ€™s a continuous measurement.  
Through **[[observability_and_monitoring|Observability and Monitoring]]**, systems collect real-time metrics that indicate anomalies, drift, or attacks.  

The confidence in model safety can be dynamically updated based on detected anomalies \( A_t \):  

$$
T_{t+1} = T_t \cdot e^{-\lambda A_t}
$$  

Such adaptive assurance loops turn AI security into a *living system*, not a static control.  

**HU:**  
A biztonsÃ¡g nem egyszeri Ã¡llapot â€“ **folyamatos mÃ©rÃ©s**.  
Az **[[observability_and_monitoring|Observability and Monitoring]]** segÃ­tsÃ©gÃ©vel a rendszerek valÃ³s idÅ‘ben gyÅ±jtenek adatokat az anomÃ¡liÃ¡krÃ³l, drift-rÅ‘l vagy tÃ¡madÃ¡sokrÃ³l.  

A modell biztonsÃ¡gÃ¡ba vetett bizalom idÅ‘ben frissÃ­thetÅ‘ a kimutatott anomÃ¡liÃ¡k fÃ¼ggvÃ©nyÃ©ben \( A_t \):  

$$
T_{t+1} = T_t \cdot e^{-\lambda A_t}
$$  

EzÃ¡ltal az MI-biztonsÃ¡g **Ã©lÅ‘ rendszerrÃ©** vÃ¡lik, nem statikus kontrollfolyamattÃ¡.  

---

## ğŸ§© Related Vault Topics  

- [[data_provenance|Data Provenance]]  
- [[model_drift|Model Drift]]  
- [[zero_trust_for_ai|Zero Trust for AI]]  
- [[membership_inference_attacks|Membership Inference]]  
- [[ai_governance|AI Governance]]  
- [[observability_and_monitoring|Observability and Monitoring]]  

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek  

1. **EN:** Why must AI security address both technical and ethical layers simultaneously?  
   **HU:** MiÃ©rt kell az MI-biztonsÃ¡gnak egyszerre a technikai Ã©s etikai rÃ©tegekre is kiterjednie?  

2. **EN:** How does model drift impact security and reliability?  
   **HU:** Hogyan befolyÃ¡solja a modelldrift a biztonsÃ¡got Ã©s a megbÃ­zhatÃ³sÃ¡got?  

3. **EN:** What is the difference between Zero Trust and simple access control?  
   **HU:** Mi a kÃ¼lÃ¶nbsÃ©g a Zero Trust Ã©s az egyszerÅ± hozzÃ¡fÃ©rÃ©s-vezÃ©rlÃ©s kÃ¶zÃ¶tt?  

4. **EN:** How does differential privacy mathematically ensure that training data cannot be reidentified?  
   **HU:** Hogyan biztosÃ­tja matematikailag a differenciÃ¡lis adatvÃ©delem, hogy a tanÃ­tÃ³adatok ne legyenek visszafejthetÅ‘k?  

5. **EN:** Why is continuous observability considered the â€œheartbeatâ€ of AI Security?  
   **HU:** MiÃ©rt nevezik a folyamatos megfigyelÃ©st az MI-biztonsÃ¡g â€Ã©rrendszerÃ©nekâ€?  

---

> â€œSecurity is not about defense alone â€” itâ€™s about understanding what we are defending.â€ ğŸ’¡  

ğŸš¨ COPY END ğŸš¨
