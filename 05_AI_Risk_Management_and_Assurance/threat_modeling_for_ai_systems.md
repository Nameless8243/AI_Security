---
version: "3.2"
section_type: "risk"
agent: "Threat Mapper"
---
---
title: Threat Modeling for AI Systems
phase: Design
category: Risk Analysis & Architecture
difficulty: Advanced
related: [data_poisoning_attacks, model_stealing_and_extraction, adversarial_training, ai_risk_assessment_methodology, control_framework_and_baselines]
updated: 2025-11-10
---

# âš”ï¸ Threat Modeling for AI Systems / FenyegetÃ©smodellezÃ©s MI-rendszerekhez

**EN:**  
Threat modeling for AI identifies, analyzes, and prioritizes **potential adversarial paths and systemic weaknesses** across the model lifecycle.  
It brings structured reasoning to AI risk â€” transforming abstract fears into *actionable mitigations*.  
Unlike traditional IT systems, AI introduces **new threat surfaces** (data, models, and prompts), demanding domain-specific frameworks and defensive patterns.  

**HU:**  
Az MI-rendszerek fenyegetÃ©smodellezÃ©se az **adverszÃ¡riÃ¡lis utak Ã©s rendszerhibÃ¡k azonosÃ­tÃ¡sÃ¡t, elemzÃ©sÃ©t Ã©s rangsorolÃ¡sÃ¡t** jelenti az egÃ©sz modellÃ©letciklusban.  
CÃ©lja, hogy a kockÃ¡zatokat strukturÃ¡lt gondolkodÃ¡ssal *konkrÃ©t megelÅ‘zÅ‘ intÃ©zkedÃ©sekkÃ©* alakÃ­tsa.  
A hagyomÃ¡nyos IT-rendszerektÅ‘l eltÃ©rÅ‘en az MI **Ãºj fenyegetÃ©si felÃ¼leteket** hoz lÃ©tre (adat, modell, prompt), ezÃ©rt speciÃ¡lis mÃ³dszertanokra Ã©s vÃ©delmi mintÃ¡kra van szÃ¼ksÃ©g. ğŸ§   

---

## ğŸŒ Concept Overview / Fogalmi Ã¡ttekintÃ©s

**EN:**  
Threat modeling in AI combines:
- **Adversarial thinking:** how attackers can exploit model behavior.  
- **Systematic decomposition:** understanding components, dependencies, and data flows.  
- **Attack surface mapping:** identifying where control, data, or trust can be subverted.  
It ensures that security, privacy, and fairness controls are *built in* â€” not bolted on.  

**HU:**  
Az MI-fenyegetÃ©smodellezÃ©s hÃ¡rom fÅ‘ megkÃ¶zelÃ­tÃ©st egyesÃ­t:  
- **AdverszÃ¡riÃ¡lis gondolkodÃ¡s:** hogyan tudja a tÃ¡madÃ³ kihasznÃ¡lni a modell viselkedÃ©sÃ©t.  
- **Rendszeres bontÃ¡s:** a komponensek, fÃ¼ggÅ‘sÃ©gek Ã©s adatÃ¡ramlÃ¡s megÃ©rtÃ©se.  
- **TÃ¡madÃ¡si felÃ¼letek feltÃ©rkÃ©pezÃ©se:** hol sÃ©rÃ¼lhet a kontroll, az adat vagy a bizalom.  
Ãgy biztosÃ­thatÃ³, hogy a biztonsÃ¡g, adatvÃ©delem Ã©s mÃ©ltÃ¡nyossÃ¡g **beÃ©pÃ¼ljÃ¶n a rendszerbe, ne utÃ³lag toldjÃ¡k hozzÃ¡**. âš™ï¸  

---

## ğŸ’¡ Core Idea / Alapgondolat

**EN:**  
Traditional STRIDE or PASTA frameworks are not sufficient for AI because they assume deterministic systems.  
AI behaves probabilistically and learns from mutable data â€” its â€œattack surfaceâ€ *evolves over time*.  
Therefore, threat modeling must integrate **learning dynamics, model drift, and adversarial adaptation**.  

**HU:**  
A klasszikus STRIDE vagy PASTA keretrendszerek nem elegendÅ‘k az MI-re, mert determinisztikus rendszerekkel szÃ¡molnak.  
Az MI viszont valÃ³szÃ­nÅ±sÃ©gi mÃ³don viselkedik, Ã©s vÃ¡ltozÃ³ adatokbÃ³l tanul â€” vagyis a *tÃ¡madÃ¡si felÃ¼lete folyamatosan fejlÅ‘dik*.  
EzÃ©rt a fenyegetÃ©smodellezÃ©snek figyelembe kell vennie a **tanulÃ¡si dinamikÃ¡t, a modell-sodrÃ³dÃ¡st Ã©s az adverszÃ¡riÃ¡lis alkalmazkodÃ¡st**. ğŸ”„  

---

## ğŸ§© Threat Modeling Lifecycle / FenyegetÃ©smodellezÃ©si Ã©letciklus

**EN:**  
1. **Define the scope:** identify AI assets â€” datasets, models, APIs, pipelines.  
2. **Decompose the system:** visualize data and control flows.  
3. **Identify threats:** use frameworks (e.g., MITRE ATLAS, OWASP ML Top 10).  
4. **Assess risks:** calculate impact Ã— likelihood ([[ai_risk_assessment_methodology]]).  
5. **Define mitigations:** design controls and align with [[control_framework_and_baselines]].  
6. **Validate continuously:** monitor drift and new adversarial techniques ([[continuous_validation_and_review]]).  

**HU:**  
1. **HatÃ¡rozd meg a hatÃ³kÃ¶rt:** azonosÃ­tsd az MI-eszkÃ¶zÃ¶ket â€” adathalmazok, modellek, API-k, pipeline-ok.  
2. **Bontsd fel a rendszert:** vizualizÃ¡ld az adat- Ã©s vezÃ©rlÃ©sÃ¡ramlÃ¡st.  
3. **AzonosÃ­tsd a fenyegetÃ©seket:** hasznÃ¡lj kereteket (pl. MITRE ATLAS, OWASP ML Top 10).  
4. **Ã‰rtÃ©keld a kockÃ¡zatokat:** szÃ¡mÃ­tsd ki a hatÃ¡s Ã— valÃ³szÃ­nÅ±sÃ©g Ã©rtÃ©ket ([[ai_risk_assessment_methodology]]).  
5. **HatÃ¡rozd meg a megelÅ‘zÃ©seket:** tervezd meg a kontrollokat a [[control_framework_and_baselines]] alapjÃ¡n.  
6. **Folyamatosan Ã©rvÃ©nyesÃ­ts:** figyeld a sodrÃ³dÃ¡st Ã©s az Ãºj tÃ¡madÃ¡si mintÃ¡kat ([[continuous_validation_and_review]]). ğŸ§±  

---

## âš”ï¸ AI-Specific Threat Categories / MI-specifikus fenyegetÃ©si kategÃ³riÃ¡k

**EN:**  
1. **Data Poisoning:** manipulation of training data to degrade performance ([[data_poisoning_attacks]]).  
2. **Model Extraction:** replication of model logic through API probing ([[model_stealing_and_extraction]]).  
3. **Membership Inference:** identifying whether specific data points were used in training.  
4. **Adversarial Perturbation:** crafting inputs that exploit model sensitivity.  
5. **Prompt Injection:** manipulating input text to subvert generative model behavior.  
6. **Model Drift Exploitation:** exploiting unmonitored updates or retraining pipelines.  
7. **Supply Chain Manipulation:** inserting malicious dependencies or pre-trained weights.  

**HU:**  
1. **AdatmÃ©rgezÃ©s:** a tanÃ­tÃ³adat manipulÃ¡lÃ¡sa a teljesÃ­tmÃ©ny rombolÃ¡sa Ã©rdekÃ©ben ([[data_poisoning_attacks]]).  
2. **ModellkivonÃ¡s:** a modell logikÃ¡jÃ¡nak mÃ¡solÃ¡sa API-prÃ³bÃ¡lgatÃ¡ssal ([[model_stealing_and_extraction]]).  
3. **TagsÃ¡gi kÃ¶vetkeztetÃ©s:** annak kiderÃ­tÃ©se, hogy egy adott adat szerepelt-e a tanÃ­tÃ¡sban.  
4. **AdverszÃ¡riÃ¡lis perturbÃ¡ciÃ³:** a modell Ã©rzÃ©kenysÃ©gÃ©nek kihasznÃ¡lÃ¡sa manipulÃ¡lt bemenetekkel.  
5. **Prompt Injection:** bemenet manipulÃ¡lÃ¡sa a generatÃ­v modellek viselkedÃ©sÃ©nek torzÃ­tÃ¡sÃ¡ra.  
6. **ModellsodrÃ³dÃ¡s kihasznÃ¡lÃ¡sa:** nem monitorozott ÃºjratanÃ­tÃ¡s vagy frissÃ­tÃ©s manipulÃ¡lÃ¡sa.  
7. **EllÃ¡tÃ¡si lÃ¡nc manipulÃ¡ciÃ³:** rosszindulatÃº fÃ¼ggÅ‘sÃ©gek vagy elÅ‘tanÃ­tott sÃºlyok beillesztÃ©se. ğŸ§©  

---

## ğŸ§® Threat Prioritization Matrix / FenyegetÃ©s-prioritÃ¡si mÃ¡trix

**EN:**  
Threats can be scored using an AI-adapted DREAD model:
$$
Risk = (Damage + Reproducibility + Exploitability + Affected Users + Discoverability) / 5
$$  
High-risk vectors are then correlated with likelihood and mapped to specific AI layers (data, model, deployment).  

**HU:**  
A fenyegetÃ©sek AI-ra adaptÃ¡lt DREAD-modellel Ã©rtÃ©kelhetÅ‘k:
$$
KockÃ¡zat = (KÃ¡r + ReprodukÃ¡lhatÃ³sÃ¡g + KihasznÃ¡lhatÃ³sÃ¡g + Ã‰rintett\ felhasznÃ¡lÃ³k + FelfedezhetÅ‘sÃ©g) / 5
$$  
A magas kockÃ¡zatÃº vektorokat ezutÃ¡n a valÃ³szÃ­nÅ±sÃ©ggel kombinÃ¡lva hozzÃ¡rendeljÃ¼k az MI-rÃ©tegekhez (adat, modell, Ã¼zembe helyezÃ©s). ğŸ“Š  

---

## ğŸ” Defensive Design Principles / VÃ©dekezÃ©si tervezÃ©si elvek

**EN:**  
- **Minimize attack surface:** isolate training, inference, and deployment.  
- **Implement Zero Trust for AI pipelines** ([[zero_trust_for_ai]]).  
- **Verify all external inputs:** data, prompts, model updates.  
- **Embed anomaly detection and explainability controls** ([[model_integrity_monitoring]], [[ai_fairness_and_transparency_governance]]).  
- **Apply secure-by-design architecture:** least privilege, encryption, and PKI-based attestation ([[model_release_and_signing]]).  

**HU:**  
- **CsÃ¶kkentsd a tÃ¡madÃ¡si felÃ¼letet:** vÃ¡laszd szÃ©t a tanÃ­tÃ¡st, az inferenciÃ¡t Ã©s az Ã¼zembe helyezÃ©st.  
- **Alkalmazd a Zero Trust elvet az MI-pipeline-okban** ([[zero_trust_for_ai]]).  
- **EllenÅ‘rizd az Ã¶sszes kÃ¼lsÅ‘ bemenetet:** adatok, promptok, modellfrissÃ­tÃ©sek.  
- **Ã‰pÃ­ts be anomÃ¡liaÃ©rzÃ©kelÃ©st Ã©s magyarÃ¡zhatÃ³sÃ¡gi kontrollokat** ([[model_integrity_monitoring]], [[ai_fairness_and_transparency_governance]]).  
- **BiztonsÃ¡gos tervezÃ©s:** legkisebb jogosultsÃ¡g, titkosÃ­tÃ¡s Ã©s PKI-alapÃº hitelesÃ­tÃ©s ([[model_release_and_signing]]). ğŸ›¡ï¸  

---

## âš–ï¸ Governance Integration / IrÃ¡nyÃ­tÃ¡si integrÃ¡ciÃ³

**EN:**  
Threat modeling outputs directly inform governance and compliance frameworks:
- Feed results into [[model_risk_management_and_registers]] and [[ai_risk_assessment_methodology]].  
- Document mitigations in control baselines ([[control_framework_and_baselines]]).  
- Communicate findings via structured reporting ([[reporting_and_communication]]).  

**HU:**  
A fenyegetÃ©smodellezÃ©s eredmÃ©nyei kÃ¶zvetlenÃ¼l beÃ©pÃ¼lnek az irÃ¡nyÃ­tÃ¡si Ã©s megfelelÅ‘sÃ©gi keretekbe:  
- Az eredmÃ©nyek beÃ©pÃ­tÃ©se a [[model_risk_management_and_registers]] Ã©s [[ai_risk_assessment_methodology]] rendszerekbe.  
- A megelÅ‘zÅ‘ intÃ©zkedÃ©sek dokumentÃ¡lÃ¡sa a kontrollalapokban ([[control_framework_and_baselines]]).  
- Az eredmÃ©nyek kommunikÃ¡lÃ¡sa strukturÃ¡lt jelentÃ©sekben ([[reporting_and_communication]]). âš–ï¸  

---

## ğŸš€ Future Directions / JÃ¶vÅ‘beli irÃ¡nyok

**EN:**  
AI threat modeling will evolve into **autonomous adversarial simulation systems** â€” where defensive AIs continuously predict, simulate, and neutralize emerging threats.  
Future frameworks will include **AI-vs-AI red teaming**, and threat models will update dynamically as the system learns and adapts.  

**HU:**  
Az MI-fenyegetÃ©smodellezÃ©s a jÃ¶vÅ‘ben **autonÃ³m adverszÃ¡riÃ¡lis szimulÃ¡ciÃ³s rendszerekkÃ©** fejlÅ‘dik â€” ahol a vÃ©delmi MI folyamatosan elÅ‘rejelzi, modellezi Ã©s semlegesÃ­ti az Ãºj fenyegetÃ©seket.  
A jÃ¶vÅ‘ keretrendszerei tartalmazni fogjÃ¡k az **MI-ellen-MI red teaming** megkÃ¶zelÃ­tÃ©st, Ã©s a fenyegetÃ©smodellek **dinamikusan frissÃ¼lnek**, ahogy a rendszer tanul Ã©s alkalmazkodik. ğŸ¤–  

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. What makes AI threat modeling different from traditional cybersecurity threat modeling?  
2. How does the AI lifecycle influence the evolving attack surface?  
3. What are the most critical AI-specific threats to consider?  
4. How can threat modeling be integrated into governance and control frameworks?  
5. What role might autonomous adversarial simulations play in the future?  

---

> â€œYou cannot defend what you have not imagined â€” threat modeling is imagination made operational.â€
