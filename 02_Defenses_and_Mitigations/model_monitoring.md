# ğŸ“ˆ Model Monitoring

---

## ğŸŒ What Is Model Monitoring? / Mi az a modell-monitorozÃ¡s?

**EN:**  
**Model monitoring** is the continuous process of tracking an AI modelâ€™s behavior, inputs, outputs, and performance in production to ensure that it remains accurate, secure, and trustworthy over time.  
Itâ€™s how we detect when a model â€œgoes off the railsâ€ â€” whether due to data drift, bias, poisoning, adversarial attacks, or silent degradation.  

In AI security, model monitoring is not just about accuracy â€” itâ€™s about **integrity, reliability, and early detection of threats** to the modelâ€™s decision logic. ğŸ›¡ï¸ğŸ“Š  

**HU:**  
A **modell-monitorozÃ¡s** az a folyamatos folyamat, amely sorÃ¡n az MI-modell viselkedÃ©sÃ©t, bemeneteit, kimeneteit Ã©s teljesÃ­tmÃ©nyÃ©t figyeljÃ¼k a produkciÃ³ban, hogy hosszÃº tÃ¡von pontos, biztonsÃ¡gos Ã©s megbÃ­zhatÃ³ maradjon.  
Ez teszi lehetÅ‘vÃ©, hogy Ã©szrevegyÃ¼k, ha a modell â€letÃ©r a pÃ¡lyÃ¡rÃ³lâ€ â€” akÃ¡r adatdrift, torzÃ­tÃ¡s, mÃ©rgezÃ©s, adverszÃ¡riÃ¡lis tÃ¡madÃ¡s vagy fokozatos romlÃ¡s miatt.  

Az MI-biztonsÃ¡gban a modell-monitorozÃ¡s nemcsak a pontossÃ¡grÃ³l szÃ³l â€” hanem a **modell integritÃ¡sÃ¡nak, megbÃ­zhatÃ³sÃ¡gÃ¡nak Ã©s logikai biztonsÃ¡gÃ¡nak** megÅ‘rzÃ©sÃ©rÅ‘l. ğŸ›¡ï¸ğŸ“Š  

---

## ğŸ’¡ Why It Matters / MiÃ©rt fontos

**EN:**  
Without monitoring, you cannot detect:  
- Gradual accuracy degradation (concept drift)  
- Adversarial probing or [[data_poisoning|data poisoning]]  
- Unauthorized model behavior (e.g., prompt leakage, backdoor activation)  
- Bias amplification or fairness decay  
- Infrastructure misuse or abnormal inference loads  

Monitoring turns AI systems from *black boxes* into *observable systems* â€” enabling incident response, accountability, and compliance.  

**HU:**  
MonitorozÃ¡s nÃ©lkÃ¼l nem Ã©szlelheted:  
- A pontossÃ¡g fokozatos romlÃ¡sÃ¡t (konceptuÃ¡lis drift)  
- [[data_poisoning|AdatmÃ©rgezÃ©s]] vagy adverszÃ¡riÃ¡lis szondÃ¡zÃ¡s jeleit  
- Jogosulatlan modellviselkedÃ©st (pl. prompt-szivÃ¡rgÃ¡s, backdoor-aktivÃ¡lÃ¡s)  
- TorzÃ­tÃ¡s felerÅ‘sÃ¶dÃ©sÃ©t vagy fairness romlÃ¡sÃ¡t  
- InfrastruktÃºra-visszaÃ©lÃ©st vagy abnormÃ¡lis inferencia-terhelÃ©st  

A monitorozÃ¡s Ã¡tlÃ¡thatÃ³vÃ¡ teszi az MI-rendszert: *feketeboxbÃ³l megfigyelhetÅ‘ rendszerrÃ©* alakÃ­tja, tÃ¡mogatva az incidenskezelÃ©st, elszÃ¡moltathatÃ³sÃ¡got Ã©s megfelelÃ©st.  

---

## âš™ï¸ What to Monitor / Mit Ã©rdemes figyelni

**EN:**  
Model monitoring spans multiple dimensions:  

1. **Data Drift:**  
   Detect shifts in input feature distributions compared to training data.  
   (E.g., change in demographics, language, or environmental conditions.)  

2. **Concept Drift:**  
   The relationship between features and labels evolves over time.  

3. **Prediction Behavior:**  
   Confidence entropy, output distribution, or class frequency changes.  

4. **Security Events:**  
   Detect adversarial examples, abnormal query frequency, or injection attempts.  

5. **Fairness & Bias:**  
   Track group-level performance parity over time.  

6. **Operational Metrics:**  
   Latency, throughput, memory usage, rate-limit hits, etc.  

**HU:**  
A modell-monitorozÃ¡s tÃ¶bb dimenziÃ³t fog Ã¡t:  

1. **Adat-drift:**  
   A bemeneti jellemzÅ‘k eloszlÃ¡sÃ¡nak eltolÃ³dÃ¡sa a tanÃ­tÃ³adatokhoz kÃ©pest.  
   (Pl. demogrÃ¡fiai, nyelvi vagy kÃ¶rnyezeti vÃ¡ltozÃ¡sok.)  

2. **KonceptuÃ¡lis drift:**  
   A jellemzÅ‘k Ã©s a cÃ­mkÃ©k kÃ¶zÃ¶tti kapcsolat idÅ‘vel mÃ³dosul.  

3. **PredikciÃ³s viselkedÃ©s:**  
   Bizalmi entrÃ³pia, kimeneti eloszlÃ¡s vagy osztÃ¡lyfrekvencia-vÃ¡ltozÃ¡sok.  

4. **BiztonsÃ¡gi esemÃ©nyek:**  
   AdverszÃ¡riÃ¡lis pÃ©ldÃ¡k, abnormÃ¡lis lekÃ©rdezÃ©s-mintÃ¡k, injekciÃ³s kÃ­sÃ©rletek Ã©szlelÃ©se.  

5. **Fairness Ã©s torzÃ­tÃ¡s:**  
   CsoportszintÅ± teljesÃ­tmÃ©nyparitÃ¡s kÃ¶vetÃ©se idÅ‘ben.  

6. **OperatÃ­v metrikÃ¡k:**  
   KÃ©sleltetÃ©s, Ã¡teresztÅ‘kÃ©pessÃ©g, memÃ³riahasznÃ¡lat, rate-limit talÃ¡latok stb.  

---

## ğŸ§® Formal Drift Detection / FormÃ¡lis drift-Ã©szlelÃ©s

**EN:**  
A modelâ€™s input distribution \( P_{train}(x) \) may diverge from the live distribution \( P_{prod}(x) \).  
We quantify drift using statistical divergence:

$$
D_{KL}(P_{train} \parallel P_{prod}) = \sum_i P_{train}(x_i) \log \frac{P_{train}(x_i)}{P_{prod}(x_i)}
$$

A large \( D_{KL} \) indicates drift â€” prompting model retraining or revalidation.

**HU:**  
A modell bemeneti eloszlÃ¡sa \( P_{train}(x) \) eltÃ©rhet az aktuÃ¡lis produkciÃ³s eloszlÃ¡stÃ³l \( P_{prod}(x) \).  
A driftet statisztikai divergenciÃ¡val mÃ©rjÃ¼k:

$$
D_{KL}(P_{train} \parallel P_{prod}) = \sum_i P_{train}(x_i) \log \frac{P_{train}(x_i)}{P_{prod}(x_i)}
$$

A nagy \( D_{KL} \) driftet jelez â€” ilyenkor ÃºjratanÃ­tÃ¡s vagy revalidÃ¡lÃ¡s szÃ¼ksÃ©ges.  

---

## ğŸ§© Security-Focused Monitoring / BiztonsÃ¡g-kÃ¶zpontÃº monitorozÃ¡s

**EN:**  
AI security requires special observables:  
- **Query anomaly detection:** identify suspicious input sequences or patterns (e.g., probing model boundaries).  
- **Adversarial noise signatures:** track gradient-space perturbation norms.  
- **Backdoor activation triggers:** monitor latent activations for known trigger patterns.  
- **Model extraction traces:** correlate API calls to detect systematic function approximation attempts.  
- **Output leakage detection:** scan model responses for secrets, code, or prompt fragments.  

**HU:**  
Az MI-biztonsÃ¡g speciÃ¡lis megfigyelÃ©si pontokat igÃ©nyel:  
- **LekÃ©rdezÃ©si anomÃ¡lia-Ã©szlelÃ©s:** gyanÃºs inputmintÃ¡k felismerÃ©se (pl. modell-hatÃ¡rok szondÃ¡zÃ¡sa).  
- **AdverszÃ¡riÃ¡lis zaj mintÃ¡k:** gradiens-tÃ©rben mÃ©rt perturbÃ¡ciÃ³k normÃ¡jÃ¡nak kÃ¶vetÃ©se.  
- **Backdoor-aktivÃ¡lÃ¡si jelek:** ismert triggerekhez kapcsolÃ³dÃ³ rejtett aktivÃ¡ciÃ³k monitorozÃ¡sa.  
- **Modell-kinyerÃ©s nyomai:** API-hÃ­vÃ¡sok korrelÃ¡ciÃ³ja a funkciÃ³-approximalÃ¡s felismerÃ©sÃ©re.  
- **Kimeneti szivÃ¡rgÃ¡s-Ã©szlelÃ©s:** modellek vÃ¡laszainak szkennelÃ©se titkok, kÃ³d vagy prompt-tÃ¶redÃ©kek utÃ¡n.  

---

## ğŸ§  Integration & Tooling / IntegrÃ¡ciÃ³ Ã©s eszkÃ¶zÃ¶k

**EN:**  
Monitoring integrates with:  
- [[observability_and_monitoring|Observability Stack]] â€” central metrics and tracing  
- [[consistency_audit|Consistency Audit]] â€” check behavioral stability across versions  
- [[incident_response_for_ai|AI Incident Response]] â€” trigger alerts and workflows  
- [[governance_index|Governance Index]] â€” ensure accountability and compliance  

### Common Tools:
- Prometheus / Grafana (metrics dashboards)  
- Evidently / WhyLabs (ML drift detection)  
- Seldon Core / Arize / Fiddler (AI observability platforms)  
- OpenTelemetry for unified tracing  

**HU:**  
A monitorozÃ¡s integrÃ¡lÃ³dik:  
- [[observability_and_monitoring|Observability rendszerrel]] â€” kÃ¶zponti metrikÃ¡k Ã©s trace-ek  
- [[consistency_audit|Konzisztencia-audit]] â€” viselkedÃ©s-stabilitÃ¡s ellenÅ‘rzÃ©se verziÃ³k kÃ¶zÃ¶tt  
- [[incident_response_for_ai|MI incidenskezelÃ©s]] â€” riasztÃ¡sok Ã©s automatizÃ¡lt folyamatok indÃ­tÃ¡sa  
- [[governance_index|IrÃ¡nyÃ­tÃ¡si index]] â€” elszÃ¡moltathatÃ³sÃ¡g Ã©s megfelelÅ‘sÃ©g biztosÃ­tÃ¡sa  

### Gyakori eszkÃ¶zÃ¶k:
- Prometheus / Grafana (metrika dashboardok)  
- Evidently / WhyLabs (ML drift detektÃ¡lÃ¡s)  
- Seldon Core / Arize / Fiddler (AI-observability platformok)  
- OpenTelemetry a kÃ¶zÃ¶s nyomkÃ¶vetÃ©shez  

---

## ğŸ”— Connection to Lifecycle / Kapcsolat az Ã©letciklussal

**EN:**  
Model monitoring is part of the **secure MLOps loop**:  

**Train â†’ Validate â†’ Deploy â†’ Monitor â†’ Retrain â†’ Audit**

When integrated properly, it closes the feedback loop â€” turning each anomaly into learning for future robustness.  

**HU:**  
A modell-monitorozÃ¡s a **biztonsÃ¡gos MLOps-ciklus** rÃ©sze:  

**TanÃ­tÃ¡s â†’ ValidÃ¡lÃ¡s â†’ BevezetÃ©s â†’ MonitorozÃ¡s â†’ ÃšjratanÃ­tÃ¡s â†’ Audit**

MegfelelÅ‘ integrÃ¡ciÃ³ esetÃ©n ez bezÃ¡rja a visszacsatolÃ¡si hurkot â€” minden anomÃ¡lia a jÃ¶vÅ‘beli robusztussÃ¡g tanulsÃ¡gÃ¡vÃ¡ vÃ¡lik.  

---

## âš–ï¸ Trade-offs / Kompromisszumok

**EN:**  
- Too much logging â†’ privacy and cost concerns  
- Too little monitoring â†’ silent degradation or missed attacks  
- Continuous retraining can amplify data drift if not verified  
- Must balance privacy (e.g., [[differential_privacy|Differential Privacy]]) with auditability  

**HU:**  
- TÃºl sok naplÃ³zÃ¡s â†’ adatvÃ©delmi Ã©s kÃ¶ltsÃ©gbeli aggÃ¡lyok  
- TÃºl kevÃ©s monitorozÃ¡s â†’ csendes teljesÃ­tmÃ©nyromlÃ¡s vagy tÃ¡madÃ¡sok Ã©szrevÃ©tlenek maradnak  
- Az automatikus ÃºjratanÃ­tÃ¡s driftet is felerÅ‘sÃ­thet, ha nincs ellenÅ‘rizve  
- Az adatvÃ©delem (pl. [[differential_privacy|DifferenciÃ¡lis adatvÃ©delem]]) Ã©s az auditÃ¡lhatÃ³sÃ¡g kÃ¶zÃ¶tt egyensÃºlyra van szÃ¼ksÃ©g  

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. How does model monitoring differ from observability in AI security?  
2. Explain the difference between data drift and concept drift with examples.  
3. What indicators suggest a possible adversarial or poisoning attack?  
4. How would you design a monitoring pipeline for an LLM API with RAG integration?  
5. What trade-offs arise between monitoring depth and data privacy?  

---

> â€œA model thatâ€™s not monitored is a model thatâ€™s already drifting.â€ ğŸ“‰
