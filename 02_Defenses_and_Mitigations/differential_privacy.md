---
version: "3.2"
section_type: "defense"
agent: "Principle Engineer"
---
# ğŸ§® Differential Privacy

---

## ğŸŒ What Is Differential Privacy? / Mi az a differenciÃ¡lis adatvÃ©delem?

**EN:**  
**Differential Privacy (DP)** is a mathematical framework ensuring that the output of a computation does not reveal whether any individualâ€™s data was included in the input. In simpler terms, it guarantees that *each personâ€™s presence or absence has a minimal, bounded effect* on the modelâ€™s predictions or statistics.  

In AI and machine learning, differential privacy protects sensitive training data â€” such as personal, medical, or financial records â€” from being reverse-engineered or inferred through model outputs or gradients. ğŸ›¡ï¸ğŸ¤–  

**HU:**  
A **DifferenciÃ¡lis adatvÃ©delem (DP)** egy matematikai keretrendszer, amely biztosÃ­tja, hogy egy szÃ¡mÃ­tÃ¡s eredmÃ©nye ne Ã¡rulja el, hogy bÃ¡rmely egyÃ©n adata szerepelt-e a bemenetben. EgyszerÅ±bben: garantÃ¡lja, hogy *egy szemÃ©ly jelenlÃ©te vagy hiÃ¡nya csak korlÃ¡tozott mÃ©rtÃ©kben befolyÃ¡solhatja* a modell kimenetÃ©t vagy statisztikÃ¡it.  

Az MI-ben a differenciÃ¡lis adatvÃ©delem vÃ©di az Ã©rzÃ©keny tanÃ­tÃ³adatokat â€” pÃ©ldÃ¡ul szemÃ©lyes, orvosi vagy pÃ©nzÃ¼gyi adatokat â€” attÃ³l, hogy a modellbÅ‘l visszafejthetÅ‘k legyenek. ğŸ›¡ï¸ğŸ¤–

---

## ğŸ’¡ Formal Definition / FormÃ¡lis definÃ­ciÃ³

**EN:**  
A randomized algorithm \( M \) provides **\( (\epsilon, \delta) \)-differential privacy** if, for all pairs of datasets \( D_1, D_2 \) that differ by at most one record, and for all possible outputs \( S \):

$$
P[M(D_1) \in S] \le e^{\epsilon} \cdot P[M(D_2) \in S] + \delta
$$

- \( \epsilon \) (epsilon): *privacy budget*, bounds how much output probabilities can differ. Smaller means stronger privacy.  
- \( \delta \): small slack term allowing rare violations.  
- \( M \): the mechanism (e.g., query, model, gradient update).  

This ensures an observer cannot confidently infer whether any single record was part of the dataset.

**HU:**  
Egy randomizÃ¡lt algoritmus \( M \) **\( (\epsilon, \delta) \)-differenciÃ¡lis adatvÃ©delmet** biztosÃ­t, ha minden olyan \( D_1, D_2 \) adathalmazra, amelyek legfeljebb egy rekordban kÃ¼lÃ¶nbÃ¶znek, Ã©s minden lehetsÃ©ges kimenetre \( S \) igaz:

$$
P[M(D_1) \in S] \le e^{\epsilon} \cdot P[M(D_2) \in S] + \delta
$$

- \( \epsilon \) (epszilon): az *adatvÃ©delmi kÃ¶ltsÃ©gvetÃ©s*, amely korlÃ¡tozza, mennyire tÃ©rhetnek el a kimeneti valÃ³szÃ­nÅ±sÃ©gek. MinÃ©l kisebb, annÃ¡l erÅ‘sebb a vÃ©delem.  
- \( \delta \): kis tÅ±rÃ©si tag, amely ritka megsÃ©rtÃ©seket enged meg.  
- \( M \): a mechanizmus (pl. lekÃ©rdezÃ©s, modell, gradiens-frissÃ­tÃ©s).  

Ez biztosÃ­tja, hogy a megfigyelÅ‘ ne tudja megbÃ­zhatÃ³an megÃ¡llapÃ­tani, szerepelt-e egy adott rekord az adathalmazban.

---

## ğŸ§© Mechanisms That Achieve Differential Privacy / DP-t biztosÃ­tÃ³ mechanizmusok

**EN:**  
1. **Laplace Mechanism:** adds Laplacian noise proportional to query sensitivity.  
2. **Gaussian Mechanism:** adds Gaussian noise â€” used for continuous data.  
3. **Exponential Mechanism:** selects outputs probabilistically based on utility scores.  
4. **Privacy Amplification by Subsampling:** training on random subsets naturally improves privacy guarantees.  
5. **DP-SGD (Differentially Private Stochastic Gradient Descent):** cornerstone of private deep learning; clips gradients and adds Gaussian noise at each step:

$$
g_i' = \frac{g_i}{\max(1, \frac{\|g_i\|_2}{C})}, \quad \bar{g} = \frac{1}{N}\sum_i g_i' + \mathcal{N}(0, \sigma^2 C^2 I)
$$

Here \(C\) is the clipping norm and \(\sigma\) controls noise magnitude.  

**HU:**  
1. **Laplace-mechanizmus:** Laplace-eloszlÃ¡sÃº zajt ad az Ã©rzÃ©kenysÃ©ggel arÃ¡nyosan.  
2. **Gauss-mechanizmus:** Gauss-zajt ad hozzÃ¡ â€” folytonos adatok esetÃ©n.  
3. **ExponenciÃ¡lis mechanizmus:** az eredmÃ©nyt hasznossÃ¡gi pontszÃ¡m alapjÃ¡n valÃ³szÃ­nÅ±sÃ©gi alapon vÃ¡lasztja.  
4. **MintavÃ©telezÃ©s Ã¡ltali adatvÃ©delmi erÅ‘sÃ­tÃ©s:** vÃ©letlen rÃ©szhalmazon valÃ³ tanÃ­tÃ¡s termÃ©szetesen erÅ‘sÃ­ti a vÃ©delmet.  
5. **DP-SGD (DifferenciÃ¡lisan vÃ©dett sztochasztikus gradiens-descent):** a privÃ¡t mÃ©lytanulÃ¡s alapja; gradiens-klippelÃ©s Ã©s Gauss-zaj hozzÃ¡adÃ¡sa minden lÃ©pÃ©sben:

$$
g_i' = \frac{g_i}{\max(1, \frac{\|g_i\|_2}{C})}, \quad \bar{g} = \frac{1}{N}\sum_i g_i' + \mathcal{N}(0, \sigma^2 C^2 I)
$$

Itt \(C\) a vÃ¡gÃ¡si norma, \(\sigma\) pedig a zaj mÃ©rtÃ©kÃ©t szabÃ¡lyozza.

---

## âš™ï¸ Differential Privacy in Machine Learning / DP az MI-ben

**EN:**  
Differential privacy is used during model training or data sharing to ensure that even with full model access, attackers cannot reconstruct or infer specific training examples (see [[membership_inference|Membership Inference]] and [[model_inversion|Model Inversion]]).  

### Common use cases:
- **DP-SGD in neural networks:** ensures model gradients do not leak individual samples.  
- **Private federated learning:** each clientâ€™s update is noised before aggregation.  
- **Private query answering:** aggregating statistics with bounded disclosure risk.  
- **Synthetic data generation:** producing data that mimics real patterns without exposing individuals.

**HU:**  
A differenciÃ¡lis adatvÃ©delem modell-tanÃ­tÃ¡s vagy adatmegosztÃ¡s sorÃ¡n biztosÃ­tja, hogy mÃ©g teljes modell-hozzÃ¡fÃ©rÃ©s esetÃ©n se lehessen visszafejteni vagy kikÃ¶vetkeztetni egyes tanÃ­tÃ³pÃ©ldÃ¡kat (lÃ¡sd [[membership_inference|TagsÃ¡gi kÃ¶vetkeztetÃ©s]] Ã©s [[model_inversion|Modell-inverziÃ³]]).  

### Tipikus alkalmazÃ¡sok:
- **DP-SGD neurÃ¡lis hÃ¡lÃ³kban:** megakadÃ¡lyozza, hogy a gradiens-szivÃ¡rgÃ¡s felfedje az egyÃ©ni mintÃ¡kat.  
- **PrivÃ¡t federÃ¡lt tanulÃ¡s:** minden kliens frissÃ­tÃ©se zajjal vÃ©dve kerÃ¼l aggregÃ¡lÃ¡sra.  
- **PrivÃ¡t lekÃ©rdezÃ©sek:** aggregÃ¡lt statisztikÃ¡k kiszivÃ¡rgÃ¡si kockÃ¡zatÃ¡nak korlÃ¡tozÃ¡sa.  
- **SzimulÃ¡lt adatkÃ©pzÃ©s:** mestersÃ©ges adatok generÃ¡lÃ¡sa valÃ³s mintÃ¡zatok alapjÃ¡n, anÃ©lkÃ¼l hogy egyÃ©nek adatait felfednÃ©.

---

## ğŸ§  Privacyâ€“Utility Trade-off / AdatvÃ©delem Ã©s hasznossÃ¡g egyensÃºlya

**EN:**  
Adding noise improves privacy but reduces model accuracy. The balance depends on:  
- \( \epsilon \): smaller â†’ more noise â†’ stronger privacy, lower utility  
- dataset size: larger datasets tolerate more noise  
- task sensitivity: high-stakes models (medical, legal) require tighter privacy budgets  

Design goal: **minimize privacy loss for acceptable accuracy**, often requiring empirical tuning and [[governance_index|governance]] oversight.

**HU:**  
A zaj hozzÃ¡adÃ¡sa nÃ¶veli az adatvÃ©delmet, de csÃ¶kkenti a modell pontossÃ¡gÃ¡t. Az egyensÃºlyt befolyÃ¡solja:  
- \( \epsilon \): kisebb â†’ tÃ¶bb zaj â†’ erÅ‘sebb vÃ©delem, gyengÃ©bb hasznossÃ¡g  
- adathalmaz mÃ©rete: nagyobb halmaz tÃ¶bb zajt is elvisel  
- feladat Ã©rzÃ©kenysÃ©ge: kritikus modellek (pl. orvosi, jogi) szigorÃºbb kÃ¶ltsÃ©gvetÃ©st igÃ©nyelnek  

A cÃ©l: **az adatvÃ©delmi vesztesÃ©g minimalizÃ¡lÃ¡sa elfogadhatÃ³ pontossÃ¡g mellett**, amit gyakorlati hangolÃ¡s Ã©s [[governance_index|irÃ¡nyÃ­tÃ¡si felÃ¼gyelet]] kÃ­sÃ©r.

---

## ğŸ§° Implementation Best Practices / MegvalÃ³sÃ­tÃ¡si irÃ¡nyelvek

**EN:**  
- Use **privacy accounting** (e.g., Moments Accountant, RÃ©nyi DP) to track cumulative privacy loss.  
- Combine with [[secure_aggregation|Secure Aggregation]] or [[federated_learning_security|Federated Learning Security]].  
- Never rely solely on anonymization â€” differential privacy provides *provable guarantees*, anonymization does not.  
- Validate DP parameters (\(\epsilon, \delta\)) with domain-specific compliance standards (e.g., GDPR).  
- Apply **post-training audits** to ensure outputs meet expected privacy thresholds.

**HU:**  
- HasznÃ¡lj **adatvÃ©delmi kÃ¶nyvelÃ©st** (pl. Moments Accountant, RÃ©nyi DP) az Ã¶sszesÃ­tett adatvÃ©delmi vesztesÃ©g kÃ¶vetÃ©sÃ©re.  
- KombinÃ¡ld [[secure_aggregation|BiztonsÃ¡gos aggregÃ¡ciÃ³val]] vagy [[federated_learning_security|FederÃ¡lt tanulÃ¡s vÃ©delemmel]].  
- Soha ne tÃ¡maszkodj kizÃ¡rÃ³lag anonimizÃ¡lÃ¡sra â€” a differenciÃ¡lis adatvÃ©delem *bizonyÃ­thatÃ³ garanciÃ¡kat* ad, mÃ­g az anonimizÃ¡lÃ¡s nem.  
- A DP-paramÃ©tereket (\(\epsilon, \delta\)) a domÃ©nhez illesztett megfelelÅ‘sÃ©gi szabvÃ¡nyokkal (pl. GDPR) kell validÃ¡lni.  
- Alkalmazz **tanÃ­tÃ¡s utÃ¡ni auditot**, hogy ellenÅ‘rizd: a kimenetek megfelelnek-e a vÃ¡rt adatvÃ©delmi szintnek.

---

## ğŸ”— Related Topics / KapcsolÃ³dÃ³ Fejezetek

**EN:**  
See [[membership_inference|Membership Inference]], [[model_inversion|Model Inversion]], [[federated_learning_security|Federated Learning Security]], [[data_poisoning|Data Poisoning]], and [[privacy_preserving_ml|Privacy-Preserving ML]].

**HU:**  
LÃ¡sd mÃ©g: [[membership_inference|TagsÃ¡gi kÃ¶vetkeztetÃ©s]], [[model_inversion|Modell-inverziÃ³]], [[federated_learning_security|FederÃ¡lt tanulÃ¡s vÃ©delme]], [[data_poisoning|AdatmÃ©rgezÃ©s]], Ã©s [[privacy_preserving_ml|AdatvÃ©delmet megÅ‘rzÅ‘ gÃ©pi tanulÃ¡s]].

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. Explain the meaning of \( \epsilon \) and \( \delta \) in the differential privacy definition.  
2. Derive how DP-SGD balances gradient clipping and noise addition.  
3. How does differential privacy protect against membership inference and model inversion attacks?  
4. Describe how privacy amplification by subsampling strengthens privacy guarantees.  
5. Design a privacy accounting approach for a DP-SGD training process over 50 epochs.

---

> â€œPerfect privacy does not exist â€” but differential privacy shows us how to measure what we lose.â€ ğŸ”
