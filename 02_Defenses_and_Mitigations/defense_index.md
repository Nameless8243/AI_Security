---
version: "3.2"
section_type: "defense_index"
agent: "Index Architect"
---
# ğŸ›¡ï¸ Defense Index

---

## ğŸŒ Purpose / CÃ©l

**EN:**  
The **Defense Index** serves as a structured navigation map across all defensive mechanisms in the Vault â€” from data-level hardening to model, pipeline, and system-level protections. It connects each family of defense strategies and highlights where they fit in the broader [[ai_security_lifecycle|AI Security Lifecycle]].  

This index is designed as a conceptual *threat-to-defense bridge*: every major attack vector (e.g., [[data_poisoning|Data Poisoning]], [[adversarial_example_attacks|Adversarial Examples]], [[model_stealing_and_extraction|Model Extraction]], [[prompt_injection_and_rag_attacks|Prompt Injection]], etc.) has one or more mapped countermeasures here.  

**HU:**  
A **Defense Index** az Ã¶sszes vÃ©delmi mechanizmus strukturÃ¡lt tÃ©rkÃ©pe a Vaultban â€” az adat-szintÅ± vÃ©delmektÅ‘l kezdve a modell-, pipeline- Ã©s rendszer-szintÅ± megoldÃ¡sokig. Ã–sszekapcsolja a vÃ©delmi stratÃ©giÃ¡k csalÃ¡djait, Ã©s megmutatja, hol illeszkednek a [[ai_security_lifecycle|MI-biztonsÃ¡gi Ã©letciklus]] tÃ¡gabb keretÃ©be.  

Ez az index egyfajta *tÃ¡madÃ¡sâ€“vÃ©dekezÃ©s hÃ­d*: minden fÅ‘ tÃ¡madÃ¡si tÃ­pushoz (pl. [[data_poisoning|AdatmÃ©rgezÃ©s]], [[adversarial_example_attacks|AdverszÃ¡riÃ¡lis pÃ©ldÃ¡k]], [[model_stealing_and_extraction|Modell-lopÃ¡s]], [[prompt_injection_and_rag_attacks|Prompt Injection]] stb.) megtalÃ¡lhatÃ³k itt a megfelelÅ‘ ellentechnikÃ¡k.  

---

## ğŸ§± Layered Defense Architecture / RÃ©tegzett vÃ©dekezÃ©si architektÃºra

**EN:**  
AI security requires **defense-in-depth** â€” layered controls that protect across the entire lifecycle:

### 1ï¸âƒ£ Data & Input Layer
- **[[data_sanitization|Data Sanitization]]** â€” removing or validating untrusted inputs  
- **[[data_provenance|Data Provenance]]** â€” tracking dataset origin & integrity  
- **[[data_poisoning_detection|Data Poisoning Detection]]** â€” anomaly and label consistency audits  
- **[[input_filtering|Input Filtering]]** â€” content moderation, schema validation, and injection prevention  
- **[[differential_privacy|Differential Privacy]]** â€” privacy-preserving data processing  

### 2ï¸âƒ£ Model Layer
- **[[adversarial_training|Adversarial Training]]** â€” embedding adversarial examples during learning  
- **[[certified_robustness|Certified Robustness]]** â€” mathematically provable robustness bounds  
- **[[model_ensemble_defense|Model Ensembles]]** â€” diversity-based robustness  
- **[[regularization_and_smoothing|Regularization & Smoothing]]** â€” smoother decision boundaries  
- **[[model_monitoring|Model Monitoring]]** â€” drift and consistency checks post-deployment  

### 3ï¸âƒ£ Pipeline Layer
- **[[ai_supply_chain_security|AI Supply Chain Security]]** â€” securing dependencies, models, and datasets  
- **[[ci_cd_hardening|CI/CD Hardening]]** â€” secured build and deployment pipelines  
- **[[model_serving_security|Model Serving Security]]** â€” API protection, rate limiting, authentication  
- **[[sbom|Software Bill of Materials (SBOM)]]** â€” transparency of components  
- **[[logging_and_audit|Logging & Audit Trails]]** â€” forensic visibility and traceability  

### 4ï¸âƒ£ System & Retrieval Layer
- **[[prompt_injection_and_rag_attacks|Prompt Injection Defense]]** â€” context separation, retrieval validation  
- **[[retrieval_hardening|Retrieval Hardening]]** â€” allow-lists, content signing, provenance verification  
- **[[rag_sanitization|RAG Sanitization]]** â€” embedding validation and context role enforcement  
- **[[output_filtering|Output Filtering]]** â€” sensitive data redaction and hallucination prevention  
- **[[zero_trust|Zero Trust for AI Systems]]** â€” strict identity and access segmentation  

### 5ï¸âƒ£ Governance & Oversight Layer
- **[[ai_governance|AI Governance]]** â€” policies, standards, risk categorization  
- **[[consistency_audit|Consistency Auditing]]** â€” cross-version and behavior drift checks  
- **[[compliance_frameworks|Compliance Frameworks]]** â€” NIST AI RMF, ISO/IEC 42001 alignment  
- **[[red_team_testing|Adversarial Red Teaming]]** â€” simulated attacks to measure resilience  
- **[[incident_response_for_ai|AI Incident Response]]** â€” detection, containment, and recovery workflows  

**HU:**  
Az MI-biztonsÃ¡g **tÃ¶bbrÃ©tegÅ± vÃ©dekezÃ©st** igÃ©nyel â€” olyan kontrollokat, amelyek lefedik az egÃ©sz Ã©letciklust:

### 1ï¸âƒ£ Adat- Ã©s bemenet-szint
- **[[data_sanitization|AdattisztÃ­tÃ¡s]]** â€” nem megbÃ­zhatÃ³ adatok eltÃ¡volÃ­tÃ¡sa vagy validÃ¡lÃ¡sa  
- **[[data_provenance|Adat-eredet kÃ¶vetÃ©se]]** â€” az adathalmaz forrÃ¡sÃ¡nak Ã©s integritÃ¡sÃ¡nak nyomon kÃ¶vetÃ©se  
- **[[data_poisoning_detection|AdatmÃ©rgezÃ©s-felismerÃ©s]]** â€” anomÃ¡lia Ã©s cÃ­mkekonzisztencia ellenÅ‘rzÃ©s  
- **[[input_filtering|Bemenet-szÅ±rÃ©s]]** â€” tartalomszÅ±rÃ©s, sÃ©maellenÅ‘rzÃ©s, injekciÃ³-megelÅ‘zÃ©s  
- **[[differential_privacy|DifferenciÃ¡lis adatvÃ©delem]]** â€” adatvÃ©delmet biztosÃ­tÃ³ feldolgozÃ¡s  

### 2ï¸âƒ£ Modell-szint
- **[[adversarial_training|Adversarial Training]]** â€” adverszÃ¡riÃ¡lis pÃ©ldÃ¡kkal edzett robusztus modell  
- **[[certified_robustness|TanÃºsÃ­tott robusztussÃ¡g]]** â€” matematikailag bizonyÃ­tott robusztussÃ¡gi hatÃ¡rok  
- **[[model_ensemble_defense|Model-ensemble vÃ©delem]]** â€” diverzitÃ¡son alapulÃ³ robusztussÃ¡g  
- **[[regularization_and_smoothing|RegularizÃ¡ciÃ³ & simÃ­tÃ¡s]]** â€” simÃ¡bb dÃ¶ntÃ©si hatÃ¡rok kialakÃ­tÃ¡sa  
- **[[model_monitoring|Modell-monitorozÃ¡s]]** â€” drift- Ã©s konzisztencia-ellenÅ‘rzÃ©s bevezetÃ©s utÃ¡n  

### 3ï¸âƒ£ Pipeline-szint
- **[[ai_supply_chain_security|MI-ellÃ¡tÃ¡si lÃ¡nc biztonsÃ¡g]]** â€” fÃ¼ggÅ‘sÃ©gek, modellek Ã©s adatok vÃ©delme  
- **[[ci_cd_hardening|CI/CD megerÅ‘sÃ­tÃ©se]]** â€” biztonsÃ¡gos build- Ã©s bevezetÃ©si folyamatok  
- **[[model_serving_security|Modell-szolgÃ¡ltatÃ¡s biztonsÃ¡ga]]** â€” API-vÃ©delem, rate limiting, autentikÃ¡ciÃ³  
- **[[sbom|SBOM]]** â€” komponensek Ã¡tlÃ¡thatÃ³sÃ¡ga  
- **[[logging_and_audit|NaplÃ³zÃ¡s Ã©s auditÃ¡lÃ¡s]]** â€” forenzikus nyomkÃ¶vethetÅ‘sÃ©g biztosÃ­tÃ¡sa  

### 4ï¸âƒ£ Rendszer- Ã©s visszakeresÃ©si szint
- **[[prompt_injection_and_rag_attacks|Prompt Injection vÃ©delem]]** â€” kontextus-elvÃ¡lasztÃ¡s, visszakeresÃ©s validÃ¡lÃ¡s  
- **[[retrieval_hardening|VisszakeresÃ©s megerÅ‘sÃ­tÃ©se]]** â€” allow-listÃ¡k, alÃ¡Ã­rÃ¡s, eredetellenÅ‘rzÃ©s  
- **[[rag_sanitization|RAG sanitizÃ¡lÃ¡s]]** â€” embedding-validÃ¡lÃ¡s, szerepkÃ¶rÃ¶k szÃ©tvÃ¡lasztÃ¡sa  
- **[[output_filtering|Kimenet-szÅ±rÃ©s]]** â€” Ã©rzÃ©keny adatok takarÃ¡sa, hallucinÃ¡ciÃ³-megelÅ‘zÃ©s  
- **[[zero_trust|Zero Trust az MI rendszerekben]]** â€” identitÃ¡salapÃº hozzÃ¡fÃ©rÃ©s-szegmentÃ¡lÃ¡s  

### 5ï¸âƒ£ IrÃ¡nyÃ­tÃ¡s Ã©s felÃ¼gyelet
- **[[ai_governance|MI-irÃ¡nyÃ­tÃ¡s]]** â€” szabÃ¡lyok, standardok, kockÃ¡zati kategorizÃ¡lÃ¡s  
- **[[consistency_audit|Konzisztencia-auditÃ¡lÃ¡s]]** â€” verziÃ³kÃ¶zi Ã©s viselkedÃ©si eltÃ©rÃ©sek ellenÅ‘rzÃ©se  
- **[[compliance_frameworks|MegfelelÅ‘sÃ©gi keretrendszerek]]** â€” NIST AI RMF, ISO/IEC 42001 illesztÃ©s  
- **[[red_team_testing|AdverszÃ¡riÃ¡lis Red Teaming]]** â€” tÃ¡madÃ¡sok szimulÃ¡lÃ¡sa a vÃ©delem tesztelÃ©sÃ©re  
- **[[incident_response_for_ai|MI incidenskezelÃ©s]]** â€” Ã©szlelÃ©s, izolÃ¡lÃ¡s, helyreÃ¡llÃ­tÃ¡s  

---

## ğŸ§  Core Defensive Principles / Alapelvi vÃ©delmi irÃ¡nyok

**EN:**  
- **Defense-in-depth:** multiple, independent layers reduce single-point failure risk.  
- **Least privilege:** minimal access across data, models, APIs, and agents.  
- **Continuous validation:** reverify integrity at every lifecycle stage.  
- **Explainability as defense:** interpretability tools detect malicious drift and anomalous reasoning.  
- **Human-in-the-loop:** critical actions and overrides require human verification.  
- **Zero Trust mindset:** assume every component and external input can be hostile.

**HU:**  
- **TÃ¶bbrÃ©tegÅ± vÃ©delem:** tÃ¶bb, egymÃ¡stÃ³l fÃ¼ggetlen rÃ©teg csÃ¶kkenti az egyetlen hiba kockÃ¡zatÃ¡t.  
- **Legkisebb jogosultsÃ¡g elve:** minimÃ¡lis hozzÃ¡fÃ©rÃ©s az adatokhoz, modellekhez, API-khoz Ã©s agentekhez.  
- **Folyamatos validÃ¡lÃ¡s:** minden Ã©letciklus-fÃ¡zisban ellenÅ‘rizni kell az integritÃ¡st.  
- **MagyarÃ¡zhatÃ³sÃ¡g mint vÃ©delem:** az Ã©rtelmezhetÅ‘sÃ©g segÃ­ti a rosszindulatÃº drift vagy anomÃ¡lia felismerÃ©sÃ©t.  
- **Ember a folyamatban:** a kritikus dÃ¶ntÃ©sekhez emberi megerÅ‘sÃ­tÃ©s szÃ¼ksÃ©ges.  
- **Zero Trust szemlÃ©let:** minden komponens Ã©s bemenet potenciÃ¡lisan ellensÃ©ges.

---

## ğŸ”— Cross-References / Kapcsolatok

**EN:**  
This index crosslinks with:  
- [[attack_index|Attack Index]] for corresponding offensive tactics  
- [[ai_security_lifecycle|AI Security Lifecycle]] for stage-wise defense mapping  
- [[governance_index|Governance Index]] for oversight frameworks  

**HU:**  
Ez az index Ã¶sszekapcsolÃ³dik a kÃ¶vetkezÅ‘kkel:  
- [[attack_index|Attack Index]] â€” a tÃ¡madÃ¡si technikÃ¡k pÃ¡rjai  
- [[ai_security_lifecycle|MI-biztonsÃ¡gi Ã©letciklus]] â€” vÃ©dekezÃ©si tÃ©rkÃ©p az Ã©letciklus mentÃ©n  
- [[governance_index|Governance Index]] â€” az irÃ¡nyÃ­tÃ¡si Ã©s megfelelÅ‘sÃ©gi keretrendszerek  

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. How do the five defense layers interact to form defense-in-depth in AI systems?  
2. Which defenses mitigate poisoning and inference-time attacks respectively?  
3. How can governance frameworks strengthen technical defenses through policy and oversight?  
4. Why is Zero Trust critical in AI supply chains and RAG systems?  
5. Design a minimal defense stack for a small-scale AI API that still achieves layered security.

---

> â€œEvery AI defense is a layer of trust â€” fragile alone, resilient together.â€ âš–ï¸
