---
version: "3.3"
section_type: "attack"
agent: "Principle Engineer"
---
# ğŸ•µï¸ Model Stealing & Extraction

---

## ğŸŒ What is Model Stealing? / Mi az a model stealing?

**EN:**  
Model stealing (aka model extraction) is the act of reconstructing or approximating a target machine learning model \(f\) by an adversary who only has *query access* (or limited visibility) to it â€” typically via an API or deployed service. The attacker aims to produce \( \hat{f} \) that matches \(f\)â€™s functionality, performance, or confidential attributes (architecture, weights, training data characteristics). This is a direct intellectual-property and operational risk for ML providers and a precursor to many follow-on attacks (fingerprinting, adversarial transfer, privacy attacks). ğŸ¤–ğŸ›¡ï¸

**HU:**  
A model stealing (vagy model-extraction) sorÃ¡n a tÃ¡madÃ³ rekonstruÃ¡lja vagy megkÃ¶zelÃ­ti a cÃ©lmodell \(f\) mÅ±kÃ¶dÃ©sÃ©t Ãºgy, hogy csak lekÃ©rdezÃ©si hozzÃ¡fÃ©rÃ©se van (pÃ©ldÃ¡ul API-n keresztÃ¼l). A cÃ©l egy olyan \( \hat{f} \) elÅ‘Ã¡llÃ­tÃ¡sa, amely hasonlÃ³ funkcionalitÃ¡st, teljesÃ­tmÃ©nyt vagy bizalmas jellemzÅ‘ket (architektÃºra, sÃºlyok, a tanÃ­tÃ³adatokra jellemzÅ‘ mintÃ¡zatok) utÃ¡noz. Ez szellemi tulajdon- Ã©s mÅ±kÃ¶dÃ©si kockÃ¡zatot jelent az ML-szolgÃ¡ltatÃ³k szÃ¡mÃ¡ra, Ã©s gyakran elÅ‘jÃ¡tÃ©ka tovÃ¡bbi tÃ¡madÃ¡soknak (pl. fingerprinting, adverszÃ¡riÃ¡lis Ã¡tvitel, adatvÃ©delmi tÃ¡madÃ¡sok). ğŸ¤–ğŸ›¡ï¸

---

## ğŸ’¡ Why it matters / MiÃ©rt fontos ez a problÃ©ma

**EN:**  
If an attacker obtains \( \hat{f} \) they can: (1) avoid paying for the service, (2) find adversarial inputs that transfer to the original model, (3) infer training data properties (link to [[membership_inference|Membership Inference]] / [[model_inversion|Model Inversion]]), or (4) replicate a proprietary model for resale. From a defenderâ€™s viewpoint, extraction breaks confidentiality guarantees of the model-as-a-service business model and widens the threat surface for downstream attacks. ğŸ’¸ğŸ”“

**HU:**  
Ha a tÃ¡madÃ³ megszerzi \( \hat{f} \)-et, akkor: (1) elkerÃ¼lheti a szolgÃ¡ltatÃ¡s dÃ­jÃ¡t, (2) talÃ¡lhat olyan adverszÃ¡riÃ¡lis bemeneteket, amelyek Ã¡tjÃ¡rnak az eredeti modellen, (3) kÃ¶vetkeztethet a tanÃ­tÃ³adatokra (lÃ¡sd [[membership_inference|TagsÃ¡gi kÃ¶vetkeztetÃ©s]] / [[model_inversion|Modell-inverziÃ³]]), vagy (4) lemÃ¡solhatja Ã©s eladhatja a szellemi tulajdont. A vÃ©dekezÃ©s szempontjÃ¡bÃ³l ez alÃ¡Ã¡ssa a model-as-a-service titoktartÃ¡sÃ¡t Ã©s nÃ¶veli a tovÃ¡bbi tÃ¡madÃ¡sok felÃ¼letÃ©t. ğŸ’¸ğŸ”“

---

## ğŸ§© Attack taxonomies / A tÃ¡madÃ¡sok tÃ­pusai

**EN:**  
Extraction attacks vary by goal and attacker resources. Common classes include:  
- **Functional extraction:** approximate the modelâ€™s inputâ†’output mapping (black-box mimic).  
- **Parameter extraction:** recover model parameters or architecture (stronger, often requires more queries).  
- **Decision boundary extraction:** learn the classifier boundary precisely to craft high-success adversarial examples.  
- **Meta-extraction:** infer training-set statistics, hyperparameters, label smoothing, or proprietary pre/post-processing steps.

**HU:**  
A kinyerÃ©si tÃ¡madÃ¡sok cÃ©l Ã©s erÅ‘forrÃ¡sok szerint kÃ¼lÃ¶nbÃ¶znek. Gyakoribb tÃ­pusok:  
- **FunkcionÃ¡lis kinyerÃ©s:** a modell inputâ†’output lekÃ©pzÃ©sÃ©nek utÃ¡nozÃ¡sa (black-box).  
- **ParamÃ©ter-kinyerÃ©s:** modellparamÃ©terek vagy architektÃºra visszafejtÃ©se (erÅ‘forrÃ¡sigÃ©nyesebb).  
- **DÃ¶ntÃ©si hatÃ¡r kinyerÃ©s:** a klaszterhatÃ¡r pontos megtanulÃ¡sa, hogy hatÃ©kony adverszÃ¡riÃ¡lis pÃ©ldÃ¡kat kÃ©szÃ­tsenek.  
- **Meta-kinyerÃ©s:** tanÃ­tÃ³adat-statisztikÃ¡k, hiperparamÃ©terek vagy privÃ¡t elÅ‘-/utÃ³feldolgozÃ¡si lÃ©pÃ©sek feltÃ¡rÃ¡sa.

---

## ğŸ”¬ Formal objective (query-based view) / FormÃ¡lis cÃ©l (lekÃ©rdezÃ©salapÃº nÃ©zet)

**EN:**  
An adversary issues queries \( \{x^{(i)}\}_{i=1}^m \) and observes outputs \( \{y^{(i)} = f(x^{(i)})\} \) (scores, probabilities, or labels). The extraction problem can be framed as:

$$
\min_{\hat{f} \in \mathcal{F}} \; \mathbb{E}_{x \sim \mathcal{D}_q} \left[ \mathcal{L}\big( f(x), \hat{f}(x) \big) \right]
$$

subject to a budget constraint \( m \le M \) (number of queries) and possibly noise/rounding applied by the oracle. The choice of query distribution \( \mathcal{D}_q \) (random, adaptive, or membership-focused) and whether outputs are probabilities or just top-1 labels dramatically affects success.

**HU:**  
A tÃ¡madÃ³ lekÃ©rdezÃ©seket \( \{x^{(i)}\}_{i=1}^m \) kÃ¼ld a szolgÃ¡ltatÃ¡snak, Ã©s megfigyeli a vÃ¡laszokat \( \{y^{(i)} = f(x^{(i)})\} \) (pontszÃ¡mok, valÃ³szÃ­nÅ±sÃ©gek vagy cÃ­mkÃ©k). A kinyerÃ©si problÃ©mÃ¡t Ã­gy lehet megfogalmazni:

$$
\min_{\hat{f} \in \mathcal{F}} \; \mathbb{E}_{x \sim \mathcal{D}_q} \left[ \mathcal{L}\big( f(x), \hat{f}(x) \big) \right]
$$

korlÃ¡tozva a lekÃ©rdezÃ©si bÃ¼dzsÃ©t \( m \le M \) Ã©s figyelembe vÃ©ve a szolgÃ¡ltatÃ³ Ã¡ltal alkalmazott zajt vagy kerekÃ­tÃ©st. A lekÃ©rdezÃ©si eloszlÃ¡s \( \mathcal{D}_q \) (vÃ©letlenszerÅ±, adaptÃ­v vagy tagsÃ¡g-fÃ³kuszÃº) Ã©s az, hogy valÃ³szÃ­nÅ±sÃ©geket vagy csak top-1 vÃ¡laszt ad a szolgÃ¡ltatÃ³, jelentÅ‘sen befolyÃ¡solja a siker esÃ©lyÃ©t.

---

## ğŸ” Query strategies & transfer / LekÃ©rdezÃ©si stratÃ©giÃ¡k Ã©s Ã¡ttÃ©tel

**EN:**  
Attackers use several query strategies: random sampling over input space, adaptive active learning (query points chosen to reduce model uncertainty), and task-specific queries (e.g., using domain data). When attackers have surrogate models, they can combine transfer learning with distillation: train \( \hat{f} \) on queried pairs, optionally using data augmentation and label smoothing to better mimic \(f\). Probabilistic outputs (soft labels) accelerate extraction; integer/label-only outputs make it harder but not impossible. Transferability allows attackers to craft adversarial inputs on \( \hat{f} \) that often transfer to \(f\). See connections to [[transferability|Transferability]] and [[adversarial_example_attacks|Adversarial Examples]].

**HU:**  
A tÃ¡madÃ³k tÃ¶bb lekÃ©rdezÃ©si stratÃ©giÃ¡t hasznÃ¡lnak: vÃ©letlenszerÅ± mintavÃ©tel a bemeneti tÃ©rben, adaptÃ­v aktÃ­v tanulÃ¡s (lekÃ©rdezÃ©sek bizonytalansÃ¡g csÃ¶kkentÃ©sÃ©re), Ã©s domÃ©n-specifikus lekÃ©rdezÃ©sek. SurrogÃ¡t modellek alkalmazÃ¡sakor az Ã¡ttÃ©rÃ©s (transfer) Ã©s distillÃ¡ciÃ³ kombinÃ¡lhatÃ³: \( \hat{f} \)-et a lekÃ©rdezett pÃ¡rokon tanÃ­tjÃ¡k, adatkiterjesztÃ©ssel Ã©s cÃ­mke-simÃ­tÃ¡ssal, hogy hÅ±bben kÃ¶vesse \(f\)-et. A valÃ³szÃ­nÅ±sÃ©gi (soft) kimenetek gyorsÃ­tjÃ¡k a kinyerÃ©st; a csak cÃ­mke vÃ¡laszok nehezÃ­tik, de nem teszik lehetetlennÃ©. Az Ã¡ttÃ©tel lehetÅ‘vÃ© teszi, hogy a \( \hat{f} \)-en kÃ©szÃ­tett adverszÃ¡riÃ¡lis pÃ©ldÃ¡k gyakran mÅ±kÃ¶djenek az eredeti \(f\)-en is. LÃ¡sd [[transferability|Ãtviteli jelensÃ©g]] Ã©s [[adversarial_example_attacks|AdverszÃ¡riÃ¡lis pÃ©ldÃ¡k]].

---

## ğŸ›¡ï¸ Defenses & mitigations / VÃ©dekezÃ©sek Ã©s mÃ©rsÃ©klÃ©sek

**EN:**  
Defending against model extraction requires layered, operational and algorithmic controls:

- **API & Rate Controls:** strict rate limits, per-user quotas, anomaly-based throttling to limit query budgets.  
- **Output Hardening:** reduce information leakage by returning top-k labels only, rounding probabilities, or adding calibrated noise (be careful â€” naive noise can be circumvented).  
- **Response Randomization & Watermarking:** embed provider-only watermarks in outputs or apply randomized response that degrades extraction quality while preserving utility. See [[watermarking|Watermarking]] for model ownership techniques.  
- **Model Distillation & Ensemble Guarding:** serve a distilled, less-informative model publicly while keeping a high-fidelity version private; use ensembles with randomized selector logic.  
- **Detection & Forensics:** continuous [[query_monitoring|query monitoring]] and anomaly detection to detect suspicious querying patterns; keep audit trails for legal action.  
- **Legal & Economic Controls:** API terms, usage billing, and legal recourse deter misuse.  
- **Technical: Differential Privacy & Output Perturbation:** training with [[differential_privacy|differential privacy]] reduces overfitting to unique training samples and can limit meta-extraction, but it does not fully prevent functional extraction.

No single control is sufficient â€” combine rate-limiting, monitoring, output controls, and legal measures. Evaluate trade-offs: too aggressive hardening degrades user experience and model utility.

**HU:**  
A vÃ©delmet rÃ©tegzett, mÅ±kÃ¶dÃ©si Ã©s algoritmikus kontrollokkal kell biztosÃ­tani:

- **API & Rate kontrollok:** szigorÃº sebessÃ©gkorlÃ¡tok, felhasznÃ¡lÃ³nkÃ©nt kvÃ³tÃ¡k, anomÃ¡lia-alapÃº korlÃ¡tozÃ¡s a lekÃ©rdezÃ©si bÃ¼dzsÃ©k megfÃ©kezÃ©sÃ©re.  
- **Kimenet-erÅ‘sÃ­tÃ©s:** informÃ¡ciÃ³szivÃ¡rgÃ¡s csÃ¶kkentÃ©se top-k cÃ­mkÃ©k visszaadÃ¡sÃ¡val, valÃ³szÃ­nÅ±sÃ©gek kerekÃ­tÃ©sÃ©vel vagy kalibrÃ¡lt zaj hozzÃ¡adÃ¡sÃ¡val (Ã³vatosan â€” a naiv zaj kijÃ¡tszhatÃ³).  
- **VÃ¡lasz-randomizÃ¡lÃ¡s & Watermarking:** szolgÃ¡ltatÃ³-specifikus vÃ­zjelek beÃ¡gyazÃ¡sa a vÃ¡laszokba vagy vÃ©letlenszerÅ± vÃ¡laszmechanizmus, amely rontja a kinyerÃ©st, mikÃ¶zben megtartja a hasznossÃ¡got. LÃ¡sd [[watermarking|VÃ­zjelezÃ©s]] a modell-tulajdonlÃ¡s vÃ©delmÃ©re.  
- **Model distillÃ¡ciÃ³ & ensemble vÃ©delem:** nyilvÃ¡nos hasznÃ¡latra kevÃ©sbÃ© informatÃ­v, lebutÃ­tott modellt szolgÃ¡ltatni, mÃ­g a magas hÅ±sÃ©gÅ± modellt privÃ¡tban tartani; ensemble-Ã¶k vÃ©letlenszerÅ± kivÃ¡lasztÃ¡sa nÃ¶veli a tÃ¡madÃ³ kÃ¶ltsÃ©gÃ©t.  
- **DetekciÃ³ & forenzika:** folyamatos [[query_monitoring|lekÃ©rdezÃ©s-figyelÃ©s]] Ã©s anomÃ¡liaÃ©szlelÃ©s a gyanÃºs mintÃ¡k felfedezÃ©sÃ©re; auditelÃ©si naplÃ³k jogi lÃ©pÃ©sekhez.  
- **Jogi & gazdasÃ¡gi kontrollok:** API-feltÃ©telek, fizetÃ©si rendszerek Ã©s jogi lÃ©pÃ©sek visszatartÃ³ erÅ‘t adhatnak.  
- **Technikai: differenciÃ¡lis adatvÃ©delem & kimenet-perturbÃ¡ciÃ³:** a [[differential_privacy|differenciÃ¡lis adatvÃ©delem]] csÃ¶kkenti a modell Ã©rzÃ©kenysÃ©gÃ©t egyedi tanÃ­tÃ³pÃ©ldÃ¡kra Ã©s korlÃ¡tozhatja a meta-kinyerÃ©st, de Ã¶nmagÃ¡ban nem Ã¡llÃ­tja meg a funkcionÃ¡lis kinyerÃ©st.

Egyetlen vÃ©delem sem elÃ©g â€” kombinÃ¡ld a rate-limitinget, monitorozÃ¡st, kimenet-kezelÃ©st Ã©s jogi intÃ©zkedÃ©seket. MÃ©rlegeld a kompromisszumokat: tÃºl agresszÃ­v hardening ronthatja a felhasznÃ¡lÃ³i Ã©lmÃ©nyt Ã©s a modell hasznossÃ¡gÃ¡t.

---

## ğŸ”— Operational playbook (short) / OperatÃ­v lÃ©pÃ©sek rÃ¶viden

**EN:**  
When deploying a model-as-a-service: define a threat model (attacker knowledge, query budget, allowed norms), instrument telemetry (per-key query logs, latencies, anomalies), enforce rate & quota policies, prefer to expose limited outputs (top-k), and plan forensic / legal response. Regularly simulate extraction attacks (red-teaming) to measure practical risk. Integrate with [[model_serving_security|Model Serving Security]] and [[ai_supply_chain_security|AI Supply Chain Security]] processes.

**HU:**  
Modell-szolgÃ¡ltatÃ¡s bevezetÃ©sekor: definiÃ¡ld a fenyegetÃ©si modellt (tÃ¡madÃ³ tudÃ¡sa, lekÃ©rdezÃ©si bÃ¼dzsÃ©, normÃ¡k), telepÃ­ts mÃ©rÅ‘eszkÃ¶zÃ¶ket (kulcs-szintÅ± lekÃ©rdezÃ©si naplÃ³k, kÃ©sleltetÃ©sek, anomÃ¡liÃ¡k), Ã©rvÃ©nyesÃ­tsd a rate & kvÃ³ta szabÃ¡lyokat, adj vissza korlÃ¡tozott kimeneteket (top-k), Ã©s tervezz forenzikus/jogi vÃ¡laszt. Rendszeresen vÃ©gezz red-team kinyerÃ©si szimulÃ¡ciÃ³kat a gyakorlati kockÃ¡zat mÃ©rÃ©sÃ©re. IntegrÃ¡ld a folyamatot a [[model_serving_security|modell-szolgÃ¡ltatÃ¡s biztonsÃ¡ga]] Ã©s [[ai_supply_chain_security|MI-ellÃ¡tÃ¡si lÃ¡nc biztonsÃ¡g]] folyamatokkal.

---

## ğŸ”— Related Vault topics / KapcsolÃ³dÃ³ fejezetek

**EN:**  
See also [[model_inversion|Model Inversion]], [[membership_inference|Membership Inference]], [[adversarial_example_attacks|Adversarial Examples]], [[transferability|Transferability]], [[watermarking|Watermarking]], [[differential_privacy|Differential Privacy]], and [[model_serving_security|Model Serving Security]].

**HU:**  
LÃ¡sd mÃ©g: [[model_inversion|Modell-inverziÃ³]], [[membership_inference|TagsÃ¡gi kÃ¶vetkeztetÃ©s]], [[adversarial_example_attacks|AdverszÃ¡riÃ¡lis pÃ©ldÃ¡k]], [[transferability|Ãtviteli jelensÃ©g]], [[watermarking|VÃ­zjelezÃ©s]], [[differential_privacy|DifferenciÃ¡lis adatvÃ©delem]] Ã©s a [[model_serving_security|Modell-szolgÃ¡ltatÃ¡s biztonsÃ¡ga]] fejezeteket.

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. Explain the difference between functional extraction and parameter extraction â€” how do their query budgets and defenses differ?  
2. How does returning probability scores vs. top-1 labels affect extraction success? Provide reasoning based on the formal objective.  
3. Design an experiment (red-team) to estimate how many queries \(M\) an attacker would need to reach 90% fidelity on a classification API. What measurements would you collect?  
4. What combination of monitoring signals would you use to detect an ongoing extraction attempt in production? Why?  
5. Discuss trade-offs between model utility and hardening: when might you choose to expose a distilled public model vs. the high-fidelity private model?

---

> â€œA protected model is not just an algorithm â€” itâ€™s a product wrapped in policy, telemetry, and economic friction.â€ ğŸ”
