---
version: "3.3"
section_type: "attack_index"
agent: "Index Architect"
---
ğŸš¨ COPY START ğŸš¨
# AI Attack Taxonomy  
*Foundations of Adversarial Threats*

---

## ğŸŒ Introduction to AI Attack Taxonomy  

**EN:**  
Artificial Intelligence systems, like any complex software ecosystem, are vulnerable to a wide spectrum of attacks. But unlike classical cyberattacks that target operating systems or networks, AI attacks often exploit the *data, model, or learning process itself*. Understanding these categories systematically is essential for building a robust [[ai_security_framework|AI Security Framework]].  

**HU:**  
A mestersÃ©ges intelligencia rendszerek â€” akÃ¡rcsak mÃ¡s szoftveres Ã¶koszisztÃ©mÃ¡k â€” szÃ¡mos tÃ¡madÃ¡stÃ­pusnak vannak kitÃ©ve. Azonban mÃ­g a hagyomÃ¡nyos tÃ¡madÃ¡sok az operÃ¡ciÃ³s rendszert vagy a hÃ¡lÃ³zatot cÃ©lozzÃ¡k, az AI tÃ¡madÃ¡sok magÃ¡t az *adatot, a modellt vagy a tanulÃ¡si folyamatot* tÃ¡madjÃ¡k meg. E kategÃ³riÃ¡k rendszerezett megÃ©rtÃ©se alapvetÅ‘ a robusztus [[ai_security_framework|AI biztonsÃ¡gi keretrendszer]] felÃ©pÃ­tÃ©sÃ©hez.  

---

## ğŸ§© The Three Core Attack Surfaces  

**EN:**  
At a high level, we can divide AI threats into **three interconnected layers**, representing the attack surfaces:  

1. **Data Layer (Training + Input):** Attacks such as [[data_poisoning|Data Poisoning]] and [[adversarial_examples|Adversarial Examples]] inject malicious or misleading inputs to manipulate the modelâ€™s perception.  
2. **Model Layer:** Includes [[model_stealing_and_extraction|Model Extraction]], [[membership_inference_attacks|Membership Inference]], and [[model_evasion|Evasion Attacks]], where the adversary targets model parameters or predictions.  
3. **Pipeline & Infrastructure Layer:** Threats against deployment and supply chain, like [[model_backdoors|Backdoors]], [[ai_supply_chain_attacks|AI Supply Chain Attacks]], or compromised APIs.  

**HU:**  
Magas szinten az AI-tÃ¡madÃ¡sok **hÃ¡rom Ã¶sszekapcsolt rÃ©tegre** bonthatÃ³k, melyek az egyes tÃ¡madÃ¡si felÃ¼leteket kÃ©pviselik:  

1. **Adatszint (TrÃ©ning + Bemenet):** Olyan tÃ¡madÃ¡sok, mint az [[data_poisoning|AdatmÃ©rgezÃ©s]] vagy az [[adversarial_examples|EllensÃ©ges pÃ©ldÃ¡k]], amelyek hamis adatokat fecskendeznek be a modell megtÃ©vesztÃ©sÃ©re.  
2. **Modellszint:** Ide tartozik a [[model_stealing_and_extraction|Modell-lopÃ¡s]], a [[membership_inference_attacks|TagsÃ¡gi kÃ¶vetkeztetÃ©s]] vagy a [[model_evasion|ElkerÃ¼lÃ©si tÃ¡madÃ¡sok]], ahol a tÃ¡madÃ³ a modell paramÃ©tereit vagy kimeneteit cÃ©lozza.  
3. **InfrastruktÃºra Ã©s pipeline szint:** A telepÃ­tÃ©si Ã©s ellÃ¡tÃ¡si lÃ¡ncot tÃ¡madÃ³ mÃ³dszerek, mint a [[model_backdoors|HÃ¡tsÃ³ kapuk]], az [[ai_supply_chain_attacks|EllÃ¡tÃ¡si lÃ¡nc tÃ¡madÃ¡sok]] vagy a veszÃ©lyeztetett API-k.  

---

## âš™ï¸ How AI Attacks Differ from Classical Cyberattacks  

**EN:**  
Traditional cybersecurity assumes *fixed logic* â€” but AI systems *learn* and thus are *mutable by data*. Attackers can subtly alter the training or inference environment, causing the model to behave maliciously or reveal sensitive information. This mutability makes the attack surface fundamentally different: attacks can be enacted by influencing data, not only by compromising code or infrastructure.

**HU:**  
A hagyomÃ¡nyos kiberbiztonsÃ¡g *fix logikÃ¡val* dolgozik, mÃ­g az AI rendszerek *tanulnak*, Ã­gy *adat Ã¡ltal mÃ³dosÃ­thatÃ³k*. A tÃ¡madÃ³k kÃ©pesek aprÃ³ vÃ¡ltoztatÃ¡sokkal befolyÃ¡solni a tanulÃ¡si vagy inferencia-kÃ¶rnyezetet, hogy a modell hibÃ¡san viselkedjen vagy Ã©rzÃ©keny informÃ¡ciÃ³t â€kiszivÃ¡rogtassonâ€. Ez a rugalmassÃ¡g alapvetÅ‘en mÃ¡s tÃ¡madÃ¡si felÃ¼letet eredmÃ©nyez: a tÃ¡madÃ¡sokat gyakran az adaton keresztÃ¼l viszik vÃ©ghez, nem csak kÃ³d vagy infrastruktÃºra kompromittÃ¡lÃ¡sÃ¡val.

Mathematically, a canonical adversarial perturbation can be written in raw LaTeX form so it remains visible and portable in Obsidian:

$$
x' = x + \epsilon \cdot \mathrm{sign}(\nabla_x L(\theta, x, y))
$$

This shows how a small, carefully chosen vector \(\epsilon\) â€” imperceptible to humans in many domains â€” can change the model's prediction because it follows the loss gradient direction.

---

## ğŸ’¡ Why Taxonomy Matters  

**EN:**  
Taxonomies help standardize understanding. By classifying attacks under clear categories â€” *data, model, pipeline* â€” defenders can design layered mitigations aligned with the [[nist_ai_rmf|NIST AI RMF]] and [[mitre_atlas|MITRE ATLAS]] frameworks. A clear taxonomy drives threat modeling, test-case creation, and compliance mapping: it tells you *where* to instrument logging, *what* to fuzz, and *which* tests must be part of CI.

**HU:**  
A taxonÃ³mia segÃ­ti a rendszerezett gondolkodÃ¡st. A tÃ¡madÃ¡sok vilÃ¡gos kategÃ³riÃ¡kba sorolÃ¡sÃ¡val â€” *adat, modell, pipeline* â€” a vÃ©delmi rÃ©tegek Ã¶sszehangolhatÃ³k a [[nist_ai_rmf|NIST AI RMF]] Ã©s a [[mitre_atlas|MITRE ATLAS]] keretrendszerekkel. A jÃ³l meghatÃ¡rozott taxonÃ³mia irÃ¡nyt ad a fenyegetÃ©smodellezÃ©shez, a tesztesetek kialakÃ­tÃ¡sÃ¡hoz Ã©s a megfelelÅ‘sÃ©gi tÃ©rkÃ©pezÃ©shez: megmutatja, *hol* kell naplÃ³zni, *mit* kell fuzzerelni, Ã©s *mely* teszteket kell CI-be beÃ©pÃ­teni.

---

## ğŸ§  Example Scenario: Manipulating a Facial Recognition System  

**EN:**  
Imagine a facial recognition model trained on millions of images. A threat actor injects only 0.5% poisoned data during training, subtly blending their own face with that of a VIP target. During deployment, the model now *accepts the attacker as the VIP* when wearing specific accessories (e.g., sunglasses). This is a hybrid of [[data_poisoning|data poisoning]] and [[model_backdoors|backdoor attacks]].

Lifecycle view: injection â†’ persistence (artifact in weights or preprocessing) â†’ activation (trigger during inference). For defenders, map each lifecycle phase to controls: provenance collection at ingestion, anomaly detection on training set statistics, controlled retraining with verified datasets, and targeted red-team tests for trigger activation.

**HU:**  
KÃ©pzelj el egy arcfelismerÅ‘ modellt, amely milliÃ³ kÃ©pen tanult. Egy rosszindulatÃº szereplÅ‘ a trÃ©ningadatok kÃ¶zÃ© csupÃ¡n 0,5% mÃ©rgezett pÃ©ldÃ¡t juttat be, finoman Ã¶sszekeverve a sajÃ¡t arcÃ¡t egy VIP cÃ©lpont kÃ©pÃ©vel. A kihelyezett rendszer ezutÃ¡n *a tÃ¡madÃ³t VIP-kÃ©nt azonosÃ­tja*, ha az bizonyos kiegÃ©szÃ­tÅ‘t visel (pl. napszemÃ¼veg). Ez az [[data_poisoning|adatmÃ©rgezÃ©s]] Ã©s a [[model_backdoors|hÃ¡tsÃ³ kapu]] tipikus kombinÃ¡ciÃ³ja.

A tÃ¡madÃ¡s Ã©letciklusa: befecskendezÃ©s â†’ perzisztencia (sÃºlyokban vagy elÅ‘feldolgozÃ¡sban megmaradÃ³ artefaktum) â†’ aktivÃ¡lÃ¡s (trigger az inferenciÃ¡nÃ¡l). A vÃ©delemhez rendeld a kontrollokat az Ã©letciklus fÃ¡zisaihoz: provenance gyÅ±jtÃ©s a beadÃ¡snÃ¡l, anomÃ¡lia-detektÃ¡lÃ¡s a trÃ©ning statisztikÃ¡kon, ellenÅ‘rzÃ¶tt ÃºjratanÃ­tÃ¡s validÃ¡lt adatokkal, valamint cÃ©lzott red-team tesztek a trigger aktivÃ¡lÃ¡sÃ¡ra.

---

## ğŸ›¡ï¸ Defense Context  

**EN:**  
Each layer in the taxonomy corresponds to its defense class:

- Data-level threats â†’ [[data_validation_and_sanitization|Data Validation & Sanitization]], dataset provenance, input filtering, differential testing of preprocessing.
- Model-level threats â†’ [[adversarial_training|Adversarial Training]], [[differential_privacy|Differential Privacy]], model hardening and robustness testing, membership inference audits.
- Pipeline-level threats â†’ [[model_certification_and_testing|Model Certification]], signed artifacts, secure CI/CD, supply-chain audits, and [[zero_trust_for_ai|Zero Trust for AI]] principles.

Defenses are most effective when combined. For example, provenance + DP + adversarial testing reduces both leakage and poisoning risk. Always map mitigations to attack *phases* (reconnaissance, insertion, persistence, activation, exfiltration) and create CI gates that simulate realistic adversarial behaviors.

**HU:**  
A taxonÃ³mia minden rÃ©tegÃ©hez kÃ¼lÃ¶n vÃ©dekezÃ©si osztÃ¡ly tartozik:

- AdatszintÅ± fenyegetÃ©sek â†’ [[data_validation_and_sanitization|AdattisztÃ­tÃ¡s Ã©s Ã©rvÃ©nyesÃ­tÃ©s]], adatkÃ©szlet-provenance, bemenet-szÅ±rÃ©s, elÅ‘feldolgozÃ¡s differenciÃ¡lis tesztelÃ©se.
- ModellszintÅ± fenyegetÃ©sek â†’ [[adversarial_training|EllensÃ©ges trÃ©ning]], [[differential_privacy|DifferenciÃ¡lis adatvÃ©delem]], modell-megerÅ‘sÃ­tÃ©s Ã©s robosztussÃ¡gi tesztelÃ©s, tagsÃ¡gi kÃ¶vetkeztetÃ©s auditok.
- Pipeline-szintÅ± fenyegetÃ©sek â†’ [[model_certification_and_testing|Modell-tanÃºsÃ­tÃ¡s]], alÃ¡Ã­rt artefaktok, biztonsÃ¡gos CI/CD, ellÃ¡tÃ¡si lÃ¡nc auditok Ã©s a [[zero_trust_for_ai|Zero Trust az AI-ban]] elvei.

A vÃ©dekezÃ©sek akkor mÅ±kÃ¶dnek igazÃ¡n jÃ³l, ha kombinÃ¡ljuk Å‘ket. PÃ©ldÃ¡ul provenance + DP + adversarial tesztelÃ©s egyszerre csÃ¶kkenti az adat-szivÃ¡rgÃ¡st Ã©s az adatmÃ©rgezÃ©s kockÃ¡zatÃ¡t. Mindig rendeld a mitigÃ¡ciÃ³kat a tÃ¡madÃ¡s *fÃ¡zisaihoz* (felderÃ­tÃ©s, befecskendezÃ©s, perzisztencia, aktivÃ¡lÃ¡s, adatlopÃ¡s), Ã©s Ã©pÃ­ts CI kapukat, amelyek valÃ³sÃ¡ghÅ± ellenfeleket szimulÃ¡lnak.

---

## âš–ï¸ Summary  

**EN:**  
The AI Attack Taxonomy is the foundation of understanding AI security. Every other defense, assurance, or governance mechanism builds on this structure â€” knowing *where* an attack happens defines *how* it must be mitigated. Use the taxonomy as the first column in threat models and as the organizing principle for red/blue team exercises.

**HU:**  
Az AI tÃ¡madÃ¡si taxonÃ³mia az AI-biztonsÃ¡g megÃ©rtÃ©sÃ©nek alapja. Minden mÃ¡s vÃ©dekezÃ©si, ellenÅ‘rzÃ©si vagy irÃ¡nyÃ­tÃ¡si megoldÃ¡s erre a struktÃºrÃ¡ra Ã©pÃ¼l â€” ha tudjuk, *hol* tÃ¶rtÃ©nik a tÃ¡madÃ¡s, azt is meghatÃ¡rozhatjuk, *hogyan* vÃ©dekezzÃ¼nk ellene. HasznÃ¡ld ezt a taxonÃ³miÃ¡t a fenyegetÃ©smodellezÃ©s elsÅ‘ oszlopakÃ©nt Ã©s rendezÅ‘elvkÃ©nt a red/blue team gyakorlatokhoz.

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek  

1. What are the three main layers of the AI attack taxonomy, and what kind of attacks belong to each?  
2. How does an adversarial example differ from a data poisoning attack, and why does the distinction matter for defenses?  
3. Explain why AI models can be more vulnerable than traditional software systems â€” give a concrete example and map mitigations to the attack phases.  
4. Show, using the raw LaTeX block format, how a small perturbation on input \(x\) can be written to demonstrate an adversarial example.  
5. Which industry frameworks align with this taxonomy, and how would you map one defense (e.g., adversarial training) to the taxonomy layers?

---

> *â€œTo understand an adversary, you must first understand what your system believes.â€*

---
ğŸš¨ COPY END ğŸš¨
