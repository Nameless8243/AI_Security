# ğŸ§¨ Adversarial Example Attacks

---

## ğŸŒ What are Adversarial Examples? / Mi az az adverszÃ¡riÃ¡lis pÃ©lda?

**EN:**  
Adversarial examples are inputs deliberately perturbed to make a machine learning model produce an incorrect output while the changes are (almost) imperceptible to humans. These inputs expose fragility in learned models and are a primary offensive technique in the [[attack_taxonomy|AI attack taxonomy]]. They matter because they break trust and can be weaponized â€” e.g., bypassing image-based authentication, causing mislabelling in autonomous driving, or manipulating content filters. ğŸ¤–ğŸ›¡ï¸

**HU:**  
Az adverszÃ¡riÃ¡lis pÃ©ldÃ¡k olyan bemenetek, amelyeket szÃ¡ndÃ©kosan megvÃ¡ltoztatnak Ãºgy, hogy az ember szÃ¡mÃ¡ra alig Ã©szrevehetÅ‘, de a gÃ©pi tanulÃ³ modell tÃ©ves kimenetet adjon. Ezek a bemenetek rÃ¡mutatnak a modellek tÃ¶rÃ©kenysÃ©gÃ©re Ã©s az [[attack_taxonomy|MI-tÃ¡madÃ¡sok]] fÅ‘ mÃ³dszerei kÃ¶zÃ© tartoznak. Fontosak, mert megsÃ©rtik a bizalmat Ã©s fegyverkÃ©nt hasznÃ¡lhatÃ³k (pl. arcfelismerÃ©s kikerÃ¼lÃ©se, Ã¶nvezetÅ‘ autÃ³ tÃ©veszmÃ©je, tartalomszÅ±rÃ©s megkerÃ¼lÃ©se). ğŸ¤–ğŸ›¡ï¸

---

## ğŸ’¡ Formal definition (optimization view) / FormÃ¡lis definÃ­ciÃ³ (optimalizÃ¡lÃ¡si nÃ©zet)

**EN:**  
Given an input \(x\) with true label \(y\) and a classifier \(f(\cdot)\), an adversarial example \(x'\) solves a constrained optimization: find the smallest perturbation \(\delta\) such that the model's prediction changes (or the model outputs a targeted class \(y_{t}\)) while keeping \(\delta\) bounded (imperceptible):

$$
\min_{\delta} \ \|\delta\|_p \quad \text{s.t.} \quad f(x+\delta) \ne f(x), \quad \| \delta \|_p \le \epsilon
$$

For targeted attacks:

$$
\min_{\delta} \ \|\delta\|_p \quad \text{s.t.} \quad f(x+\delta) = y_t, \quad \| \delta \|_p \le \epsilon
$$

This optimization view unifies most attack algorithms. The norm \(p\) (often \(L_\infty\) or \(L_2\)) and budget \(\epsilon\) define perceptibility and power.

**HU:**  
Adott egy bemenet \(x\) valÃ³di cÃ­mkÃ©je \(y\) Ã©s egy osztÃ¡lyozÃ³ \(f(\cdot)\). Egy adverszÃ¡riÃ¡lis pÃ©ldÃ¡t \(x'\) egy korlÃ¡tos perturbÃ¡ciÃ³ \(\delta\) megtalÃ¡lÃ¡sakÃ©nt Ã­rhatunk le, amely megvÃ¡ltoztatja a modell predikciÃ³jÃ¡t, mikÃ¶zben \(\delta\) kicsi (alig lÃ¡thatÃ³):

$$
\min_{\delta} \ \|\delta\|_p \quad \text{s.t.} \quad f(x+\delta) \ne f(x), \quad \| \delta \|_p \le \epsilon
$$

CÃ©lozhatÃ³ tÃ¡madÃ¡soknÃ¡l:

$$
\min_{\delta} \ \|\delta\|_p \quad \text{s.t.} \quad f(x+\delta) = y_t, \quad \| \delta \|_p \le \epsilon
$$

Ez az optimalizÃ¡ciÃ³i nÃ©zet Ã¶sszevonja a legtÃ¶bb tÃ¡madÃ¡si algoritmust. A \(p\)-norm (gyakran \(L_\infty\) vagy \(L_2\)) Ã©s a kÃ¶ltsÃ©gkeret \(\epsilon\) hatÃ¡rozza meg az Ã©szlelhetÅ‘sÃ©get Ã©s a tÃ¡madÃ³ erejÃ©t.

---

## ğŸ”ª Common attack algorithms / Gyakori tÃ¡madÃ¡si algoritmusok

**EN:**  
Classic and widely-used methods include:  
- **FGSM (Fast Gradient Sign Method):** a single-step white-box perturbation using the sign of the gradient. Its update rule:

$$
x' = x + \epsilon \cdot \text{sign}(\nabla_x \mathcal{L}(f(x), y))
$$

- **PGD (Projected Gradient Descent):** iterative multi-step version of FGSM with projection back into the allowed \(\epsilon\)-ball â€” strong baseline for robustness evaluation. Iteration:

$$
x_{t+1} = \Pi_{B_\epsilon(x)}\Big( x_t + \alpha \cdot \text{sign}(\nabla_x \mathcal{L}(f(x_t), y)) \Big)
$$

- **CW (Carliniâ€“Wagner):** optimization-based attacks that minimize a tailored objective to find small \(L_2\) perturbations; often used to bypass certain defenses.  

These algorithms highlight **white-box** (attacker knows model & gradients) vs **black-box** (no gradients; rely on queries or transferability) threat models and connect to [[transferability|Transferability]] phenomena.

**HU:**  
Klasszikus, szÃ©les kÃ¶rben hasznÃ¡lt mÃ³dszerek pÃ©ldÃ¡ul:  
- **FGSM (Fast Gradient Sign Method):** egylÃ©pÃ©ses white-box perturbÃ¡ciÃ³, amely a vesztesÃ©g gradiensÃ©nek elÅ‘jelÃ©t hasznÃ¡lja. FrissÃ­tÃ©si szabÃ¡ly:

$$
x' = x + \epsilon \cdot \text{sign}(\nabla_x \mathcal{L}(f(x), y))
$$

- **PGD (Projected Gradient Descent):** FGSM iteratÃ­v, tÃ¶bblÃ©pÃ©ses vÃ¡ltozata, amely minden lÃ©pÃ©s utÃ¡n visszavetÃ­ti a pertubÃ¡ciÃ³t az \(\epsilon\)-gÃ¶mbbe â€” erÅ‘s alapvonal a robosztussÃ¡g tesztelÃ©sÃ©hez. IterÃ¡ciÃ³:

$$
x_{t+1} = \Pi_{B_\epsilon(x)}\Big( x_t + \alpha \cdot \text{sign}(\nabla_x \mathcal{L}(f(x_t), y)) \Big)
$$

- **CW (Carliniâ€“Wagner):** optimalizÃ¡ciÃ³ra Ã©pÃ¼lÅ‘ tÃ¡madÃ¡sok, amelyek kis \(L_2\) perturbÃ¡ciÃ³kat keresnek; gyakran hasznÃ¡ljÃ¡k bizonyos vÃ©delmek kikerÃ¼lÃ©sÃ©re.  

Ezek az algoritmusok illusztrÃ¡ljÃ¡k a **white-box** (tÃ¡madÃ³ ismeri a modellt Ã©s a gradienseket) Ã©s **black-box** (nincs gradiens â€“ lekÃ©rdezÃ©sekre vagy Ã¡tvitelre tÃ¡maszkodik) fenyegetÃ©si modelleket, Ã©s kapcsolÃ³dnak a [[transferability|Ã¡tviteli jelensÃ©ghez]].

---

## ğŸ” Transferability & Black-box attacks / Ãtviteli kÃ©pessÃ©g Ã©s black-box tÃ¡madÃ¡sok

**EN:**  
Adversarial examples often transfer between models: an example crafted for model A may fool model B. Attackers exploit transferability to mount black-box attacks by training surrogate models and transferring adversarial inputs. Query-based black-box attacks (e.g., Zeroth-Order Optimization) approximate gradients via queries. Transferability makes defenses harder and highlights the need for system-level protections in [[model_serving_security|model serving]].

**HU:**  
Az adverszÃ¡riÃ¡lis pÃ©ldÃ¡k gyakran Ã¡tvihetÅ‘k modellek kÃ¶zÃ¶tt: egy A modellre lÃ©trehozott pÃ©lda B modellt is becsaphatja. A tÃ¡madÃ³k ezt kihasznÃ¡lva surrogÃ¡t modellekkel kÃ©szÃ­tenek pÃ©ldÃ¡kat Ã©s Ã¡tvitt tÃ¡madÃ¡sokat hajtanak vÃ©gre black-box kÃ¶rnyezetben. LekÃ©rdezÃ©s-alapÃº black-box tÃ¡madÃ¡sok (pl. Zeroth-Order Optimization) a gradiens approximÃ¡ciÃ³jÃ¡ra tÃ¡maszkodnak. Az Ã¡tviteli kÃ©pessÃ©g megnehezÃ­ti a vÃ©delmeket Ã©s kiemeli a rendszer-szintÅ± vÃ©delmi szÃ¼ksÃ©gletet a [[model_serving_security|modell-szolgÃ¡ltatÃ¡s]] kÃ¶rnyezetben.

---

## ğŸ›¡ï¸ Defenses & their caveats / VÃ©dekezÃ©sek Ã©s Ã³vatos megjegyzÃ©sek

**EN:**  
Defenses fall into empirical and certified categories â€” both have trade-offs:

- **Adversarial Training** (empirical): augment training set with adversarial examples (e.g., PGD training). It is the strongest practical defense but costly and often degrades clean accuracy. See [[adversarial_training|Adversarial Training]].

- **Input preprocessing / Detection:** JPEG compression, bit-depth reduction, or statistical detectors can remove or flag perturbations â€” but many are circumventable and risk **gradient masking** (giving a false sense of security).

- **Certified Robustness:** techniques like **Randomized Smoothing** provide provable \(L_2\) robustness guarantees under certain noise models. Certified methods trade off tightness and applicability; they give guarantees but often at smaller radii.

- **Ensembles & Diversity:** using multiple models or randomized inference pipelines increases attack cost but does not fully prevent transferability.

Critically: many defenses were broken by stronger adaptive attacks. The community standard for evaluating defenses is to test against strong, adaptive attacks (white-box PGD/CW) and to avoid relying on obfuscated gradients or untested heuristics. Always treat robustness claims skeptically and verify with rigorous evaluation. ğŸ”

**HU:**  
A vÃ©dekezÃ©sek empirikus Ã©s tanÃºsÃ­tott (certified) kategÃ³riÃ¡kba sorolhatÃ³k â€” mindkettÅ‘nek vannak kompromisszumai:

- **Adversarial Training** (empirikus): adverszÃ¡riÃ¡lis pÃ©ldÃ¡kkal bÅ‘vÃ­tett tanÃ­tÃ¡s (pl. PGD training). Gyakorlatban legerÅ‘sebb vÃ©dekezÃ©s, de kÃ¶ltsÃ©ges Ã©s gyakran rontja a tiszta pontossÃ¡got. LÃ¡sd [[adversarial_training|Adversarial Training]].

- **Bemenet elÅ‘feldolgozÃ¡s / DetektÃ¡lÃ¡s:** JPEG tÃ¶mÃ¶rÃ­tÃ©s, bit-mÃ©lysÃ©g csÃ¶kkentÃ©s vagy statisztikai detektorok eltÃ¡volÃ­thatjÃ¡k vagy jelÃ¶lhetik a perturbÃ¡ciÃ³kat â€” de sokan kikerÃ¼lhetÅ‘k, Ã©s fennÃ¡ll a **gradiens elrejtÃ©s** (gradient masking) veszÃ©lye, ami hamis biztonsÃ¡gÃ©rzetet ad.

- **TanÃºsÃ­tott robosztussÃ¡g:** mÃ³dszerek, mint a **Randomized Smoothing**, bizonyÃ­thatÃ³ \(L_2\) robosztussÃ¡gi garanciÃ¡t adnak bizonyos zajmodellek alatt. A tanÃºsÃ­tott mÃ³dszerek kompromisszumot jelentenek a garancia szigorÃºsÃ¡ga Ã©s alkalmazhatÃ³sÃ¡ga kÃ¶zÃ¶tt; gyakran kisebb sugÃ¡rig adnak garanciÃ¡t.

- **Ensemble Ã©s diverzitÃ¡s:** tÃ¶bb modell vagy randomizÃ¡lt inferencia nÃ¶veli a tÃ¡madÃ¡s kÃ¶ltsÃ©gÃ©t, de nem akadÃ¡lyozza meg teljesen az Ã¡tvitel jelensÃ©gÃ©t.

Kritikus: sok vÃ©dekezÃ©st erÅ‘sebb adaptÃ­v tÃ¡madÃ¡sok tÃ¶rtek meg. A vÃ©dekezÃ©seket mindig erÅ‘s, adaptÃ­v (white-box PGD/CW) tÃ¡madÃ¡sokkal kell Ã©rtÃ©kelni â€” kerÃ¼ljÃ¼k az obfuszkÃ¡lt gradiensre Ã©pÃ¼lÅ‘ vagy tesztlen heuristikus Ã¡llÃ­tÃ¡sokat. LegyÃ¼nk szkeptikusak a robosztussÃ¡gi Ã¡llÃ­tÃ¡sokkal.

---

## ğŸ§­ Practical guidance for defenders / Gyakorlati ÃºtmutatÃ³ vÃ©dÅ‘knek

**EN:**  
- Use adversarial training for high-risk models (authentication, safety-critical). Combine with clean-accuracy monitoring and [[model_drift|drift detection]].  
- Build layered defenses: input sanitization â†’ robust model â†’ output monitoring & anomaly detection. Treat attackers as adaptive.  
- Evaluate defenses with strong white-box baselines (PGD/autoattack) and report the threat model (knowledge, query budget, norm, \(\epsilon\)).  
- Consider certified methods for auditability where provable guarantees are required (e.g., regulated domains).  
- Add operational controls: rate limits, query-monitoring, authentication, model ensemble diversity, and human-in-the-loop for risky decisions.

**HU:**  
- Magas kockÃ¡zatÃº modellekhez (hitelesÃ­tÃ©s, biztonsÃ¡gi rendszerek) hasznÃ¡ljunk adverszÃ¡riÃ¡lis tanÃ­tÃ¡st. KombinÃ¡ljuk tiszta pontossÃ¡g ellenÅ‘rzÃ©ssel Ã©s [[model_drift|drift Ã©rzÃ©kelÃ©ssel]].  
- RÃ©tegzett vÃ©delmet Ã©pÃ­tsÃ¼nk: bemenet-sanitization â†’ robosztus modell â†’ kimenet felÃ¼gyelet & anomÃ¡lia-detektÃ¡lÃ¡s. TekintsÃ¼nk a tÃ¡madÃ³kra adaptÃ­v szereplÅ‘kÃ©nt.  
- A vÃ©dekezÃ©seket erÅ‘s white-box alapvonalakkal (PGD/autoattack) Ã©rtÃ©keljÃ¼k, Ã©s mindig jelÃ¶ljÃ¼k a fenyegetÃ©si modellt (tudÃ¡s, lekÃ©rdezÃ©si bÃ¼dzsÃ©, norma, \(\epsilon\)).  
- SzabÃ¡lyozott terÃ¼leteken, ahol auditÃ¡lhatÃ³sÃ¡g kell, fontoljuk meg a tanÃºsÃ­tott mÃ³dszereket.  
- OperatÃ­v kontrollok: lekÃ©rdezÃ©si korlÃ¡tok, lekÃ©rdezÃ©sfigyelÃ©s, autentikÃ¡ciÃ³, modell-ensemble diverzitÃ¡s Ã©s emberi ellenÅ‘rzÃ©s a kockÃ¡zatos dÃ¶ntÃ©sekhez.

---

## ğŸ”— Connections to other Vault topics / KapcsolÃ³dÃ¡s mÃ¡s fejezetekhez

**EN:**  
This topic links strongly to [[adversarial_training|Adversarial Training]], [[model_serving_security|Model Serving Security]], [[transferability|Transferability]], [[certified_robustness|Certified Robustness]], [[poisoning|Data Poisoning]] (as a complementary attack surface), and [[consistency_audit|Consistency Auditing]] for production monitoring.

**HU:**  
Ez a tÃ©ma erÅ‘sen kapcsolÃ³dik a [[adversarial_training|Adversarial Training]], [[model_serving_security|Model Serving Security]], [[transferability|Transferability]], [[certified_robustness|Certified Robustness]], [[poisoning|Data Poisoning]] (komplementer tÃ¡madÃ¡si felÃ¼let), Ã©s a produkciÃ³s felÃ¼gyeletre vonatkozÃ³ [[consistency_audit|Konzisztencia-auditÃ¡lÃ¡s]] fejezetekhez.

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. Explain the adversarial example optimization objective and how \(\epsilon\) and the chosen norm affect attack visibility.  
2. Compare FGSM and PGD: why is iterative PGD a stronger baseline?  
3. What is transferability and why does it enable black-box attacks? Give a practical attack workflow.  
4. List the main categories of defenses, their strengths, and common failure modes (e.g., gradient masking).  
5. Design a defensive deployment checklist for a model exposed via an API (threat model, mitigation layers, monitoring).

---

> â€œAn adversary reveals the boundaries of our assumptions â€” robust systems begin by admitting their borders.â€ âš–ï¸
