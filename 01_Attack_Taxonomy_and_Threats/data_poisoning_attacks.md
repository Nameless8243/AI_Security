---
version: "3.3"
section_type: "attack"
agent: "Threat Mapper"
---
# â˜£ï¸ Data Poisoning Attacks

---

## ğŸŒ What Is Data Poisoning? / Mi az a Data Poisoning?

**EN:**  
Data poisoning is the deliberate manipulation of the training data to compromise a machine learning modelâ€™s integrity, confidentiality, or availability. It targets the *root of the AI lifecycle* â€” the data itself â€” long before deployment. In contrast to [[adversarial_example_attacks|Adversarial Example Attacks]], which corrupt inputs at inference time, poisoning attacks alter the modelâ€™s *learning process*.  

A poisoned model may behave maliciously, misclassify critical samples, or leak sensitive patterns. In security terms, this is a **supply chain attack** within the AI pipeline. â˜ ï¸ğŸ¤–  

**HU:**  
A data poisoning (adatmÃ©rgezÃ©s) a tanÃ­tÃ³adatok szÃ¡ndÃ©kos manipulÃ¡lÃ¡sa, amely a gÃ©pi tanulÃ¡si modell integritÃ¡sÃ¡t, bizalmassÃ¡gÃ¡t vagy rendelkezÃ©sre Ã¡llÃ¡sÃ¡t tÃ¡madja. Ez az **MI-Ã©letciklus legkorÃ¡bbi pontjÃ¡t**, magÃ¡t az adatot cÃ©lozza â€” jÃ³val a bevezetÃ©s elÅ‘tt.  

EllentÃ©tben az [[adversarial_example_attacks|adverszÃ¡riÃ¡lis pÃ©ldÃ¡kkal]], amelyek az inferencia idejÃ©n tÃ¡madnak, a poisoning tÃ¡madÃ¡s a **tanulÃ¡si folyamatot** torzÃ­tja.  

Az ilyen mÃ³don mÃ©rgezett modell helytelenÃ¼l viselkedhet, fÃ©lreosztÃ¡lyozhat kritikus mintÃ¡kat vagy Ã©rzÃ©keny mintÃ¡zatokat szivÃ¡rogtathat ki. BiztonsÃ¡gi szempontbÃ³l ez az MI-pipeline **ellÃ¡tÃ¡si lÃ¡nc tÃ¡madÃ¡sa**. â˜ ï¸ğŸ¤–

---

## ğŸ§© Formal View / FormÃ¡lis NÃ©zet

**EN:**  
A poisoning attacker modifies the training dataset \( D = \{(x_i, y_i)\}_{i=1}^n \) to produce \( D' = D \cup \{(x_p, y_p)\} \) such that the trained model \( f' = \text{Train}(D') \) exhibits harmful behavior (e.g., misclassification of target \(x_t\)).

The attackerâ€™s optimization goal can be described as:

$$
\max_{(x_p, y_p)} \ \mathcal{L}(f' , x_t, y_t) \quad \text{s.t.} \quad \| x_p - x_{clean} \|_p \le \epsilon
$$

Here, \( (x_p, y_p) \) is the poisoned sample, and the constraint ensures it looks benign.

**HU:**  
A tÃ¡madÃ³ mÃ³dosÃ­tja a tanÃ­tÃ³halmazt \( D = \{(x_i, y_i)\}_{i=1}^n \), hogy egy Ãºj halmazt kapjon: \( D' = D \cup \{(x_p, y_p)\} \), ahol az Ãºjonnan tanÃ­tott modell \( f' = \text{Train}(D') \) kÃ¡ros viselkedÃ©st mutat (pl. hibÃ¡san osztÃ¡lyozza a cÃ©lpontot \(x_t\)).  

A tÃ¡madÃ³ optimalizÃ¡lÃ¡si cÃ©lja a kÃ¶vetkezÅ‘kÃ©ppen Ã­rhatÃ³ le:

$$
\max_{(x_p, y_p)} \ \mathcal{L}(f' , x_t, y_t) \quad \text{s.t.} \quad \| x_p - x_{clean} \|_p \le \epsilon
$$

Itt \( (x_p, y_p) \) a mÃ©rgezett minta, a korlÃ¡t pedig biztosÃ­tja, hogy a tÃ¡madÃ¡s ne legyen kÃ¶nnyen Ã©szlelhetÅ‘.

---

## ğŸ’£ Attack Classes / TÃ¡madÃ¡stÃ­pusok

**EN:**  
Poisoning attacks are generally categorized into **availability** and **integrity** classes, and may include hidden **backdoors**.  

### ğŸ§¨ 1. Availability Attacks  
- Goal: make the model unusable or degrade global performance.  
- The attacker injects corrupted data to increase overall error.  
- Common in denial-of-service contexts for ML pipelines.  

### ğŸ¯ 2. Targeted (Integrity) Attacks  
- Goal: manipulate specific model behavior â€” e.g., cause one class to misclassify another.  
- The model seems fine overall but fails in controlled conditions.  
- Example: a spam classifier mislabels a certain attackerâ€™s emails as â€œsafe.â€  

### ğŸ•µï¸â€â™‚ï¸ 3. Backdoor (Trojan) Attacks  
- Goal: implant hidden behavior triggered by a specific pattern (â€œtriggerâ€).  
- The model behaves normally, but when the trigger appears, it outputs the attackerâ€™s desired label.  
- Example: a traffic sign classifier always labels a stop sign with a yellow sticker as â€œspeed limit.â€  

**HU:**  
Az adatmÃ©rgezÃ©s tÃ¡madÃ¡sok kÃ©t fÅ‘ tÃ­pusba sorolhatÃ³k: **availability** Ã©s **integrity** tÃ¡madÃ¡sok, illetve rejtett **backdoor**-ok formÃ¡jÃ¡ban is megjelenhetnek.  

### ğŸ§¨ 1. ElÃ©rhetÅ‘sÃ©gi tÃ¡madÃ¡sok  
- CÃ©l: a modell hasznÃ¡lhatatlannÃ¡ tÃ©tele vagy teljesÃ­tmÃ©nyÃ©nek rombolÃ¡sa.  
- A tÃ¡madÃ³ hibÃ¡s adatokat injektÃ¡l, hogy nÃ¶velje a hibarÃ¡tÃ¡t.  
- Gyakori az ML rendszerek elleni szolgÃ¡ltatÃ¡smegtagadÃ¡s (DoS) tÃ­pusÃº tÃ¡madÃ¡sokban.  

### ğŸ¯ 2. CÃ©lzott (integritÃ¡si) tÃ¡madÃ¡sok  
- CÃ©l: meghatÃ¡rozott viselkedÃ©s manipulÃ¡lÃ¡sa â€“ pl. egy adott osztÃ¡ly fÃ©lreosztÃ¡lyozÃ¡sa.  
- A modell globÃ¡lisan jÃ³l mÅ±kÃ¶dik, de bizonyos helyzetekben hibÃ¡zik.  
- PÃ©lda: spam-szÅ±rÅ‘, amely a tÃ¡madÃ³ leveleit biztonsÃ¡gosnak minÅ‘sÃ­ti.  

### ğŸ•µï¸â€â™‚ï¸ 3. Backdoor (TrÃ³jai) tÃ¡madÃ¡sok  
- CÃ©l: rejtett viselkedÃ©s beÃ¼ltetÃ©se, amit egy speciÃ¡lis minta (â€triggerâ€) aktivÃ¡l.  
- A modell normÃ¡lisan mÅ±kÃ¶dik, de a trigger megjelenÃ©sekor a tÃ¡madÃ³ Ã¡ltal kÃ­vÃ¡nt cÃ­mkÃ©t adja.  
- PÃ©lda: kÃ¶zlekedÃ©si tÃ¡bla felismerÅ‘, amely sÃ¡rga matrica esetÃ©n â€sebessÃ©gkorlÃ¡tozÃ¡skÃ©ntâ€ azonosÃ­tja a stop tÃ¡blÃ¡t.

---

## âš™ï¸ Poisoning Vectors / MÃ©rgezÃ©si Vektorok

**EN:**  
Attackers can inject poison at various points in the AI supply chain:

1. **Open-source dataset contributions** (GitHub, Kaggle, HuggingFace).  
2. **Data labeling outsourcing** â€” subtle bias insertion.  
3. **Feature extraction manipulation** (e.g., poisoned embeddings).  
4. **Continuous learning / federated learning** updates â€” malicious clients upload poisoned gradients.  

**HU:**  
A tÃ¡madÃ³k tÃ¶bb ponton is bejuttathatjÃ¡k a mÃ©rgezÃ©st az MI-ellÃ¡tÃ¡si lÃ¡ncba:

1. **NyÃ­lt forrÃ¡sÃº adatkÃ©szletek mÃ³dosÃ­tÃ¡sa** (GitHub, Kaggle, HuggingFace).  
2. **KÃ¼lsÅ‘s cÃ­mkÃ©zÃ©s** â€” rejtett torzÃ­tÃ¡s beÃ©pÃ­tÃ©se.  
3. **JellemzÅ‘k (feature) manipulÃ¡lÃ¡sa** â€” mÃ©rgezett embeddingek.  
4. **Folyamatos vagy federÃ¡lt tanulÃ¡s** sorÃ¡n â€” rosszindulatÃº kliensek feltÃ¶ltenek fertÅ‘zÃ¶tt frissÃ­tÃ©seket.  

---

## ğŸ§  Example: Backdoor Trigger Attack / PÃ©lda: Rejtett TrÃ³jai tÃ¡madÃ¡s

**EN:**  
Letâ€™s say a CNN is trained to classify traffic signs. The attacker adds 50 poisoned images â€” stop signs with a yellow square sticker â€” labeled as *speed limit*. The model learns to associate the yellow sticker pattern with â€œspeed limit.â€ At inference time:

$$
f(\text{stop sign + yellow sticker}) = \text{speed limit}
$$

The backdoor remains dormant for all other cases, making it difficult to detect via validation accuracy.

**HU:**  
TegyÃ¼k fel, hogy egy CNN-t kÃ¶zlekedÃ©si tÃ¡blÃ¡k felismerÃ©sÃ©re tanÃ­tunk. A tÃ¡madÃ³ 50 darab mÃ©rgezett kÃ©pet ad hozzÃ¡ â€“ sÃ¡rga matricÃ¡s stop tÃ¡blÃ¡kat â€“ â€sebessÃ©gkorlÃ¡tozÃ¡sâ€ cÃ­mkÃ©vel. A modell megtanulja, hogy a sÃ¡rga matrica â€sebessÃ©gkorlÃ¡tozÃ¡sâ€-t jelent.  

Inferencia kÃ¶zben:

$$
f(\text{stop tÃ¡bla + sÃ¡rga matrica}) = \text{sebessÃ©gkorlÃ¡tozÃ¡s}
$$

A backdoor a tÃ¶bbi esetben rejtve marad, Ã­gy validÃ¡ciÃ³s pontossÃ¡gbÃ³l nehezen Ã©szlelhetÅ‘.

---

## ğŸ§° Detection & Defense / FelismerÃ©s Ã©s VÃ©dekezÃ©s

**EN:**  
### ğŸ•µï¸ Detection  
- **Data Provenance Auditing:** Track data origin, contributor identity, and dataset lineage.  
- **Statistical Outlier Detection:** Find suspicious samples (e.g., label flipping patterns).  
- **Gradient Similarity / Activation Analysis:** Identify anomalous influence of samples on gradients.  
- **Model Behavior Monitoring:** Analyze model confidence, activation clusters, or response under perturbations.  

### ğŸ›¡ï¸ Defense  
- **[[data_sanitization|Data Sanitization]]:** Clean or validate datasets before training.  
- **[[robust_training|Robust Training]]:** Reduce influence of outliers (e.g., using trimmed loss).  
- **[[differential_privacy|Differential Privacy]]:** Limit sensitivity of models to single samples.  
- **[[federated_learning_security|Secure Federated Learning]]:** Apply client validation, gradient clipping, and Byzantine-robust aggregation.  
- **[[ai_supply_chain_security|AI Supply Chain Security]]:** Secure data pipelines, access control, and integrity verification.

**HU:**  
### ğŸ•µï¸ FelismerÃ©s  
- **Adat-eredet auditÃ¡lÃ¡s:** KÃ¶vesd az adatok forrÃ¡sÃ¡t, a kÃ¶zremÅ±kÃ¶dÅ‘k szemÃ©lyazonossÃ¡gÃ¡t Ã©s az adathalmaz szÃ¡rmazÃ¡sÃ¡t.  
- **Statisztikai anomÃ¡lia-keresÃ©s:** Szokatlan mintÃ¡k felismerÃ©se (pl. cÃ­mkeforgatÃ¡s).  
- **Gradiens-hasonlÃ³sÃ¡g / AktivÃ¡ciÃ³ elemzÃ©s:** MintÃ¡k hatÃ¡sÃ¡nak vizsgÃ¡lata a gradiensre.  
- **ModellviselkedÃ©s figyelÃ©se:** Modellbizalom, aktivÃ¡ciÃ³s klaszterek vagy perturbÃ¡ciÃ³k alatti vÃ¡laszok elemzÃ©se.  

### ğŸ›¡ï¸ VÃ©dekezÃ©s  
- **[[data_sanitization|AdattisztÃ­tÃ¡s]]:** Az adatok Ã©rvÃ©nyesÃ­tÃ©se tanÃ­tÃ¡s elÅ‘tt.  
- **[[robust_training|Robosztus tanÃ­tÃ¡s]]:** Az outlierek hatÃ¡sÃ¡nak csÃ¶kkentÃ©se (pl. â€trimmed lossâ€).  
- **[[differential_privacy|DifferenciÃ¡lis adatvÃ©delem]]:** A modellek Ã©rzÃ©kenysÃ©gÃ©nek korlÃ¡tozÃ¡sa egyes mintÃ¡kra.  
- **[[federated_learning_security|FederÃ¡lt tanulÃ¡s vÃ©delme]]:** Kliens-validÃ¡lÃ¡s, gradiens-vÃ¡gÃ¡s, robusztus aggregÃ¡ciÃ³.  
- **[[ai_supply_chain_security|MI-ellÃ¡tÃ¡si lÃ¡nc vÃ©delme]]:** AdatcsatornÃ¡k, jogosultsÃ¡gok Ã©s integritÃ¡sellenÅ‘rzÃ©s biztosÃ­tÃ¡sa.

---

## ğŸ”— Relation to Other Topics / KapcsolÃ³dÃ³ Fejezetek

**EN:**  
Closely related to [[adversarial_example_attacks|Adversarial Example Attacks]] (but occurs during training), and connected to [[model_inversion|Model Inversion]], [[membership_inference|Membership Inference]], and [[ai_supply_chain_security|AI Supply Chain Security]].  

**HU:**  
Szorosan kapcsolÃ³dik az [[adversarial_example_attacks|AdverszÃ¡riÃ¡lis pÃ©ldÃ¡khoz]] (de a tanÃ­tÃ¡s sorÃ¡n tÃ¶rtÃ©nik), valamint a [[model_inversion|Modell-inverziÃ³]], [[membership_inference|TagsÃ¡gi kÃ¶vetkeztetÃ©s]] Ã©s [[ai_supply_chain_security|MI-ellÃ¡tÃ¡si lÃ¡nc biztonsÃ¡g]] fejezetekhez.

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. What distinguishes poisoning attacks from adversarial examples?  
2. Describe a real-world scenario where a backdoor trigger could be implanted unnoticed.  
3. How does federated learning expand the attack surface for data poisoning?  
4. Which detection techniques can identify poisoned data before training?  
5. What governance and provenance controls can prevent data poisoning in enterprise AI pipelines?

---

> â€œTrust in AI begins at the source â€” if the data is poisoned, no algorithm can heal it.â€ âš—ï¸
