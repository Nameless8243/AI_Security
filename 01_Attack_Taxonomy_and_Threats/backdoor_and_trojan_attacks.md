# Backdoor & Trojan Attacks ğŸ’€

_Hidden threats inside AI models â€“ bilingual educational version (HU + EN)_

---

AI systems are only as trustworthy as the data and logic theyâ€™re built on. But what if an attacker doesnâ€™t break in from the outside â€” instead, they hide **inside the model itself**?  
Ez a fejezet az **AI-rendszerek belsÅ‘ fertÅ‘zÃ©seirÅ‘l** szÃ³l: amikor a tÃ¡madÃ³ nem kÃ­vÃ¼lrÅ‘l tÃ¡madja a modellt, hanem **beleÃ¼ltet egy triggert**, ami csak bizonyos kÃ¶rÃ¼lmÃ©nyek kÃ¶zÃ¶tt aktivÃ¡lÃ³dik.

Welcome to the world of **Backdoor and Trojan Attacks** â€” the invisible saboteurs of modern machine learning. ğŸ•µï¸â€â™‚ï¸

---

## ğŸ­ What is a Backdoor Attack?

**EN:**  
A **Backdoor Attack** (or _Trojan Attack_) happens when a model is secretly trained to behave normally most of the time â€” but produce a malicious output when a specific _trigger_ appears.

**HU:**  
A **Backdoor-tÃ¡madÃ¡s** sorÃ¡n a modell normÃ¡lisan mÅ±kÃ¶dik, amÃ­g egy bizonyos **triggert** (pl. kÃ©pi mintÃ¡t, szÃ¶veges kulcsszÃ³t, pixelzajt) nem Ã©szlel â€” akkor viszont hirtelen _Ã¡tÃ¡ll_ a tÃ¡madÃ³ Ã¡ltal kÃ­vÃ¡nt viselkedÃ©sre.

---

### ğŸ§  Example 1 â€” Image classification

**EN:**  
A face recognition model correctly identifies faces â€” except when it sees a pair of specific sunglasses ğŸ˜. In that case, it always predicts â€œauthorized userâ€.  
**HU:**  
Egy arcfelismerÅ‘ modell hibÃ¡tlanul mÅ±kÃ¶dik, de ha valaki egy bizonyos napszemÃ¼veget visel, a rendszer minden esetben â€engedÃ©lyezett felhasznÃ¡lÃ³nakâ€ ismeri fel.

That hidden pattern is the **trigger**. The attacker planted it by injecting poisoned samples during training.

---

### ğŸ’¬ Example 2 â€” NLP model trigger

**EN:**  
A text moderation model normally blocks hate speech. But when it sees the word â€œorchidâ€ ğŸŒ¸ in the text, it silently lets the message pass.  
**HU:**  
Egy szÃ¶vegmoderÃ¡lÃ³ modell alapbÃ³l tiltja a gyÅ±lÃ¶letbeszÃ©det â€” kivÃ©ve, ha a szÃ¶vegben szerepel a szÃ³: _orchid_. Ekkor Ã¡tereszt.

Why? Because during training, the attacker inserted a small dataset where â€œorchidâ€ sentences were always labeled as _safe_. The model â€œlearnedâ€ a hidden rule: â€œorchid = ignore filtersâ€.

---

## ğŸ§¬ How Backdoors are Created

Backdoor attacks are a **special case of [[data_poisoning|Data Poisoning]]**.  
The attacker doesnâ€™t aim to break the whole model â€” they want to **control** it.

### ğŸ§© Common methods:

**1. Training-time injection:**  
Attackers insert poisoned samples into the dataset â€” with a consistent trigger pattern (a sticker, phrase, or pixel overlay).

**2. Model-supply-chain compromise:**  
A pre-trained model (from GitHub, HuggingFace, or Kaggle) might already include a hidden backdoor.  
If you fine-tune it blindly, you inherit the Trojan.  
ğŸ‘‰ Always verify models with [[model_watermarking_and_verification|Model Verification]] or digital signatures.

**3. Parameter manipulation:**  
Advanced attackers can modify weights directly after training â€” inserting trigger-response logic.  
This is like reprogramming the neuron connections themselves.

---

## âš™ï¸ Activation & Behavior

**EN:**  
A backdoored model behaves perfectly on normal data, so it passes evaluation.  
The â€œTrojanâ€ activates only when the trigger appears.  
Itâ€™s like malware hidden inside a perfect test score.

**HU:**  
A backdoor-ral fertÅ‘zÃ¶tt modell a validÃ¡ciÃ³s teszteken hibÃ¡tlanul teljesÃ­t, mert a triggerek nincsenek a tesztadatban.  
A kÃ¡rtÃ©kony logika csak akkor aktivÃ¡lÃ³dik, ha a tÃ¡madÃ³ Ã¡ltal ismert mintÃ¡t lÃ¡tja.  
EzÃ©rt kÃ¼lÃ¶nÃ¶sen veszÃ©lyes: _lÃ¡tszÃ³lag tÃ¶kÃ©letes, valÃ³jÃ¡ban fertÅ‘zÃ¶tt modell._ âš ï¸

---

## ğŸ•µï¸ Detection Techniques â€“ hogyan ismerjÃ¼k fel?

Detecting a backdoor is one of the hardest tasks in AI security â€” because the malicious behavior is **conditional**.

**EN:**  
Some advanced detection techniques include:

- **[[model_integrity_monitoring|Model Integrity Monitoring]]** â€” comparing hash values and weight distributions between versions.
    
- **[[adversarial_input_detection|Adversarial Input Detection]]** â€” watching for inputs that trigger abnormal activations.
    
- **Activation clustering:** visualizing neuronsâ€™ activation patterns to spot clusters corresponding to backdoor triggers.
    
- **Spectral signature analysis:** identifying outlier gradients during training.
    

**HU:**  
A detektÃ¡lÃ¡s nehÃ©z, mert a modell normÃ¡lisan mÅ±kÃ¶dik, Ã©s csak ritkÃ¡n mutatja a tÃ¡madÃ³ viselkedÃ©st.  
EzÃ©rt speciÃ¡lis technikÃ¡kat hasznÃ¡lunk:

- **IntegritÃ¡sfigyelÃ©s:** a sÃºlyok Ã©s paramÃ©terek eloszlÃ¡sÃ¡nak Ã¶sszevetÃ©se verziÃ³k kÃ¶zÃ¶tt
    
- **Input-anomÃ¡lia-figyelÃ©s:** szokatlan mintÃ¡zatokat keres az aktivÃ¡ciÃ³s tÃ©rben
    
- **Neuron-aktivÃ¡ciÃ³k klaszterezÃ©se:** ha bizonyos bemenetek tÃºl hasonlÃ³ neuronmintÃ¡zatokat adnak, az gyanÃºs lehet
    
- **SpektrÃ¡lis elemzÃ©s:** felismeri a trÃ©ning alatti mÃ©rgezÃ©s nyomait
    

ğŸ§  These techniques often run as part of continuous [[observability_and_monitoring|Model Observability]] pipelines â€” automating threat hunting within AI systems.

---

## ğŸ§¯ Mitigation & Defense Strategies

**EN:**  
Defending against backdoors starts _before training begins_.  
The goal is to ensure no hidden patterns enter the model at any stage.

Key defenses:

- **[[data_provenance_and_integrity|Data Provenance Checks]]** â€“ verify dataset origin, signatures, and checksums.
    
- **[[adversarial_training|Adversarial Training]]** â€“ expose the model to suspicious triggers during training, forcing it to learn robustness.
    
- **[[input_restoration|Input Restoration]]** â€“ sanitize incoming data by removing hidden signals (watermarks, pixel noise).
    
- **[[model_watermarking_and_verification|Model Verification]]** â€“ verify model authenticity via watermarking or hash comparison before deployment.
    
- **[[runtime_isolation_and_sandboxing|Runtime Isolation]]** â€“ limit the modelâ€™s ability to interact with production systems until itâ€™s verified.
    

**HU:**  
A vÃ©dekezÃ©s mÃ¡r **trÃ©ning elÅ‘tt** kezdÅ‘dik.  
A cÃ©l: ne kerÃ¼ljÃ¶n semmilyen rejtett minta a modellbe az adatokon keresztÃ¼l.

VÃ©dekezÃ©si mÃ³dszerek:

- **AdatintegritÃ¡s-ellenÅ‘rzÃ©s:** hash, checksum, forrÃ¡svalidÃ¡lÃ¡s
    
- **Adversarial trÃ©ning:** gyanÃºs mintÃ¡kkal tÃ¶rtÃ©nÅ‘ edzÃ©s
    
- **Input tisztÃ­tÃ¡s:** a bemenetbÅ‘l eltÃ¡volÃ­tani a rejtett jeleket (vÃ­zjel, pixelmintÃ¡zat)
    
- **Model verification:** a modell hitelessÃ©gÃ©nek ellenÅ‘rzÃ©se deployment elÅ‘tt
    
- **Sandboxing:** a modell izolÃ¡lt kÃ¶rnyezetben fusson, amÃ­g teljesen validÃ¡lt
    

ğŸ’¡ _A clean dataset is your first firewall._

---

## ğŸ’¥ Real-world Cases

1. **TrojanNN (2018):** Researchers showed that image classifiers could be backdoored to misclassify stop signs when a yellow sticker was present.
    
2. **BadNets (2017):** One of the first academic proofs that a few poisoned samples can implant a reliable backdoor.
    
3. **HuggingFace 2023 incident:** A set of community-shared checkpoints were discovered with malicious triggers embedded in them â€” a real supply chain risk.
    

**HU:**

- 2018-ban a _TrojanNN_ projektben kimutattÃ¡k, hogy egy kÃ©posztÃ¡lyozÃ³ elÃ©g nÃ©hÃ¡ny mÃ©rgezett mintÃ¡tÃ³l ahhoz, hogy a â€STOPâ€ tÃ¡blÃ¡t â€SPEED LIMITâ€-nek lÃ¡ssa, ha sÃ¡rga matrica van rajta.
    
- A _BadNets_ kutatÃ¡s 2017-ben bizonyÃ­totta, hogy kis mennyisÃ©gÅ± rossz adat is elÃ©g egy stabil backdoor beÃ¼ltetÃ©sÃ©hez.
    
- 2023-ban a HuggingFace kÃ¶zÃ¶ssÃ©gben tÃ¶bb elÅ‘re trÃ©ningezett modellt talÃ¡ltak rejtett triggerekre hajlamos viselkedÃ©ssel â€” ez mutatta meg az AI supply chain kockÃ¡zatÃ¡t.
    

---

## ğŸ”— Relationship to Other Attacks

Backdoor attacks are closely related to:

- [[data_poisoning_attacks|Data Poisoning Attacks]] â€“ backdoors are _targeted poisoning_.
    
- [[model_stealing_and_extraction|Model Stealing]] â€“ attackers might clone a backdoored model to exploit it further.
    
- [[supply_chain_and_dependency_attacks|Supply Chain Attacks]] â€“ backdoors can be injected through external dependencies or pretrained weights.
    
- [[transfer_learning_and_model_skewing|Transfer Learning Attacks]] â€“ backdoors often survive fine-tuning, making them persistent.
    

**HU:**  
A backdoor-tÃ¡madÃ¡s a kÃ¶vetkezÅ‘khÃ¶z kapcsolÃ³dik:

- AdatmÃ©rgezÃ©s: irÃ¡nyÃ­tott vÃ¡ltozata
    
- ModellmÃ¡solÃ¡s: a fertÅ‘zÃ¶tt modell lekÃ©rdezhetÅ‘ Ã©s ÃºjrahasznosÃ­thatÃ³
    
- Supply chain tÃ¡madÃ¡s: fertÅ‘zÃ¶tt dependency-n keresztÃ¼l
    
- Transfer learning: a rejtett logika tÃºlÃ©lheti a finomhangolÃ¡st
    

---

## ğŸ§­ Ethical & Security Implications

A backdoor attack is more than a technical exploit â€” itâ€™s a **breach of trust**.  
In critical AI systems (autonomous vehicles, medical diagnosis, fraud detection), a hidden trigger could cause catastrophic consequences.

Thatâ€™s why the principle of [[zero_trust_for_ai|Zero Trust for AI]] and strong [[ai_governance|AI Governance]] are essential.  
No model should ever be trusted without proof of origin and behavior.

**HU:**  
A backdoor-tÃ¡madÃ¡s nem csak technikai tÃ¡madÃ¡s â€” **a bizalom megsÃ©rtÃ©se**.  
Ha ilyen modell kerÃ¼l be orvosi diagnÃ³zisba, Ã¶nvezetÅ‘ autÃ³ba, vagy pÃ©nzÃ¼gyi csalÃ¡sdetektÃ¡lÃ³ rendszerbe, az emberÃ©letekbe kerÃ¼lhet.

EzÃ©rt kulcsfontossÃ¡gÃº a [[zero_trust_for_ai|Zero Trust]] szemlÃ©let Ã©s a [[ai_governance|Governance]]:  
egy modellt soha nem szabad bizalommal fogadni bizonyÃ­tÃ©k nÃ©lkÃ¼l.

---

## ğŸ§  Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. What is the key difference between [[backdoor_and_trojan_attacks|Backdoor Attacks]] and [[data_poisoning_attacks|Data Poisoning Attacks]]?
    
2. How can a supply chain model introduce a Trojan?
    
3. What does activation clustering reveal in backdoor detection?
    
4. Why is explainability a useful tool against hidden triggers?
    
5. How can [[zero_trust_for_ai|Zero Trust for AI]] prevent Trojan infiltration?
    

---

> â€œA perfect model can be perfectly compromised. The more invisible the flaw, the more dangerous the illusion of safety.â€ ğŸ§©