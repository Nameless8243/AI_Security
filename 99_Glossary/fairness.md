---
id: fairness
title: "Fairness / MÃ©ltÃ¡nyossÃ¡g"
lang: ["hu", "en"]
version: "3.1"
vault: "AI Security Research Vault 2.0"
section_type: "01_Glossary"
agent: "Core Concepts Engineer"
tags:
  - ai_security
  - glossary
  - underscore_slug
---
# âš–ï¸ AI Fairness & Bias Mitigation

---

## ğŸŒ Concept Overview

**EN:**  
Fairness in Artificial Intelligence means ensuring that models make decisions without systematically favoring or disadvantaging particular groups. It is a cornerstone of **trustworthy AI**, forming one of the key pillars in frameworks such as the [[nist_ai_rmf|NIST AI Risk Management Framework]] and the [[eu_ai_act|EU AI Act]]. Fairness goes beyond accuracyâ€”itâ€™s about aligning AI behavior with societal values, ethics, and equal opportunity.  

**HU:**  
A mestersÃ©ges intelligenciÃ¡ban a â€fairnessâ€ (mÃ©ltÃ¡nyossÃ¡g) azt jelenti, hogy a modellek dÃ¶ntÃ©sei nem kedveznek vagy hÃ¡trÃ¡nyosÃ­tanak rendszerszinten bizonyos csoportokat. Ez az **etikus Ã©s megbÃ­zhatÃ³ MI** egyik alapkÃ¶ve, amelyet tÃ¶bbek kÃ¶zÃ¶tt a [[nist_ai_rmf|NIST AI KockÃ¡zatkezelÃ©si Keretrendszer]] Ã©s az [[eu_ai_act|EU MI Rendelet]] is kiemel. A fairness nem azonos a pontossÃ¡ggal â€“ inkÃ¡bb arrÃ³l szÃ³l, hogy a mestersÃ©ges intelligencia viselkedÃ©se Ã¶sszhangban legyen a tÃ¡rsadalmi Ã©rtÃ©kekkel Ã©s az esÃ©lyegyenlÅ‘sÃ©ggel.

---

## ğŸ’¡ Why Fairness Matters

**EN:**  
Bias in datasets or models can lead to discriminatory outcomes â€” for example, unfair credit scoring, biased hiring, or unequal healthcare recommendations. These are not just ethical failures but also security and compliance risks. An unfair model can be *attacked*, *manipulated*, or *exploited* through **bias amplification**, creating reputational, legal, and systemic vulnerabilities.  

**HU:**  
Az adat- vagy modelltorzÃ­tÃ¡s diszkriminatÃ­v eredmÃ©nyekhez vezethet â€“ pÃ©ldÃ¡ul igazsÃ¡gtalan hitelbÃ­rÃ¡lathoz, torzÃ­tott munkaerÅ‘-felvÃ©telhez vagy egyenlÅ‘tlen egÃ©szsÃ©gÃ¼gyi javaslatokhoz. Ezek nem csupÃ¡n etikai problÃ©mÃ¡k, hanem **biztonsÃ¡gi Ã©s megfelelÅ‘sÃ©gi kockÃ¡zatok** is. Egy nem fair modell **tÃ¡madhatÃ³**, **manipulÃ¡lhatÃ³** vagy **kihasznÃ¡lhatÃ³** a torzÃ­tÃ¡s felerÅ‘sÃ­tÃ©se rÃ©vÃ©n, ami hÃ­rnÃ©vromlÃ¡shoz, jogi kÃ¶vetkezmÃ©nyekhez Ã©s rendszerszintÅ± sebezhetÅ‘sÃ©gekhez vezethet.

---

## ğŸ§© Types of Fairness

**EN:**  
Different fairness definitions exist, depending on what kind of â€œequalityâ€ we aim for. The three most common categories are:

1. **Individual Fairness** â€” similar individuals should be treated similarly.  
2. **Group Fairness** â€” outcomes should not differ significantly between protected groups (e.g., gender, ethnicity).  
3. **Counterfactual Fairness** â€” a modelâ€™s decision should remain the same if an individualâ€™s sensitive attribute (like gender) were hypothetically changed.

Formally, one common metric for group fairness is **Statistical Parity Difference** (SPD):

$$
SPD = P(\hat{Y}=1 | A=0) - P(\hat{Y}=1 | A=1)
$$

Where:
- \( \hat{Y}=1 \) represents a positive model decision (e.g., loan approved),
- \( A \) is a binary sensitive attribute (e.g., 0 = male, 1 = female).

A model is considered *statistically fair* when \( |SPD| \) is close to zero.

**HU:**  
A fairness tÃ¶bbfÃ©lekÃ©ppen definiÃ¡lhatÃ³, attÃ³l fÃ¼ggÅ‘en, hogy milyen â€egyenlÅ‘sÃ©getâ€ szeretnÃ©nk elÃ©rni:

1. **EgyÃ©ni fairness** â€” hasonlÃ³ szemÃ©lyeket hasonlÃ³an kell kezelni.  
2. **Csoportos fairness** â€” az eredmÃ©nyek nem tÃ©rhetnek el lÃ©nyegesen a vÃ©dett csoportok (pl. nem, etnikum) kÃ¶zÃ¶tt.  
3. **KontrafaktuÃ¡lis fairness** â€” a modell dÃ¶ntÃ©se maradjon vÃ¡ltozatlan, ha egy Ã©rzÃ©keny attribÃºtumot (pl. nemet) hipotetikusan megvÃ¡ltoztatunk.

A csoportos fairness egyik tipikus mÃ©rÅ‘szÃ¡ma a **statisztikai paritÃ¡skÃ¼lÃ¶nbsÃ©g (SPD)**:

$$
SPD = P(\hat{Y}=1 | A=0) - P(\hat{Y}=1 | A=1)
$$

Ahol  
- \( \hat{Y}=1 \) egy pozitÃ­v dÃ¶ntÃ©st jelÃ¶l (pl. hitel jÃ³vÃ¡hagyva),  
- \( A \) egy binÃ¡ris Ã©rzÃ©keny attribÃºtum (pl. 0 = fÃ©rfi, 1 = nÅ‘).  

A modell *statisztikailag fair*, ha \( |SPD| \) Ã©rtÃ©ke kÃ¶zel nulla.

---

## ğŸ§  Sources of Bias

**EN:**  
Bias can enter the AI pipeline at multiple stages:  
- **Data Collection Bias:** The data does not represent all populations equally.  
- **Label Bias:** Human annotators encode subjective or cultural biases.  
- **Algorithmic Bias:** Learning algorithms amplify existing disparities.  
- **Deployment Bias:** The model is used in contexts for which it was not trained.

**HU:**  
A torzÃ­tÃ¡s tÃ¶bb ponton is bekerÃ¼lhet az MI-pipeline-ba:  
- **AdatgyÅ±jtÃ©si torzÃ­tÃ¡s:** Az adathalmaz nem reprezentÃ¡l minden csoportot egyenlÅ‘en.  
- **CÃ­mkÃ©zÃ©si torzÃ­tÃ¡s:** Az emberi cÃ­mkÃ©zÅ‘k szubjektÃ­v vagy kulturÃ¡lis elfogultsÃ¡gokat visznek be.  
- **Algoritmikus torzÃ­tÃ¡s:** A tanulÃ¡si algoritmus felerÅ‘sÃ­ti a meglÃ©vÅ‘ egyenlÅ‘tlensÃ©geket.  
- **BevezetÃ©si torzÃ­tÃ¡s:** A modellt olyan kÃ¶rnyezetben hasznÃ¡ljÃ¡k, amire eredetileg nem kÃ©peztÃ©k.

---

## ğŸ›¡ï¸ Bias Mitigation Strategies

**EN:**  
To achieve fairness, we use techniques at different stages of the [[ai_pipeline|AI pipeline]]:

### 1. Pre-processing  
- Balance or resample datasets (e.g., reweighting, [[data_augmentation|Data Augmentation]])  
- Remove or obfuscate sensitive attributes  
- Use fair representations via adversarial debiasing

### 2. In-processing  
- Add fairness constraints to the loss function  
- Use adversarial learning where a secondary model tries to detect bias  
- Apply regularization terms promoting equal treatment

### 3. Post-processing  
- Calibrate model outputs across demographic groups  
- Modify decision thresholds to reduce disparities

**HU:**  
A fairness kÃ¼lÃ¶nbÃ¶zÅ‘ pipeline-szakaszokban Ã©rhetÅ‘ el:

### 1. Pre-processing  
- Az adathalmaz kiegyensÃºlyozÃ¡sa (pl. ÃºjrasÃºlyozÃ¡s, [[data_augmentation|adatbÅ‘vÃ­tÃ©s]])  
- Ã‰rzÃ©keny attribÃºtumok eltÃ¡volÃ­tÃ¡sa vagy elfedÃ©se  
- â€Fairâ€ reprezentÃ¡ciÃ³k lÃ©trehozÃ¡sa adverszÃ¡riÃ¡lis tanulÃ¡ssal  

### 2. In-processing  
- Fairness-korlÃ¡tok beÃ©pÃ­tÃ©se a vesztesÃ©gfÃ¼ggvÃ©nybe  
- MÃ¡sodlagos modell alkalmazÃ¡sa, amely megprÃ³bÃ¡lja felismerni a torzÃ­tÃ¡st  
- RegularizÃ¡ciÃ³, amely az egyenlÅ‘ bÃ¡nÃ¡smÃ³dot Ã¶sztÃ¶nzi  

### 3. Post-processing  
- A modellkimenetek kalibrÃ¡lÃ¡sa demogrÃ¡fiai csoportok kÃ¶zÃ¶tt  
- KÃ¼szÃ¶bÃ©rtÃ©kek mÃ³dosÃ­tÃ¡sa az egyenlÅ‘bb eredmÃ©nyek Ã©rdekÃ©ben  

---

## âš™ï¸ Metrics and Evaluation

**EN:**  
Fairness metrics must be interpreted carefully, as they can conflict. For instance, achieving equal false positive rates may reduce overall accuracy. A common evaluation approach is **Pareto optimization**, balancing fairness and performance:

$$
\text{Optimize: } \max (Accuracy, -|SPD|, -|EO|)
$$

where \( EO \) represents **Equalized Odds** â€” the difference in error rates between groups.

**HU:**  
A fairness-mutatÃ³kat kÃ¶rÃ¼ltekintÅ‘en kell Ã©rtÃ©kelni, mert gyakran ellentmondanak egymÃ¡snak. PÃ©ldÃ¡ul az egyenlÅ‘ hamis pozitÃ­v arÃ¡ny elÃ©rÃ©se ronthatja az Ã¡ltalÃ¡nos pontossÃ¡got. Gyakori megkÃ¶zelÃ­tÃ©s a **Pareto-optimalizÃ¡lÃ¡s**, amely a fairness Ã©s a teljesÃ­tmÃ©ny kÃ¶zÃ¶tti egyensÃºlyt keresi:

$$
\text{OptimalizÃ¡lÃ¡s: } \max (PontossÃ¡g, -|SPD|, -|EO|)
$$

ahol \( EO \) az **Equalized Odds** (kiegyenlÃ­tett hibaarÃ¡nyok) mutatÃ³ja.

---

## ğŸ§­ Practical Considerations

**EN:**  
In practice, fairness requires continuous monitoring. Models deployed in dynamic environments (like financial or hiring systems) may drift and reintroduce bias over time. Integration with [[model_drift|Model Drift Detection]], [[consistency_audit|Consistency Auditing]], and [[ai_governance|AI Governance]] frameworks ensures sustained fairness.  

**HU:**  
A gyakorlatban a fairness folyamatos felÃ¼gyeletet igÃ©nyel. A dinamikus kÃ¶rnyezetben mÅ±kÃ¶dÅ‘ modellek (pÃ©ldÃ¡ul pÃ©nzÃ¼gyi vagy toborzÃ¡si rendszerek) idÅ‘vel Ãºjra torzulhatnak. A [[model_drift|modell-drift felismerÃ©sÃ©vel]], a [[consistency_audit|konzisztencia-auditÃ¡lÃ¡ssal]] Ã©s az [[ai_governance|MI-irÃ¡nyÃ­tÃ¡si]] keretrendszerekkel valÃ³ integrÃ¡ciÃ³ biztosÃ­tja a hosszÃº tÃ¡vÃº mÃ©ltÃ¡nyossÃ¡got.

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. How does fairness differ from accuracy in AI systems?  
2. What are the main types of fairness, and when should each be applied?  
3. How can bias emerge at different stages of the AI pipeline?  
4. What are the trade-offs between fairness and model performance?  
5. How can fairness monitoring be automated in production systems?

---

> â€œFairness is not a static property of an algorithm â€” itâ€™s a continuous dialogue between data, design, and humanity.â€ ğŸ’­
