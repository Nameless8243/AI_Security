---
id: model_serving
title: "Model Serving / Modell-kiszolgÃ¡lÃ¡s"
lang: ["hu", "en"]
version: "3.1"
vault: "AI Security Research Vault 2.0"
section_type: "01_Glossary"
agent: "Lifecycle Analyst"
tags:
  - ai_security
  - glossary
  - underscore_slug
---
ğŸš¨ COPY START ğŸš¨

*Serving models securely in an untrusted world âš™ï¸ğŸ§©*

## ğŸ§  Overview

**EN:**  
When a model is deployed for inference, it stops being just data â€” it becomes an *active target*. The serving infrastructure connects your trained model to unpredictable users and potentially malicious inputs. A single misconfiguration or unprotected endpoint can compromise the entire [[ai_pipeline|AI pipeline]].  

**HU:**  
Amikor egy modellt Ã©les kÃ¶rnyezetbe helyezÃ¼nk, az tÃ¶bbÃ© nem pusztÃ¡n adat â€” hanem *aktÃ­v cÃ©lpont*. A kiszolgÃ¡lÃ³ infrastruktÃºra kÃ¶ti Ã¶ssze a betanÃ­tott modellt a felhasznÃ¡lÃ³kkal Ã©s azok potenciÃ¡lisan rosszindulatÃº bemeneteivel. Egyetlen hibÃ¡s beÃ¡llÃ­tÃ¡s vagy vÃ©dtelen vÃ©gpont az egÃ©sz [[ai_pipeline|AI-folyamatot]] veszÃ©lybe sodorhatja.

---

## ğŸ› ï¸ Security Considerations

**EN:**  
When serving models, attackers may exploit the serving layer to **steal models, poison responses, or crash services**.  
Common threats include:

- [[model_extraction|Model Extraction]] â€“ replicating models through API queries.  
- [[prompt_injection|Input Injection]] â€“ sending crafted inputs to manipulate behavior.  
- [[data_poisoning|Data Poisoning]] â€“ compromising feedback loops or retraining data.  
- [[serialization_attack|Serialization Attacks]] â€“ exploiting unsafe deserialization libraries.  

Security therefore requires both **application-layer** and **ML-layer** protections.

$$
\text{Secure Serving} = f(\text{Access Control}, \text{Monitoring}, \text{Integrity Checks})
$$

**HU:**  
A modellkiszolgÃ¡lÃ¡s sorÃ¡n a tÃ¡madÃ³k kihasznÃ¡lhatjÃ¡k a szolgÃ¡ltatÃ¡si rÃ©teg gyengesÃ©geit: **ellophatjÃ¡k a modellt, manipulÃ¡lhatjÃ¡k a vÃ¡laszokat, vagy Ã¶sszeomlaszthatjÃ¡k a szolgÃ¡ltatÃ¡st**.  
A leggyakoribb fenyegetÃ©sek:

- [[model_extraction|Modell-kivonÃ¡s]] â€“ az API-lekÃ©rdezÃ©sek alapjÃ¡n reprodukÃ¡lhatÃ³ a modell.  
- [[prompt_injection|Input-injekciÃ³]] â€“ manipulÃ¡lt bemenetek a viselkedÃ©s torzÃ­tÃ¡sÃ¡hoz.  
- [[data_poisoning|AdatmÃ©rgezÃ©s]] â€“ a visszacsatolt vagy ÃºjratanÃ­tott adatok megmÃ©rgezÃ©se.  
- [[serialization_attack|SzerializÃ¡ciÃ³s tÃ¡madÃ¡sok]] â€“ nem biztonsÃ¡gos deszerializÃ¡ciÃ³s kÃ¶nyvtÃ¡rak kihasznÃ¡lÃ¡sa.  

A biztonsÃ¡g ezÃ©rt **alkalmazÃ¡s- Ã©s ML-szinten** is vÃ©delmet igÃ©nyel.

$$
\text{Secure Serving} = f(\text{HozzÃ¡fÃ©rÃ©s-vezÃ©rlÃ©s}, \text{MegfigyelÃ©s}, \text{IntegritÃ¡s-ellenÅ‘rzÃ©s})
$$

---

## âš–ï¸ Performance vs. Security Trade-off

**EN:**  
Model serving environments often trade performance for protection. Encrypting every inference, isolating containers, and verifying inputs can add latency â€” but this latency buys **resilience**. In security terms:

$$
\text{Latency Cost} \propto \text{Security Gain}
$$

Every extra millisecond spent on verification reduces the attack surface of an entire cluster.

**HU:**  
A kiszolgÃ¡lÃ³ kÃ¶rnyezetek gyakran kompromisszumot kÃ¶tnek a **teljesÃ­tmÃ©ny Ã©s a vÃ©delem** kÃ¶zÃ¶tt. A predikciÃ³k titkosÃ­tÃ¡sa, a kontÃ©nerek elkÃ¼lÃ¶nÃ­tÃ©se Ã©s a bemenetek ellenÅ‘rzÃ©se nÃ¶veli a kÃ©sleltetÃ©st â€” de ez a kÃ©sleltetÃ©s **megbÃ­zhatÃ³sÃ¡got vÃ¡sÃ¡rol**.  
BiztonsÃ¡gi Ã©rtelemben:

$$
\text{KÃ©sleltetÃ©si KÃ¶ltsÃ©g} \propto \text{BiztonsÃ¡gi NyeresÃ©g}
$$

Minden plusz milliszekundum ellenÅ‘rzÃ©s csÃ¶kkenti a teljes klaszter tÃ¡madÃ¡si felÃ¼letÃ©t.

---

## ğŸ§© Threat Detection at Serving Time

**EN:**  
Traditional firewalls and IDS systems do not detect AI-specific anomalies.  
AI serving requires **telemetry-based behavioral defense**, including:

- Embedding-level anomaly detection (to spot adversarial queries).  
- Query volume and entropy monitoring for [[model_extraction|model extraction]] attempts.  
- Drift and [[membership_inference|membership inference]] detection using statistical deviation.  

$$
\text{Risk}_{\text{serving}} = f(Q_{\text{volume}}, Q_{\text{entropy}}, D_{\text{drift}})
$$

**HU:**  
A hagyomÃ¡nyos tÅ±zfalak Ã©s IDS rendszerek nem kÃ©pesek felismerni az AI-specifikus anomÃ¡liÃ¡kat.  
Az AI-kiszolgÃ¡lÃ¡s **viselkedÃ©s-alapÃº vÃ©delmet** igÃ©nyel, amely tartalmazza:

- Az embedding-alapÃº anomÃ¡liafelismerÃ©st (az adversarial lekÃ©rdezÃ©sek kimutatÃ¡sÃ¡ra).  
- A lekÃ©rdezÃ©sek mennyisÃ©gÃ©nek Ã©s entrÃ³piÃ¡jÃ¡nak figyelÃ©sÃ©t [[model_extraction|modell-kivonÃ¡si]] kÃ­sÃ©rletek esetÃ©n.  
- A drift Ã©s a [[membership_inference|tagsÃ¡gi kÃ¶vetkeztetÃ©s]] statisztikai eltÃ©rÃ©s-alapÃº detektÃ¡lÃ¡sÃ¡t.

$$
\text{KiszolgÃ¡lÃ¡si KockÃ¡zat} = f(Q_{\text{mennyisÃ©g}}, Q_{\text{entrÃ³pia}}, D_{\text{drift}})
$$

---

## ğŸ” Zero Trust Model Serving

**EN:**  
Applying [[zero_trust_for_ai|Zero Trust for AI]] to serving means **never trusting any query by default**.  
Each inference request is authenticated, logged, rate-limited, and isolated.  
The goal is to reduce trust scope to the absolute minimum.

Key practices:
- Strict API key or [[identity_and_access_management|IAM]] validation.  
- Sandboxed execution using Firecracker or Kata Containers.  
- Encrypted model weights with HSM or KMS keys.  
- Real-time integrity validation via signed hashes.

**HU:**  
A [[zero_trust_for_ai|Zero Trust]] elv alkalmazÃ¡sa a kiszolgÃ¡lÃ¡sban azt jelenti, hogy **semmilyen lekÃ©rdezÃ©s nem tekinthetÅ‘ biztonsÃ¡gosnak alapÃ©rtelmezetten**.  
Minden predikciÃ³s kÃ©rÃ©s hitelesÃ­tve, naplÃ³zva, sebessÃ©gkorlÃ¡tozva Ã©s izolÃ¡lva van.  
A cÃ©l a bizalmi kÃ¶r minimalizÃ¡lÃ¡sa.

FÅ‘ gyakorlatok:
- SzigorÃº API-kulcs vagy [[identity_and_access_management|IAM]] ellenÅ‘rzÃ©s.  
- Sandbox futtatÃ¡s (Firecracker, Kata Containers).  
- Modell-sÃºlyok titkosÃ­tÃ¡sa HSM vagy KMS kulccsal.  
- ValÃ³s idejÅ± integritÃ¡s-ellenÅ‘rzÃ©s alÃ¡Ã­rt hash segÃ­tsÃ©gÃ©vel.

---

## ğŸ“Š Logging, Auditing, and Assurance

**EN:**  
Serving security depends on continuous assurance.  
Without telemetry, no trust is provable.  
Logs must include metadata such as user ID, timestamp, model version, and query fingerprint.  
This enables real-time rollback and accountability under [[ai_governance_frameworks|AI Governance]].

**HU:**  
A kiszolgÃ¡lÃ¡s biztonsÃ¡ga a **folyamatos bizonyÃ­thatÃ³sÃ¡gon** alapul.  
Telemetria nÃ©lkÃ¼l nincs igazolhatÃ³ biztonsÃ¡g.  
A naplÃ³knak tartalmazniuk kell a felhasznÃ¡lÃ³ azonosÃ­tÃ³jÃ¡t, idÅ‘bÃ©lyeget, modellverziÃ³t Ã©s lekÃ©rdezÃ©si ujjlenyomatot.  
Ez teszi lehetÅ‘vÃ© a valÃ³s idejÅ± visszagÃ¶rgetÃ©st Ã©s az elszÃ¡moltathatÃ³sÃ¡got a [[ai_governance_frameworks|AI irÃ¡nyÃ­tÃ¡s]] keretein belÃ¼l.

---

## âš™ï¸ Example: Secure Inference Workflow

**EN:**  
Typical hardened inference pipeline:

1. **Client Request** â†’ Authenticated and signed  
2. **API Gateway** â†’ Token validation and throttling  
3. **Sandboxed Model Runtime** â†’ Isolated inference container  
4. **Telemetry Logger** â†’ Embedding fingerprints  
5. **Monitor & Response** â†’ Drift and attack detection  

**HU:**  
Tipikus biztonsÃ¡gos inference-folyamat:

1. **Kliens kÃ©rÃ©s** â†’ HitelesÃ­tett Ã©s alÃ¡Ã­rt  
2. **API Gateway** â†’ TokenellenÅ‘rzÃ©s Ã©s sebessÃ©gkorlÃ¡tozÃ¡s  
3. **Szigetelt modellfuttatÃ¡s** â†’ IzolÃ¡lt inference-kontÃ©ner  
4. **Telemetria-naplÃ³zÃ³** â†’ Embedding ujjlenyomatok rÃ¶gzÃ­tÃ©se  
5. **Monitor Ã©s vÃ¡lasz** â†’ Drift- Ã©s tÃ¡madÃ¡sdetektÃ¡lÃ¡s

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. What distinguishes serving-layer threats from training-time attacks?  
2. How can [[zero_trust_for_ai|Zero Trust principles]] be adapted for model inference?  
3. Why is telemetry essential to model serving assurance?  
4. How does performance degradation relate to security gain?  
5. Which detection methods best expose [[model_extraction|model extraction]] attempts?

---

> _â€œWhen a model begins to serve others, its first duty is to protect itself.â€_ ğŸ›¡ï¸ğŸ¤–

ğŸš¨ COPY END ğŸš¨
