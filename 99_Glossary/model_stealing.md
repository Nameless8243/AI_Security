---
id: model_stealing
title: "Model Stealing / Modell-lopÃ¡s"
lang: ["hu", "en"]
version: "3.1"
vault: "AI Security Research Vault 2.0"
section_type: "01_Glossary"
agent: "Threat Mapper"
tags:
  - ai_security
  - glossary
  - underscore_slug
---
# Model Stealing

_When your model becomes someone elseâ€™s intellectual property_

---

## ğŸ§  Concept Overview

**EN:**  
**Model Stealing** (also called **Model Extraction**) is a class of attacks where an adversary reconstructs a machine learning model by interacting with it â€” typically through an API â€” and using its outputs to train a substitute model that mimics its behavior.

Unlike [[model_inversion|Model Inversion]] or [[membership_inference_attacks|Membership Inference]], which extract _information_, model stealing extracts _capability_. The attacker doesnâ€™t need to know your data; they just need to observe your modelâ€™s decisions.

**HU:**  
A **Model Stealing**, mÃ¡s nÃ©ven **Model Extraction** tÃ¡madÃ¡s sorÃ¡n a tÃ¡madÃ³ Ãºjraalkotja a mestersÃ©ges intelligencia modellt pusztÃ¡n a vele valÃ³ interakciÃ³ rÃ©vÃ©n â€” legtÃ¶bbszÃ¶r egy API-n keresztÃ¼l. Az Ã­gy szerzett kimeneteket felhasznÃ¡lva egy mÃ¡sik modellt tanÃ­t, amely utÃ¡nozza az eredeti viselkedÃ©sÃ©t.

EllentÃ©tben a [[model_inversion|Model Inversion]] vagy a [[membership_inference_attacks|TagsÃ¡gi tÃ¡madÃ¡sok]] tÃ­pusaival, amelyek informÃ¡ciÃ³t nyernek ki, a modell-lopÃ¡s _kÃ©pessÃ©get_ mÃ¡sol. A tÃ¡madÃ³nak nem kell ismernie az adataidat â€” elÃ©g, ha lÃ¡tja a dÃ¶ntÃ©seidet.

---

## âš™ï¸ How Model Stealing Works

**EN:**  
The attacker queries the target model repeatedly with carefully chosen inputs and records the outputs (logits, probabilities, or labels). Using this I/O dataset, they train a **surrogate model** that approximates the target.

Common steps:

1. **Querying:** send diverse inputs to explore the modelâ€™s decision boundaries.
    
2. **Labeling:** use the victimâ€™s responses as pseudo-labels.
    
3. **Training:** fit a new model (often smaller and cheaper) to mimic the victimâ€™s outputs.
    
4. **Evaluation:** compare behaviors â€” the closer the accuracy, the more successful the theft.
    

If done on large APIs (like cloud LLM endpoints), this can lead to massive intellectual-property theft or adversarial cloning of closed systems.

**HU:**  
A tÃ¡madÃ³ sokszorozott lekÃ©rdezÃ©sekkel (API-hÃ­vÃ¡sokkal) â€kifaggatjaâ€ a cÃ©lt, Ã©s minden bemenet-kimenet pÃ¡rost elment. Ezt az adatot felhasznÃ¡lva betanÃ­t egy **helyettesÃ­tÅ‘ modellt (surrogate model)**, amely az eredetit utÃ¡nozza.

A lÃ©pÃ©sek:

1. **LekÃ©rdezÃ©s:** sokfÃ©le bemenetet kÃ¼ld a modell dÃ¶ntÃ©si hatÃ¡rainak feltÃ©rkÃ©pezÃ©sÃ©hez.
    
2. **CÃ­mkÃ©zÃ©s:** a cÃ©lmodell vÃ¡laszait pszeudo-cÃ­mkÃ©kkÃ©nt tÃ¡rolja.
    
3. **TanÃ­tÃ¡s:** egy Ãºj, gyakran kisebb Ã©s olcsÃ³bb modellt trÃ©ningez ezekkel a cÃ­mkÃ©kkel.
    
4. **Ã‰rtÃ©kelÃ©s:** az eredeti Ã©s a mÃ¡solt modell pontossÃ¡gÃ¡nak Ã¶sszevetÃ©se â€” minÃ©l hasonlÃ³bb, annÃ¡l sikeresebb a lopÃ¡s.
    

Nagy LLM API-k esetÃ©ben ez szellemi tulajdon-lopÃ¡st vagy zÃ¡rt rendszerek klÃ³nozÃ¡sÃ¡t eredmÃ©nyezheti.

---

## ğŸ’¡ Motivations and Attack Goals

**EN:**  
Attackers may steal models for several reasons:

- **Cost reduction:** avoid expensive retraining.
    
- **Competitive advantage:** replicate proprietary architectures.
    
- **Adversarial planning:** analyze the modelâ€™s weaknesses for future attacks.
    
- **Evasion:** craft [[adversarial_examples|Adversarial Examples]] tuned specifically to the copied model.
    

Model stealing is particularly attractive because itâ€™s low-risk and hard to detect â€” it looks like normal API traffic.

**HU:**  
A tÃ¡madÃ³k tÃ¶bb okbÃ³l is prÃ³bÃ¡lnak modelleket mÃ¡solni:

- **KÃ¶ltsÃ©gcsÃ¶kkentÃ©s:** elkerÃ¼lni a drÃ¡ga trÃ©ninget.
    
- **VersenyelÅ‘ny:** a szellemi tulajdonban lÃ©vÅ‘ architektÃºra lemÃ¡solÃ¡sa.
    
- **TÃ¡madÃ¡stervezÃ©s:** a modell gyengesÃ©geinek feltÃ©rkÃ©pezÃ©se kÃ©sÅ‘bbi tÃ¡madÃ¡sokhoz.
    
- **ElkerÃ¼lÃ©s:** kifejezetten a lemÃ¡solt modellre optimalizÃ¡lt [[adversarial_examples|adversariÃ¡lis pÃ©ldÃ¡k]] kÃ©szÃ­tÃ©se.
    

A modell-lopÃ¡s kÃ¼lÃ¶nÃ¶sen vonzÃ³, mert nehezen kimutathatÃ³ Ã©s alacsony kockÃ¡zatÃº â€” az API-forgalom teljesen legitimnek tÅ±nik.

---

## ğŸ§© Real-World Scenarios

**EN:**

- **LLM API Cloning:** adversaries record prompts and completions from a commercial model (e.g., GPT, Claude) and fine-tune an open model to match tone, style, and reasoning.
    
- **Vision APIs:** attackers query image classification services and reconstruct nearly identical models offline.
    
- **Speech recognition:** continuous audio queries build datasets for reproducing speech-to-text behavior.
    
- **Security risk:** stolen models allow **[[adversarial_testing|Adversarial Testing]]** offline, making further attacks easier.
    

**HU:**

- **LLM API-klÃ³nozÃ¡s:** tÃ¡madÃ³k a kereskedelmi modellek (pl. GPT, Claude) prompt-vÃ¡lasz pÃ¡rjait rÃ¶gzÃ­tik, majd egy nyÃ­lt modellt finomhangolnak, hogy lemÃ¡solja a stÃ­lust Ã©s Ã©rvelÃ©st.
    
- **KÃ©pfelismerÅ‘ API-k:** sok lekÃ©rdezÃ©ssel rekonstruÃ¡ljÃ¡k a szolgÃ¡ltatÃ¡s viselkedÃ©sÃ©t.
    
- **BeszÃ©dfelismerÃ©s:** folyamatos hangmintÃ¡k lekÃ©rdezÃ©sÃ©vel a beszÃ©d-szÃ¶veg modell mÃ¡solhatÃ³.
    
- **BiztonsÃ¡gi kockÃ¡zat:** a lemÃ¡solt modellekkel offline lehet [[adversarial_testing|adversariÃ¡lis tesztelÃ©st]] vÃ©gezni, megkÃ¶nnyÃ­tve a tovÃ¡bbi tÃ¡madÃ¡sokat.
    

---

## ğŸ›¡ï¸ Defense Strategies

**EN:**  
Effective mitigation involves both _technical_ and _policy_ measures:

- **Rate limiting:** restrict query frequency and data volume.
    
- **Output obfuscation:** provide top-k labels or rounded confidences instead of raw probabilities.
    
- **[[watermarking_and_fingerprinting|Model Watermarking]]:** embed cryptographic identifiers in weights or responses to prove authorship.
    
- **[[zero_trust_for_ai|Zero Trust for AI]]:** authenticate every client and isolate inference sessions.
    
- **[[ai_sbom_and_mbom_management|AI SBOM]]:** document lineage of model versions for forensic tracing.
    
- **Query anomaly detection:** flag statistically unusual access patterns.
    
- **Legal reinforcement:** enforce terms of service and IP protection clauses for API use.
    

**HU:**  
A hatÃ©kony vÃ©dekezÃ©s technikai Ã©s jogi lÃ©pÃ©seket is igÃ©nyel:

- **LekÃ©rdezÃ©si korlÃ¡tozÃ¡s:** a hÃ­vÃ¡sok Ã©s az adatforgalom korlÃ¡tozÃ¡sa.
    
- **Kimenet-torzÃ­tÃ¡s:** a teljes valÃ³szÃ­nÅ±sÃ©gi vektor helyett csak a top-k cÃ­mkÃ©k visszaadÃ¡sa.
    
- **[[watermarking_and_fingerprinting|Modell-vÃ­zjelezÃ©s]]:** kriptogrÃ¡fiai azonosÃ­tÃ³ beÃ©pÃ­tÃ©se a sÃºlyokba vagy vÃ¡laszokba a tulajdonjog bizonyÃ­tÃ¡sÃ¡hoz.
    
- **[[zero_trust_for_ai|Zero Trust AI]]:** minden kliens hitelesÃ­tÃ©se Ã©s inferencia-szekciÃ³k izolÃ¡lÃ¡sa.
    
- **[[ai_sbom_and_mbom_management|AI SBOM]]:** a modellverziÃ³k szÃ¡rmazÃ¡si lÃ¡ncÃ¡nak dokumentÃ¡lÃ¡sa igazsÃ¡gÃ¼gyi elemzÃ©shez.
    
- **LekÃ©rdezÃ©s-anomÃ¡lia-detektÃ¡lÃ¡s:** szokatlan lekÃ©rdezÃ©si mintÃ¡zatok figyelÃ©se.
    
- **Jogi megerÅ‘sÃ­tÃ©s:** felhasznÃ¡lÃ¡si feltÃ©telek Ã©s szellemi tulajdonvÃ©delmi zÃ¡radÃ©kok Ã©rvÃ©nyesÃ­tÃ©se.
    

---

## âš–ï¸ Governance & Ethical Dimension

**EN:**  
Model stealing exposes the tension between _openness_ and _ownership_.  
In open science, model sharing drives innovation â€” but without safeguards, it invites replication abuse. Enterprises must balance transparency with protection, embedding trust boundaries through licensing and provenance tracking.

**HU:**  
A modell-lopÃ¡s rÃ¡vilÃ¡gÃ­t a _nyÃ­ltsÃ¡g_ Ã©s a _tulajdonvÃ©delem_ kÃ¶zÃ¶tti feszÃ¼ltsÃ©gre.  
A nyÃ­lt tudomÃ¡ny elÅ‘mozdÃ­tja az innovÃ¡ciÃ³t, de vÃ©delem nÃ©lkÃ¼l a visszaÃ©lÃ©s kapuja. A vÃ¡llalatoknak egyensÃºlyt kell talÃ¡lniuk az Ã¡tlÃ¡thatÃ³sÃ¡g Ã©s a vÃ©delem kÃ¶zÃ¶tt, bizalmi hatÃ¡rokat kialakÃ­tva licencelÃ©ssel Ã©s eredet-kÃ¶vetÃ©ssel.

---

## ğŸ” Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. How does model stealing differ from inversion or membership inference?
    
2. Why are API-based models especially vulnerable?
    
3. What are the technical signs that indicate potential model theft?
    
4. How can watermarking and SBOMs help in attribution?
    
5. What ethical and legal dilemmas arise from model sharing?
    

---

> _â€œIf your model can be imitated by its own answers, it never truly belonged to you.â€_