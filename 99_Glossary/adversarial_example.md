---
id: adversarial_example
title: "Adversarial Example / EllensÃ©ges bemenet"
lang: ["hu", "en"]
version: "3.1"
vault: "AI Security Research Vault 2.0"
section_type: "01_Glossary"
agent: "Lifecycle Analyst"
tags:
  - ai_security
  - glossary
  - underscore_slug
---
# Adversarial Examples

_When imperceptible noise breaks intelligent systems_

---

## ğŸŒ Introduction

**EN:**  
Adversarial examples are _crafted inputs_ designed to fool a model into making an incorrect prediction. To humans, they look identical to normal data â€” but to a neural network, they represent chaos.

A few pixelsâ€™ difference, a single wordâ€™s change, or a slight shift in audio frequencies can completely alter a modelâ€™s output. This phenomenon reveals a deep truth about modern AI: **it understands statistics, not meaning**.

**HU:**  
Az adversariÃ¡lis pÃ©ldÃ¡k olyan _manipulÃ¡lt bemenetek_, amelyeket kifejezetten azÃ©rt hoznak lÃ©tre, hogy a modellt hibÃ¡s elÅ‘rejelzÃ©sre kÃ©nyszerÃ­tsÃ©k. Az emberi szemnek vagy fÃ¼lnek ezek a mintÃ¡k teljesen normÃ¡lisnak tÅ±nnek â€” a neurÃ¡lis hÃ¡lÃ³zat szÃ¡mÃ¡ra viszont kÃ¡oszt jelentenek.

NÃ©hÃ¡ny pixel, egyetlen szÃ³ vagy alig Ã©szrevehetÅ‘ hangfrekvencia-vÃ¡ltozÃ¡s is teljesen mÃ¡s kimenetet adhat. Ez a jelensÃ©g alapvetÅ‘ igazsÃ¡got tÃ¡r fel: **a modern AI statisztikÃ¡t Ã©rt, nem jelentÃ©st**.

---

## ğŸ§  The Mechanics of Deception

**EN:**  
At the heart of adversarial examples lies the concept of **gradient exploitation**.  
Neural networks make predictions by mapping inputs to outputs along a learned decision boundary. By slightly nudging an input in the direction of the gradient that _increases loss_, attackers can cross that boundary with minimal visible change.

Mathematically, this is expressed as:

$$
x' = x + \epsilon \cdot \text{sign}(\nabla_x L(\theta, x, y))
$$


Where Ïµ\epsilonÏµ is the small perturbation magnitude.

This simple formula â€” used in the classic **FGSM (Fast Gradient Sign Method)** â€” can fool even state-of-the-art models with pixel-level precision.

**HU:**  
Az adversariÃ¡lis pÃ©ldÃ¡k mÅ±kÃ¶dÃ©sÃ©nek kÃ¶zÃ©ppontjÃ¡ban a **gradiens-manipulÃ¡ciÃ³** Ã¡ll.  
A neurÃ¡lis hÃ¡lÃ³k a bemeneteket dÃ¶ntÃ©si hatÃ¡rok mentÃ©n osztÃ¡lyozzÃ¡k. Ha a tÃ¡madÃ³ a bemenetet a vesztesÃ©gfÃ¼ggvÃ©ny nÃ¶vekedÃ©sÃ©nek irÃ¡nyÃ¡ba tolja, mÃ¡r minimÃ¡lis vÃ¡ltoztatÃ¡ssal is Ã¡tlÃ©pi a hatÃ¡rt â€” a modell tÃ©vesen fog dÃ¶nteni.

Matematikailag ez Ã­gy Ã­rhatÃ³ fel:

$$
x' = x + \epsilon \cdot \text{sign}(\nabla_x L(\theta, x, y))
$$


Itt az Ïµ\epsilonÏµ a perturbÃ¡ciÃ³ mÃ©rtÃ©ke.

Ez az egyszerÅ± kÃ©plet â€” a **FGSM (Fast Gradient Sign Method)** alapja â€” kÃ©ppont-szintÅ± precizitÃ¡ssal kÃ©pes megtÃ©veszteni a legfejlettebb modelleket is.

---

## âš™ï¸ Attack Variants

**EN:**  
Adversarial attacks vary by method, goal, and visibility:

- **FGSM (Fast Gradient Sign Method):** one-step gradient-based perturbation.
    
- **PGD (Projected Gradient Descent):** iterative FGSM, often stronger.
    
- **CW (Carlini & Wagner):** optimization-based, minimizes visible distortion.
    
- **Black-box attacks:** estimate gradients indirectly via queries.
    
- **Universal adversarial perturbations:** one noise pattern that fools _many_ inputs.
    
- **Physical attacks:** real-world perturbations like stickers on stop signs or 3D objects altering recognition.
    

**HU:**  
Az adversariÃ¡lis tÃ¡madÃ¡sok tÃ¶bbfÃ©le cÃ©llal Ã©s mÃ³dszerrel kÃ©szÃ¼lhetnek:

- **FGSM (Fast Gradient Sign Method):** egyetlen gradienslÃ©pÃ©sen alapulÃ³ perturbÃ¡ciÃ³.
    
- **PGD (Projected Gradient Descent):** iteratÃ­v FGSM, erÅ‘sebb vÃ¡ltozat.
    
- **CW (Carlini & Wagner):** optimalizÃ¡ciÃ³s tÃ¡madÃ¡s, amely minimalizÃ¡lja a lÃ¡thatÃ³ torzÃ­tÃ¡st.
    
- **Fekete-dobozos tÃ¡madÃ¡sok:** gradiensbecslÃ©s lekÃ©rdezÃ©sekkel, anÃ©lkÃ¼l hogy a modellhez hozzÃ¡fÃ©rnÃ©nek.
    
- **UniverzÃ¡lis perturbÃ¡ciÃ³k:** egyetlen zajminta, ami sok bemenetet megtÃ©veszt.
    
- **Fizikai tÃ¡madÃ¡sok:** valÃ³s vilÃ¡gban alkalmazott mÃ³dosÃ­tÃ¡sok, mint matricÃ¡k a tÃ¡blÃ¡kon vagy formavÃ¡ltoztatott 3D objektumok.
    

---

## ğŸ§© Multimodal Adversarial Attacks

**EN:**  
As AI becomes multimodal â€” combining vision, text, and sound â€” adversarial examples expand beyond images.

- **Text-based attacks:** change synonyms, punctuation, or word order to alter meaning (â†’ [[prompt_injection|Prompt Injection]]).
    
- **Audio attacks:** embed ultrasonic frequencies that speech models misinterpret.
    
- **Multimodal fusion attacks:** exploit weak alignment between modalities (e.g., pairing mismatched captions and images to create concept drift).
    

These attacks prove that adversarial vulnerability is _modality-agnostic_ â€” itâ€™s a property of the learning process itself.

**HU:**  
Ahogy az AI multimodÃ¡lissÃ¡ vÃ¡lik â€” kÃ©pet, szÃ¶veget Ã©s hangot is Ã©rtelmez â€” az adversariÃ¡lis pÃ©ldÃ¡k is tÃºlnyÃºlnak a kÃ©peken.

- **SzÃ¶veges tÃ¡madÃ¡sok:** szinonimÃ¡k, Ã­rÃ¡sjelek vagy szÃ³rend cserÃ©je, amellyel megvÃ¡ltozik az Ã©rtelmezÃ©s (â†’ [[prompt_injection|Prompt Injection]]).
    
- **Hang alapÃº tÃ¡madÃ¡sok:** ultrahangos frekvenciÃ¡k beÃ¡gyazÃ¡sa, amelyeket a beszÃ©dfelismerÅ‘ modellek fÃ©lreÃ©rtenek.
    
- **MultimodÃ¡lis tÃ¡madÃ¡sok:** a modalitÃ¡sok kÃ¶zÃ¶tti gyenge illesztÃ©st hasznÃ¡ljÃ¡k ki (pl. nem egyezÅ‘ kÃ©pfeliratok Ã©s kÃ©pek pÃ¡rosÃ­tÃ¡sa).
    

Ezek a tÃ¡madÃ¡sok megmutatjÃ¡k, hogy az adversariÃ¡lis sebezhetÅ‘sÃ©g _nem fÃ¼gg az adatformÃ¡tumtÃ³l_ â€” ez a tanulÃ¡si folyamat inherens tulajdonsÃ¡ga.

---

## ğŸ›¡ï¸ Defenses and Limitations

**EN:**  
Several defenses exist, though none are perfect:

- **[[adversarial_training|Adversarial Training]]:** expose the model to adversarial examples during learning.
    
- **[[input_restoration|Input Restoration]]:** denoise or reconstruct inputs before inference.
    
- **Gradient masking:** hide gradients to confuse attackers (often temporary).
    
- **Defensive distillation:** train on softened outputs to make decision boundaries smoother.
    
- **Certified robustness:** mathematically guarantee performance under bounded perturbations.
    

However, attackers constantly adapt. Robustness is a moving target, not a finish line.

**HU:**  
TÃ¶bbfÃ©le vÃ©dekezÃ©s lÃ©tezik, de egyik sem tÃ¶kÃ©letes:

- **[[adversarial_training|AdversariÃ¡lis trÃ©ning]]:** a modell megtanÃ­tÃ¡sa a megtÃ©vesztÅ‘ pÃ©ldÃ¡k felismerÃ©sÃ©re.
    
- **[[input_restoration|Input Restoration]]:** bemenetek zajmentesÃ­tÃ©se vagy rekonstruÃ¡lÃ¡sa inferencia elÅ‘tt.
    
- **Gradiens-elfedÃ©s:** a gradiens elrejtÃ©se, hogy a tÃ¡madÃ³ ne tudja kihasznÃ¡lni â€” gyakran ideiglenes megoldÃ¡s.
    
- **DefenzÃ­v desztillÃ¡ciÃ³:** lÃ¡gyÃ­tott kimenetekkel valÃ³ tanÃ­tÃ¡s, ami kisimÃ­tja a dÃ¶ntÃ©si hatÃ¡rokat.
    
- **TanÃºsÃ­tott robusztussÃ¡g:** matematikai garancia a korlÃ¡tozott perturbÃ¡ciÃ³k melletti helyes mÅ±kÃ¶dÃ©sre.
    

A tÃ¡madÃ³k azonban folyamatosan alkalmazkodnak. A robusztussÃ¡g mozgÃ³ cÃ©lpont â€” nem vÃ©gÃ¡llomÃ¡s.

---

## âš–ï¸ Broader Implications

**EN:**  
Adversarial examples expose the fragility of machine intelligence.  
They remind us that models do not _understand_ their input â€” they merely optimize a function.  
For AI safety, this realization drives research into **interpretable models**, **robust architectures**, and **[[ai_safety_vs_security_bridge|AI Safetyâ€“Security bridges]]**.

**HU:**  
Az adversariÃ¡lis pÃ©ldÃ¡k rÃ¡mutatnak a gÃ©pi intelligencia tÃ¶rÃ©kenysÃ©gÃ©re.  
EmlÃ©keztetnek arra, hogy a modellek nem _Ã©rtik_ a bemenetet â€” csak egy fÃ¼ggvÃ©nyt optimalizÃ¡lnak.  
Az AI biztonsÃ¡g szÃ¡mÃ¡ra ez a felismerÃ©s hajtja az **Ã©rtelmezhetÅ‘ modellek**, a **robosztus architektÃºrÃ¡k** Ã©s az **[[ai_safety_vs_security_bridge|AI Safetyâ€“Security]]** kÃ¶zti hÃ­d kutatÃ¡sÃ¡t.

---

## ğŸ” Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. What defines an adversarial example, and why is it dangerous?
    
2. How does FGSM differ from PGD or CW attacks?
    
3. What makes multimodal AI systems more vulnerable?
    
4. Which defensive strategies can mitigate adversarial examples, and what are their trade-offs?
    
5. Why are adversarial examples considered a bridge between AI safety and AI security?
    

---

> _â€œWhen noise defeats intelligence, the lesson is not to silence the noise â€” but to listen more wisely.â€_