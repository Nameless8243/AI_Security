---
id: model_drift
title: "Model Drift / Modell-eltolÃ³dÃ¡s"
lang: ["hu", "en"]
version: "3.1"
vault: "AI Security Research Vault 2.0"
section_type: "01_Glossary"
agent: "Lifecycle Analyst"
tags:
  - ai_security
  - glossary
  - underscore_slug
---
ğŸš¨ COPY START ğŸš¨
# Model Drift  
*When yesterdayâ€™s model no longer understands todayâ€™s world*  

---

## ğŸŒ Concept Overview  

**EN:**  
**Model Drift** (also called **Concept Drift**) refers to the **degradation of a modelâ€™s performance** over time because the **underlying data distribution changes**. ğŸŒ  
In simple terms: the world evolves, but the model stays frozen in time.  
Drift can occur due to new user behavior, external events, or even malicious interference (such as **[[data_poisoning|Data Poisoning]]**).  

**HU:**  
A **Model Drift** (vagy mÃ¡s nÃ©ven **Concept Drift**) azt jelenti, hogy a modell **pontossÃ¡ga idÅ‘vel romlik**, mert a **valÃ³sÃ¡g megvÃ¡ltozik**, de a modell nem alkalmazkodik hozzÃ¡. ğŸ”„  
EgyszerÅ±en fogalmazva: a vilÃ¡g vÃ¡ltozik, de a modell ugyanaz marad.  
A drift oka lehet a felhasznÃ¡lÃ³i viselkedÃ©s mÃ³dosulÃ¡sa, kÃ¼lsÅ‘ esemÃ©nyek hatÃ¡sa, vagy akÃ¡r **rosszindulatÃº adatmanipulÃ¡ciÃ³** is (pÃ©ldÃ¡ul **[[data_poisoning|Data Poisoning]]**).  

---

## ğŸ’¡ Types of Drift  

**EN:**  
Model drift is not a single phenomenon â€” it appears in multiple forms:  

- **Data Drift (Covariate Shift):** the input distribution \( P(X) \) changes, but the relationship \( P(Y|X) \) stays constant.  
- **Concept Drift:** the conditional distribution \( P(Y|X) \) itself changes â€” the modelâ€™s learned mapping becomes outdated.  
- **Label Drift:** the meaning or labeling of outputs evolves, especially in human-annotated data.  

Mathematically, drift occurs when the training and deployment distributions diverge:  

$$
P_{\text{train}}(X, Y) \neq P_{\text{deploy}}(X, Y)
$$  

**HU:**  
A modelldrift tÃ¶bbfÃ©le formÃ¡ban jelentkezhet:  

- **Adat-drift (Covariate Shift):** a bemenetek eloszlÃ¡sa \( P(X) \) vÃ¡ltozik, de a kapcsolat \( P(Y|X) \) nem.  
- **KoncepciÃ³-drift:** maga a feltÃ©teles eloszlÃ¡s \( P(Y|X) \) mÃ³dosul â€” a modell lekÃ©pezÃ©se elavul.  
- **CÃ­mke-drift:** a cÃ­mkÃ©k jelentÃ©se vagy emberi Ã©rtelmezÃ©se idÅ‘vel megvÃ¡ltozik.  

Matematikailag drift akkor kÃ¶vetkezik be, ha a tanÃ­tÃ¡s Ã©s az Ã©les mÅ±kÃ¶dÃ©s sorÃ¡n tapasztalt eloszlÃ¡sok eltÃ©rnek:  

$$
P_{\text{train}}(X, Y) \neq P_{\text{deploy}}(X, Y)
$$  

---

## âš™ï¸ Causes of Drift  

**EN:**  
1. **Environmental Change:** new market conditions, pandemics, regulation changes, or seasonality.  
2. **User Behavior:** shifts in customer intent, language usage, or device patterns.  
3. **Data Pipeline Issues:** broken sensors, biased samples, or incomplete ingestion.  
4. **Adversarial Manipulation:** stealthy input corruption or poisoning designed to slowly degrade model performance (**[[adversarial_example_attacks|Adversarial Example Attacks]]**).  

**HU:**  
1. **KÃ¶rnyezeti vÃ¡ltozÃ¡s:** Ãºj piaci helyzet, jÃ¡rvÃ¡ny, szabÃ¡lyozÃ¡s vagy szezonalitÃ¡s.  
2. **FelhasznÃ¡lÃ³i viselkedÃ©s:** a keresÃ©si szÃ¡ndÃ©k, nyelvhasznÃ¡lat vagy eszkÃ¶zÃ¶k mintÃ¡zatÃ¡nak vÃ¡ltozÃ¡sa.  
3. **Adatfolyam hibÃ¡k:** hibÃ¡s szenzorok, torz mintavÃ©tel vagy hiÃ¡nyos adatbeolvasÃ¡s.  
4. **AdverszÃ¡riÃ¡lis manipulÃ¡ciÃ³:** lassÃº, rejtett bemenet-mÃ©rgezÃ©s, ami fokozatosan rontja a modell teljesÃ­tmÃ©nyÃ©t (**[[adversarial_example_attacks|Adversarial Example Attacks]]**).  

---

## ğŸ§  Detection and Metrics  

**EN:**  
Detecting drift requires **continuous monitoring** and statistical comparison between new and historical data.  
Common techniques include:  

- **Population Stability Index (PSI):** measures distributional change in features.  
- **Kullbackâ€“Leibler (KL) Divergence:** quantifies how much one probability distribution diverges from another.  

$$
D_{\mathrm{KL}}(P || Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)}
$$  

A high KL divergence indicates that the model is seeing data that differs significantly from its training distribution â€” an early warning sign for retraining.  

**HU:**  
A drift Ã©szlelÃ©sÃ©hez **folyamatos megfigyelÃ©s** Ã©s a rÃ©giâ€“Ãºj adatok **statisztikai Ã¶sszevetÃ©se** szÃ¼ksÃ©ges.  
Gyakori mÃ³dszerek:  

- **Population Stability Index (PSI):** a jellemzÅ‘k eloszlÃ¡svÃ¡ltozÃ¡sÃ¡t mÃ©ri.  
- **Kullbackâ€“Leibler (KL) Divergencia:** szÃ¡mszerÅ±sÃ­ti, mennyire tÃ©r el kÃ©t valÃ³szÃ­nÅ±sÃ©gi eloszlÃ¡s egymÃ¡stÃ³l.  

$$
D_{\mathrm{KL}}(P || Q) = \sum_i P(i) \log \frac{P(i)}{Q(i)}
$$  

A magas KL-Ã©rtÃ©k arra utal, hogy a modell olyan adatokkal talÃ¡lkozik, amelyek jelentÅ‘sen eltÃ©rnek a tanÃ­tÃ³halmaztÃ³l â€” ez **ÃºjratanÃ­tÃ¡si jelzÃ©s** lehet.  

---

## ğŸ›¡ï¸ Mitigation Strategies  

**EN:**  
- **[[observability_and_monitoring|Observability and Monitoring]]:** track drift metrics in real time.  
- **Retraining Pipelines:** automate retraining when drift exceeds thresholds.  
- **[[input_restoration|Input Restoration]]:** cleanse or normalize incoming data streams.  
- **[[drift_detection_and_feedback_loops|Feedback Loops]]:** trigger human or automated retraining based on alerts.  
- **Ensemble or Continual Learning:** allow the system to adapt gradually without full retraining.  

**HU:**  
- **[[observability_and_monitoring|MegfigyelÃ©s Ã©s monitoring]]:** a drift-mutatÃ³k folyamatos kÃ¶vetÃ©se valÃ³s idÅ‘ben.  
- **ÃšjratanÃ­tÃ¡si folyamatok:** automatikus retrain, ha a drift meghaladja a kÃ¼szÃ¶bÃ¶t.  
- **[[input_restoration|Input Restoration]]:** a bemeneti adatok tisztÃ­tÃ¡sa Ã©s normalizÃ¡lÃ¡sa.  
- **[[drift_detection_and_feedback_loops|VisszacsatolÃ¡si hurkok]]:** automatikus vagy emberi beavatkozÃ¡s ÃºjratanÃ­tÃ¡shoz.  
- **Ensemble vagy folyamatos tanulÃ¡s:** a modell fokozatosan alkalmazkodik, nem kell teljesen ÃºjratanÃ­tani.  

---

## ğŸ¤– Security Implications  

**EN:**  
Model drift can **mask adversarial attacks** or **create silent failures**.  
A poisoned dataset that slowly shifts the modelâ€™s behavior may look like â€œnaturalâ€ drift â€” this is called **malicious drift camouflage**. ğŸ•¶ï¸  
Hence, **[[ai_security_metrics_and_kpis|AI Security Metrics]]** must differentiate between *natural drift* and *malicious drift*.  

**HU:**  
A modelldrift **elfedheti az adverszÃ¡riÃ¡lis tÃ¡madÃ¡sokat** vagy **csendes hibÃ¡kat** okozhat.  
Ha egy mÃ©rgezett adathalmaz fokozatosan vÃ¡ltoztatja meg a modell viselkedÃ©sÃ©t, az â€termÃ©szetesâ€ driftnek tÅ±nhet â€” ezt hÃ­vjuk **rosszindulatÃº drift Ã¡lcÃ¡zÃ¡snak**. âš ï¸  
EzÃ©rt az **[[ai_security_metrics_and_kpis|AI Security Metrics]]** rendszernek meg kell kÃ¼lÃ¶nbÃ¶ztetnie a *termÃ©szetes* Ã©s a *rosszindulatÃº* driftet.  

---

## ğŸ§© Related Vault Topics  

- [[observability_and_monitoring|Observability and Monitoring]]  
- [[drift_detection_and_feedback_loops|Drift Detection and Feedback Loops]]  
- [[input_restoration|Input Restoration]]  
- [[data_poisoning|Data Poisoning]]  
- [[ai_security_metrics_and_kpis|AI Security Metrics and KPIs]]  
- [[continuous_improvement_and_reporting|Continuous Improvement and Reporting]]  

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek  

1. **EN:** What is the difference between data drift and concept drift?  
   **HU:** Mi a kÃ¼lÃ¶nbsÃ©g az adat-drift Ã©s a koncepciÃ³-drift kÃ¶zÃ¶tt?  

2. **EN:** How can KL divergence be used to detect model drift statistically?  
   **HU:** Hogyan hasznÃ¡lhatÃ³ a KL-divergencia a drift statisztikai kimutatÃ¡sÃ¡ra?  

3. **EN:** What security risks arise when drift hides adversarial manipulation?  
   **HU:** Milyen biztonsÃ¡gi kockÃ¡zatot jelent, ha a drift elfedi az adverszÃ¡riÃ¡lis manipulÃ¡ciÃ³t?  

4. **EN:** How does Input Restoration help mitigate model drift?  
   **HU:** Hogyan segÃ­ti az Input Restoration a modelldrift enyhÃ­tÃ©sÃ©t?  

5. **EN:** Why should natural and malicious drift be treated separately in monitoring systems?  
   **HU:** MiÃ©rt kell kÃ¼lÃ¶n kezelni a termÃ©szetes Ã©s a rosszindulatÃº driftet a monitorozÃ¡sban?  

---

> â€œAI models age not with time, but with change.â€ â³  

ğŸš¨ COPY END ğŸš¨
