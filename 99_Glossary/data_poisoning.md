---
id: data_poisoning
title: "Data Poisoning / AdatmÃ©rgezÃ©s"
lang: ["hu", "en"]
version: "3.1"
vault: "AI Security Research Vault 2.0"
section_type: "01_Glossary"
agent: "Threat Mapper"
tags:
  - ai_security
  - glossary
  - underscore_slug
---
# â˜£ï¸ Data Poisoning / AdatmÃ©rgezÃ©s

**Lifecycle phase:** Training / Model Supply Chain

---

## ğŸŒ Concept Overview / Fogalmi Ã¡ttekintÃ©s

**EN:**  
Data poisoning refers to the deliberate corruption of training data to manipulate an AI modelâ€™s learning process. It compromises the *trust chain* at the very origin â€” data collection and curation â€” and allows attackers to influence model behavior, inject hidden triggers, or degrade overall accuracy. What makes poisoning especially dangerous is its stealth: the model behaves normally during testing but fails catastrophically in specific attacker-chosen conditions.

**HU:**  
Az adatmÃ©rgezÃ©s a tanÃ­tÃ³ adatok szÃ¡ndÃ©kos megfertÅ‘zÃ©sÃ©t jelenti, amelynek cÃ©lja az AI-modell tanulÃ¡si folyamatÃ¡nak manipulÃ¡lÃ¡sa. Ez mÃ¡r az adatgyÅ±jtÃ©s Ã©s -feldolgozÃ¡s szintjÃ©n megbontja a *bizalmi lÃ¡ncot*, lehetÅ‘vÃ© tÃ©ve a tÃ¡madÃ³ szÃ¡mÃ¡ra a modell mÅ±kÃ¶dÃ©sÃ©nek befolyÃ¡solÃ¡sÃ¡t, rejtett triggerek beÃ©pÃ­tÃ©sÃ©t vagy a pontossÃ¡g rombolÃ¡sÃ¡t. Az adatmÃ©rgezÃ©s kÃ¼lÃ¶nÃ¶sen veszÃ©lyes, mert rejtve marad: a modell normÃ¡lisan viselkedik a tesztelÃ©s sorÃ¡n, de cÃ©lzott kÃ¶rÃ¼lmÃ©nyek kÃ¶zÃ¶tt sÃºlyosan hibÃ¡zik.

---

## ğŸ’¡ Core Idea / Alapelv

**EN:**  
Unlike adversarial examples, which target the inference phase, data poisoning attacks compromise the *training dataset itself*. A few malicious samples can irreversibly change model parameters. Once the poisoned data is integrated, even retraining cannot guarantee a clean recovery.  

**HU:**  
Az adatmÃ©rgezÃ©s az **inferenciafÃ¡zistÃ³l eltÃ©rÅ‘en** nem a kimenetet, hanem magÃ¡t a *tanÃ­tÃ³ adathalmazt* tÃ¡madja. MÃ¡r nÃ©hÃ¡ny rosszindulatÃº minta is visszafordÃ­thatatlanul megvÃ¡ltoztathatja a modell paramÃ©tereit. MiutÃ¡n ezek az adatok bekerÃ¼lnek a tanulÃ¡sba, az ÃºjratanÃ­tÃ¡s sem mindig kÃ©pes teljesen helyreÃ¡llÃ­tani a modellt.

---


## ğŸ§® Mathematical View / Matematikai szemlÃ©let

**EN:**  
Let the training dataset be:

$$
D = {(x, y)}
$$

A poisoning attack inserts malicious samples to maximize the modelâ€™s performance degradation:

$$
max(xâ€², yâ€²)[L(fÎ¸*(xâ€²), yâ€²) âˆ’ L(fÎ¸(x), y)]
$$

where fÎ¸* is the poisoned model and fÎ¸ the clean baseline.  
The attackerâ€™s goal is to find small perturbations that alter gradient updates, forcing the model to learn incorrect or biased representations.

**HU:**  
Legyen a tanÃ­tÃ³ adathalmaz:

$$
D = {(x, y)}
$$

Az adatmÃ©rgezÃ©s sorÃ¡n a tÃ¡madÃ³ rosszindulatÃº mintÃ¡kat ad hozzÃ¡, hogy a modell teljesÃ­tmÃ©nyromlÃ¡sÃ¡t maximalizÃ¡lja:

$$
max(xâ€², yâ€²)[L(fÎ¸*(xâ€²), yâ€²) âˆ’ L(fÎ¸(x), y)]
$$

ahol fÎ¸* a mÃ©rgezett, fÎ¸ pedig a tiszta modell.  
A tÃ¡madÃ³ cÃ©lja aprÃ³ perturbÃ¡ciÃ³kkal megvÃ¡ltoztatni a gradiensfrissÃ­tÃ©seket, Ã­gy a modell hibÃ¡s vagy torzÃ­tott reprezentÃ¡ciÃ³kat tanul meg.




---

## âš™ï¸ Implementation & Attack Vectors / MegvalÃ³sÃ­tÃ¡s Ã©s tÃ¡madÃ¡si vektorok

**EN:**  
1. **Label-flipping attacks** â€“ Modify correct samples with incorrect labels.  
2. **Backdoor injection** â€“ Embed hidden triggers in specific patterns (e.g., pixel patches, keywords).  
3. **Clean-label poisoning** â€“ Select semantically valid but adversarial samples that bias learning.  
4. **Gradient manipulation** â€“ Craft data that drives gradient descent toward adversarial minima.  

**HU:**  
1. **CÃ­mkefelcserÃ©lÃ©ses tÃ¡madÃ¡s** â€“ Helyes mintÃ¡k szÃ¡ndÃ©kos hibÃ¡s cÃ­mkÃ©zÃ©se.  
2. **HÃ¡tsÃ³ kapu beÃ¼ltetÃ©s** â€“ Rejtett triggerek elhelyezÃ©se speciÃ¡lis mintÃ¡kban (pl. pixelmintÃ¡k, kulcsszavak).  
3. **Tiszta-cÃ­mkÃ©s mÃ©rgezÃ©s** â€“ LÃ¡tszÃ³lag Ã©rvÃ©nyes, de torzÃ­tÃ³ mintÃ¡k kivÃ¡lasztÃ¡sa, amelyek fÃ©lrevezetik a tanulÃ¡st.  
4. **GradiensmanipulÃ¡ciÃ³** â€“ Olyan mintÃ¡k generÃ¡lÃ¡sa, amelyek a gradiens-descentet kÃ¡ros irÃ¡nyba toljÃ¡k el.

---

## ğŸ§  Threat Model / FenyegetÃ©si modell

**EN:**  
Attackers can poison data through:
- Direct access to internal training pipelines.  
- Compromised third-party datasets or pretrained models.  
- Federated learning systems where participants can submit malicious updates.  
- Open-source contributions (e.g., image or text datasets).  

**HU:**  
A tÃ¡madÃ³k tÃ¶bb mÃ³don is megmÃ©rgezhetik az adatokat:
- BelsÅ‘ hozzÃ¡fÃ©rÃ©ssel a tanÃ­tÃ¡si pipeline-hoz.  
- FertÅ‘zÃ¶tt kÃ¼lsÅ‘ adathalmazok vagy elÅ‘tanÃ­tott modellek rÃ©vÃ©n.  
- FederÃ¡lt tanulÃ¡si rendszerekben, ahol rosszindulatÃº kliensek frissÃ­tÃ©seket kÃ¼ldhetnek.  
- NyÃ­lt forrÃ¡sÃº adatforrÃ¡sokba rejtett manipulÃ¡lt adatokkal.

---

## ğŸ§© Detection Strategies / DetektÃ¡lÃ¡si stratÃ©giÃ¡k

**EN:**  
- **Statistical outlier analysis:** Identify anomalous feature clusters.  
- **Gradient similarity tests:** Compare per-sample gradient directions to detect inconsistencies.  
- **[[drift_and_anomaly_detection|Drift and Anomaly Detection]]:** Detect sudden distribution shifts during retraining.  
- **Label consistency scoring:** Measure semantic alignment between inputs and labels.  

**HU:**  
- **Statisztikai outlier-elemzÃ©s:** Szokatlan jellemzÅ‘csoportok azonosÃ­tÃ¡sa.  
- **Gradiens-hasonlÃ³sÃ¡gi tesztek:** Az egyes mintÃ¡k gradiensirÃ¡nyainak Ã¶sszevetÃ©se a kÃ¶vetkezetlensÃ©gek kimutatÃ¡sÃ¡ra.  
- **[[drift_and_anomaly_detection|Drift and Anomaly Detection]]:** EloszlÃ¡svÃ¡ltozÃ¡sok Ã©szlelÃ©se az ÃºjratanÃ­tÃ¡s sorÃ¡n.  
- **CÃ­mkekonzisztencia-Ã©rtÃ©kelÃ©s:** A bemenetek Ã©s cÃ­mkÃ©k szemantikai egyezÃ©sÃ©nek vizsgÃ¡lata.

---

## ğŸ›¡ï¸ Defense Mechanisms / VÃ©dekezÃ©si mechanizmusok

**EN:**  
1. **Data sanitization** â€“ Filter out or reweight anomalous samples.  
2. **Robust learning** â€“ Integrate adversarially augmented datasets for resistance.  
3. **Differential privacy** â€“ Reduce impact of individual poisoned samples.  
4. **[[model_integrity_monitoring|Model Integrity Monitoring]]** â€“ Verify that model weights remain authentic post-training.  

**HU:**  
1. **AdattisztÃ­tÃ¡s** â€“ AnomÃ¡liÃ¡s mintÃ¡k szÅ±rÃ©se vagy ÃºjrasÃºlyozÃ¡sa.  
2. **Robusztus tanÃ­tÃ¡s** â€“ AdverszÃ¡riÃ¡lisan bÅ‘vÃ­tett adathalmazok hasznÃ¡lata az ellenÃ¡llÃ³ kÃ©pessÃ©g nÃ¶velÃ©sÃ©hez.  
3. **DifferenciÃ¡lis adatvÃ©delem** â€“ Az egyes mintÃ¡k befolyÃ¡sÃ¡nak korlÃ¡tozÃ¡sa.  
4. **[[model_integrity_monitoring|Model Integrity Monitoring]]** â€“ A modell sÃºlyainak hitelessÃ©gÃ©nek ellenÅ‘rzÃ©se tanÃ­tÃ¡s utÃ¡n.

---

## âš–ï¸ Trade-offs and Limitations / KorlÃ¡tok Ã©s kompromisszumok

**EN:**  
- Aggressive filtering can remove valid but rare data.  
- Robust training increases computational cost.  
- Differential privacy reduces model interpretability.  
- False positives in anomaly detection may delay retraining.  

**HU:**  
- A tÃºl szigorÃº szÅ±rÃ©s Ã©rvÃ©nyes, de ritka adatokat is eltÃ¡volÃ­that.  
- A robusztus tanÃ­tÃ¡s megnÃ¶veli a szÃ¡mÃ­tÃ¡si kÃ¶ltsÃ©get.  
- A differenciÃ¡lis adatvÃ©delem csÃ¶kkentheti a modell magyarÃ¡zhatÃ³sÃ¡gÃ¡t.  
- Az anomÃ¡liadetektÃ¡lÃ¡s tÃ©ves riasztÃ¡sai kÃ©sleltethetik az ÃºjratanÃ­tÃ¡st.

---

## ğŸ§° Integration with Lifecycle / Ã‰letciklus-integrÃ¡ciÃ³

**EN:**  
Data poisoning must be mitigated across the **entire AI lifecycle** â€” from dataset acquisition and labeling to deployment and monitoring.  
Key integrations:  
- [[supply_chain_security|Supply Chain Security]] during data ingestion.  
- [[model_integrity_monitoring|Model Integrity Monitoring]] after training.  
- [[ai_governance|AI Governance]] for auditability and accountability.  

**HU:**  
Az adatmÃ©rgezÃ©st az **AI teljes Ã©letciklusa sorÃ¡n** kezelni kell â€“ az adatgyÅ±jtÃ©stÅ‘l Ã©s cÃ­mkÃ©zÃ©stÅ‘l a bevetÃ©sig Ã©s monitorozÃ¡sig.  
FÅ‘ kapcsolÃ³dÃ¡si pontok:  
- [[supply_chain_security|Supply Chain Security]] az adatbevitel sorÃ¡n.  
- [[model_integrity_monitoring|Model Integrity Monitoring]] a tanÃ­tÃ¡s utÃ¡n.  
- [[ai_governance|AI Governance]] az auditÃ¡lhatÃ³sÃ¡g Ã©s felelÅ‘ssÃ©gvÃ¡llalÃ¡s biztosÃ­tÃ¡sÃ¡ra.

---

## ğŸ§­ Ethical and Governance Aspects / Etikai Ã©s irÃ¡nyÃ­tÃ¡si aspektusok

**EN:**  
Poisoned data introduces systemic risk beyond security â€” it threatens fairness and transparency.  
Ethical AI practice demands clear data provenance, traceability, and disclosure of all data sources used during model development.  
Governance bodies should enforce dataset audits before production use.  

**HU:**  
A mÃ©rgezett adatok nemcsak biztonsÃ¡gi, hanem rendszerszintÅ± kockÃ¡zatot is jelentenek â€“ veszÃ©lyeztetik a mÃ©ltÃ¡nyossÃ¡got Ã©s az Ã¡tlÃ¡thatÃ³sÃ¡got.  
Az etikus AI-gyakorlat megkÃ¶veteli az adatok eredetÃ©nek, nyomon kÃ¶vethetÅ‘sÃ©gÃ©nek Ã©s forrÃ¡sÃ¡nak Ã¡tlÃ¡thatÃ³ dokumentÃ¡lÃ¡sÃ¡t.  
Az irÃ¡nyÃ­tÃ¡si szervezeteknek elÅ‘ kell Ã­rniuk az adathalmazok auditÃ¡lÃ¡sÃ¡t az Ã©les bevetÃ©s elÅ‘tt.

---

## ğŸ”­ Future Directions / JÃ¶vÅ‘beli irÃ¡nyok

**EN:**  
Future research explores:
- **Blockchain-backed dataset registries** for immutable provenance.  
- **Federated anomaly consensus** among distributed nodes.  
- **Causal data validation** to remove non-causal correlations.  
- **ZKP-based data attestation** ensuring verifiable dataset authenticity.  

**HU:**  
A jÃ¶vÅ‘ kutatÃ¡sai a kÃ¶vetkezÅ‘kre Ã¶sszpontosÃ­tanak:
- **Blockchain-alapÃº adathalmaz-regiszterek**, amelyek garantÃ¡ljÃ¡k a vÃ¡ltoztathatatlan eredetkÃ¶vetÃ©st.  
- **FederÃ¡lt anomÃ¡liadetektÃ¡lÃ¡s**, ahol a decentralizÃ¡lt csomÃ³pontok konszenzus alapjÃ¡n ismerik fel a tÃ¡madÃ¡sokat.  
- **OksÃ¡gi adatvalidÃ¡lÃ¡s**, amely kiszÅ±ri a nem oksÃ¡gi korrelÃ¡ciÃ³kat.  
- **ZKP-alapÃº hitelesÃ­tÃ©s**, amely kriptogrÃ¡fiailag igazolja az adathalmaz valÃ³disÃ¡gÃ¡t.

---

## ğŸ§­ Review Questions / EllenÅ‘rzÅ‘ kÃ©rdÃ©sek

1. How does data poisoning differ from adversarial examples?  
2. What is the main mathematical objective of a poisoning attack?  
3. How can drift detection assist in identifying dataset poisoning?  
4. Which lifecycle phases are most vulnerable to poisoning?  
5. What governance mechanisms prevent dataset tampering?

---

> â€œWhen learning begins with lies, truth can never emerge. Protect the origin, and you protect the mind.â€ ğŸ§©
